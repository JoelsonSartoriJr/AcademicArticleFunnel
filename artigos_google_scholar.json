[
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A multi-head attention-based transformer model for traffic flow forecasting with a comparative analysis to recurrent neural networks",
            "author": [
                "S Reza",
                "MC Ferreira",
                "JJM Machado"
            ],
            "pub_year": "2022",
            "venue": "Expert Systems with …",
            "abstract": "a multi-head attention mechanism based transformer for  have any embedded recurrent  neural network layer, this study uses  the encoder–decoder architecture deteriorated the model’s"
        },
        "filled": false,
        "gsrank": 1,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0957417422006443",
        "author_id": [
            "4q59GN0AAAAJ",
            "gEYLdsAAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:5m7EZsnr-o4J:scholar.google.com/&output=cite&scirp=0&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5m7EZsnr-o4J&ei=22ABZ7aqMqyCy9YPseaHkQg&json=",
        "num_citations": 142,
        "citedby_url": "/scholar?cites=10302806347808534246&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:5m7EZsnr-o4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0957417422006443"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Synthesizer: Rethinking self-attention for transformer models",
            "author": [
                "Y Tay",
                "D Bahri",
                "D Metzler",
                "DC Juan"
            ],
            "pub_year": "2021",
            "venue": "International …",
            "abstract": "-based content retrieval as an attention mechanism. On the other end  Synthetic Attention,  our proposed self-attention module. Our  For simplicity, we describe the per head and per layer"
        },
        "filled": false,
        "gsrank": 2,
        "pub_url": "https://proceedings.mlr.press/v139/tay21a.html",
        "author_id": [
            "VBclY_cAAAAJ",
            "j5PpTOwAAAAJ",
            "bmXpOd8AAAAJ",
            "-bgrJfsAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:d2tbBUAhy7kJ:scholar.google.com/&output=cite&scirp=1&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=d2tbBUAhy7kJ&ei=22ABZ7aqMqyCy9YPseaHkQg&json=",
        "num_citations": 355,
        "citedby_url": "/scholar?cites=13387830876140432247&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:d2tbBUAhy7kJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://proceedings.mlr.press/v139/tay21a/tay21a.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer-based attention networks for continuous pixel-wise prediction",
            "author": [
                "G Yang",
                "H Tang",
                "M Ding",
                "N Sebe"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the …",
            "abstract": "In this paper, we propose TransDepth, an architecture that  embedding module is removed  from our linear Transformer, but  , a network equipped with a channel attention mechanism to"
        },
        "filled": false,
        "gsrank": 3,
        "pub_url": "http://openaccess.thecvf.com/content/ICCV2021/html/Yang_Transformer-Based_Attention_Networks_for_Continuous_Pixel-Wise_Prediction_ICCV_2021_paper.html",
        "author_id": [
            "DHgNKnAAAAAJ",
            "9zJkeEMAAAAJ",
            "",
            "stFCYOAAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:GDRedcrYZ8YJ:scholar.google.com/&output=cite&scirp=2&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GDRedcrYZ8YJ&ei=22ABZ7aqMqyCy9YPseaHkQg&json=",
        "num_citations": 180,
        "citedby_url": "/scholar?cites=14296633906175030296&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:GDRedcrYZ8YJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Transformer-Based_Attention_Networks_for_Continuous_Pixel-Wise_Prediction_ICCV_2021_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention mechanism, transformers, BERT, and GPT: tutorial and survey",
            "author": [
                "B Ghojogh",
                "A Ghodsi"
            ],
            "pub_year": "2020",
            "venue": "NA",
            "abstract": "model with and without attention mechanism as well as the  This module applies the  attention mechanism for h times.  Then, it is fed to a feedforward neural network with layer nor"
        },
        "filled": false,
        "gsrank": 4,
        "pub_url": "https://osf.io/preprints/m6gcn/",
        "author_id": [
            "U8qAL-0AAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:zEwy2ze0e5QJ:scholar.google.com/&output=cite&scirp=3&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zEwy2ze0e5QJ&ei=22ABZ7aqMqyCy9YPseaHkQg&json=",
        "num_citations": 79,
        "citedby_url": "/scholar?cites=10699343491742452940&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:zEwy2ze0e5QJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://osf.io/m6gcn/download"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Lite vision transformer with enhanced self-attention",
            "author": [
                "C Yang",
                "Y Wang",
                "J Zhang",
                "H Zhang"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the …",
            "abstract": "network (FFN) that follows the self-attention layer. Both Swin- network [19,28], we formalize  RASA as a recursive module with  on transformer architecture with enhanced self-attention"
        },
        "filled": false,
        "gsrank": 5,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2022/html/Yang_Lite_Vision_Transformer_With_Enhanced_Self-Attention_CVPR_2022_paper.html",
        "author_id": [
            "DsumNkgAAAAJ",
            "fYqdLx4AAAAJ",
            "TkVHKDgAAAAJ",
            "HZLiJt0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:OEMusMOjJhYJ:scholar.google.com/&output=cite&scirp=4&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OEMusMOjJhYJ&ei=22ABZ7aqMqyCy9YPseaHkQg&json=",
        "num_citations": 115,
        "citedby_url": "/scholar?cites=1596143178819191608&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:OEMusMOjJhYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Lite_Vision_Transformer_With_Enhanced_Self-Attention_CVPR_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Re-transformer: a self-attention based model for machine translation",
            "author": [
                "HI Liu",
                "WL Chen"
            ],
            "pub_year": "2021",
            "venue": "Procedia Computer Science",
            "abstract": ", Re-Transformer modifies the basic architecture; there are  Self-Attention layer to Feed  Forward layer assists the model  models combine an attention mechanism to help the model to"
        },
        "filled": false,
        "gsrank": 6,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1877050921011509",
        "author_id": [
            "A4udjegAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:U_B0KviIIXMJ:scholar.google.com/&output=cite&scirp=5&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=U_B0KviIIXMJ&ei=22ABZ7aqMqyCy9YPseaHkQg&json=",
        "num_citations": 25,
        "citedby_url": "/scholar?cites=8296062588038738003&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:U_B0KviIIXMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1877050921011509/pdf?md5=c881665ea84caa32b890e41158ecf54d&pid=1-s2.0-S1877050921011509-main.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "R-transformer network based on position and self-attention mechanism for aspect-level sentiment classification",
            "author": [
                "Z Zhou",
                "Q Wang"
            ],
            "pub_year": "2019",
            "venue": "IEEE Access",
            "abstract": "Inspired by the human visual attention mechanism, the attention mechanism based on   PSRTN overall architecture. The PSRTN consists of three parts: The attention mechanism module"
        },
        "filled": false,
        "gsrank": 7,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/8822480/",
        "author_id": [
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:218NoP8W3jwJ:scholar.google.com/&output=cite&scirp=6&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=218NoP8W3jwJ&ei=22ABZ7aqMqyCy9YPseaHkQg&json=",
        "num_citations": 17,
        "citedby_url": "/scholar?cites=4385968374263144411&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:218NoP8W3jwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6287639/8600701/08822480.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Convolutional transformer: An enhanced attention mechanism architecture for remaining useful life estimation of bearings",
            "author": [
                "Y Ding",
                "M Jia"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on Instrumentation and …",
            "abstract": "(MSC) module with Swish activation for Transformer architecture to  previous layer, the MHA  module concatenates the attention  With MHA module, the network can extract long distance"
        },
        "filled": false,
        "gsrank": 8,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9792382/",
        "author_id": [
            "6UNuAmgAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:VK0_uJa_RXEJ:scholar.google.com/&output=cite&scirp=7&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VK0_uJa_RXEJ&ei=22ABZ7aqMqyCy9YPseaHkQg&json=",
        "num_citations": 57,
        "citedby_url": "/scholar?cites=8162140553736072532&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:VK0_uJa_RXEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/19/4407674/09792382.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A novel time–frequency Transformer based on self–attention mechanism and its application in fault diagnosis of rolling bearings",
            "author": [
                "Y Ding",
                "M Jia",
                "Q Miao",
                "Y Cao"
            ],
            "pub_year": "2022",
            "venue": "Mechanical Systems and Signal Processing",
            "abstract": "latest Transformer architecture based on attention mechanism has  only embed attention  mechanism as an auxiliary module into  a two-layer feed-forward network, whose hidden layer is"
        },
        "filled": false,
        "gsrank": 9,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0888327021009468",
        "author_id": [
            "6UNuAmgAAAAJ",
            "",
            "ypTIvjsAAAAJ",
            "wCD2yBYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:9PbGpF3yMeAJ:scholar.google.com/&output=cite&scirp=8&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9PbGpF3yMeAJ&ei=22ABZ7aqMqyCy9YPseaHkQg&json=",
        "num_citations": 243,
        "citedby_url": "/scholar?cites=16154959822365062900&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:9PbGpF3yMeAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0888327021009468"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multi-view self-attention based transformer for speaker recognition",
            "author": [
                "R Wang",
                "J Ao",
                "L Zhou",
                "S Liu",
                "Z Wei",
                "T Ko"
            ],
            "pub_year": "2022",
            "venue": "ICASSP 2022-2022 …",
            "abstract": "These works utilize the attention mechanism as a selection of  We study five variants of the  Transformer architecture for iden with a 6-layer encoder, a 3-layer decoder, 512 attention size,"
        },
        "filled": false,
        "gsrank": 10,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9746639/",
        "author_id": [
            "nGki_EEAAAAJ",
            "eUiG0O0AAAAJ",
            "ZnwgSXIAAAAJ",
            "6mNya-wAAAAJ",
            "",
            "26-lhTQAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:zY2_6Ym0ETEJ:scholar.google.com/&output=cite&scirp=9&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zY2_6Ym0ETEJ&ei=22ABZ7aqMqyCy9YPseaHkQg&json=",
        "num_citations": 47,
        "citedby_url": "/scholar?cites=3535805686887714253&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:zY2_6Ym0ETEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9745891/9746004/09746639.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A novel transformer-based network with attention mechanism for automatic pavement crack detection",
            "author": [
                "F Guo",
                "J Liu",
                "C Lv",
                "H Yu"
            ],
            "pub_year": "2023",
            "venue": "Construction and Building Materials",
            "abstract": "-Decoder architecture with CNN is that it cannot model the long- A shared network that  includes the multi-layer perceptron ( original design of the CBAM module, we apply the sequential"
        },
        "filled": false,
        "gsrank": 11,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0950061823015659",
        "author_id": [
            "greJMZQAAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Ib1bj-QHXNgJ:scholar.google.com/&output=cite&scirp=10&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D10%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ib1bj-QHXNgJ&ei=pmEBZ4mgFb-Ay9YPkfXz2Qs&json=",
        "num_citations": 23,
        "citedby_url": "/scholar?cites=15590344688288906529&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Ib1bj-QHXNgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0950061823015659"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Aiatrack: Attention in attention for transformer visual tracking",
            "author": [
                "S Gao",
                "C Zhou",
                "C Ma",
                "X Wang",
                "J Yuan"
            ],
            "pub_year": "2022",
            "venue": "European Conference on …",
            "abstract": "this insight to the attention mechanism for the first time, making  AiA module, we design a  simple yet effective Transformer  of a network backbone, a Transformer architecture, and two"
        },
        "filled": false,
        "gsrank": 12,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-031-20047-2_9",
        "author_id": [
            "hZtOnecAAAAJ",
            "L2AMIaoAAAAJ",
            "syoPhv8AAAAJ",
            "qNCTLV0AAAAJ",
            "fJ7seq0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:_12innEbU10J:scholar.google.com/&output=cite&scirp=11&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D10%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_12innEbU10J&ei=pmEBZ4mgFb-Ay9YPkfXz2Qs&json=",
        "num_citations": 223,
        "citedby_url": "/scholar?cites=6724748843400977919&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:_12innEbU10J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2207.09603"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "LAS-transformer: An enhanced transformer based on the local attention mechanism for speech recognition",
            "author": [
                "P Fu",
                "D Liu",
                "H Yang"
            ],
            "pub_year": "2022",
            "venue": "Information",
            "abstract": "at the embedding layer. Moreover, we propose a local attention module to explicitly   From speech to letters-using a novel neural network architecture for grapheme based ASR. In"
        },
        "filled": false,
        "gsrank": 13,
        "pub_url": "https://www.mdpi.com/2078-2489/13/5/250",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:42UWWznbQnEJ:scholar.google.com/&output=cite&scirp=12&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D10%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=42UWWznbQnEJ&ei=pmEBZ4mgFb-Ay9YPkfXz2Qs&json=",
        "num_citations": 6,
        "citedby_url": "/scholar?cites=8161326514136573411&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:42UWWznbQnEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2078-2489/13/5/250/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Aspect-based sentiment classification with multi-attention network",
            "author": [
                "Q Xu",
                "L Zhu",
                "T Dai",
                "C Yan"
            ],
            "pub_year": "2020",
            "venue": "Neurocomputing",
            "abstract": "a global and a local attention module to capture differently  is the first model in which an  attention mechanism performs aspect- We apply a transformer encoder to the word embedding to"
        },
        "filled": false,
        "gsrank": 14,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S092523122030059X",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:1Ymy4UZnW4MJ:scholar.google.com/&output=cite&scirp=13&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D10%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1Ymy4UZnW4MJ&ei=pmEBZ4mgFb-Ay9YPkfXz2Qs&json=",
        "num_citations": 113,
        "citedby_url": "/scholar?cites=9465272595981175253&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:1Ymy4UZnW4MJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S092523122030059X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Contextual attention network: Transformer meets u-net",
            "author": [
                "R Azad",
                "M Heidari",
                "Y Wu",
                "D Merhof"
            ],
            "pub_year": "2022",
            "venue": "International Workshop on Machine …",
            "abstract": "from the CNN module. Our design proposes a contextual attention mechanism for feature   can provide an additional input signal for a reliable and robust segmentation architecture. It"
        },
        "filled": false,
        "gsrank": 15,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-031-21014-3_39",
        "author_id": [
            "Qb5ildMAAAAJ",
            "mir8D5UAAAAJ",
            "qlun0AgAAAAJ",
            "0c0rMr0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:b4ZIRjAzPUsJ:scholar.google.com/&output=cite&scirp=14&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D10%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=b4ZIRjAzPUsJ&ei=pmEBZ4mgFb-Ay9YPkfXz2Qs&json=",
        "num_citations": 70,
        "citedby_url": "/scholar?cites=5421545808854550127&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:b4ZIRjAzPUsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2203.01932"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention mechanism in neural networks: where it comes and where it goes",
            "author": [
                "D Soydaner"
            ],
            "pub_year": "2022",
            "venue": "Neural Computing and Applications",
            "abstract": "A recurrent soft attention based model learns to focus  part of the Transformer, masked  multi-head attention is applied  with the vanilla Transformer architecture on image generation and"
        },
        "filled": false,
        "gsrank": 16,
        "pub_url": "https://link.springer.com/article/10.1007/s00521-022-07366-3",
        "author_id": [
            "FPXh-WkAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:tSXH40rzg04J:scholar.google.com/&output=cite&scirp=15&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D10%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tSXH40rzg04J&ei=pmEBZ4mgFb-Ay9YPkfXz2Qs&json=",
        "num_citations": 112,
        "citedby_url": "/scholar?cites=5657633059882083765&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:tSXH40rzg04J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s00521-022-07366-3.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Set transformer: A framework for attention-based permutation-invariant neural networks",
            "author": [
                "J Lee",
                "Y Lee",
                "J Kim",
                "A Kosiorek"
            ],
            "pub_year": "2019",
            "venue": "… on machine learning",
            "abstract": "deep neural network architecture called the Set Transformer, ( Section 2.1), but a distinguishing  feature is that each layer in  , we adapt the multihead attention mechanism used in Trans"
        },
        "filled": false,
        "gsrank": 17,
        "pub_url": "http://proceedings.mlr.press/v97/lee19d.html",
        "author_id": [
            "Py4URJUAAAAJ",
            "BAAZ_ysAAAAJ",
            "KXNUYWgAAAAJ",
            "i7eVfzwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:199HUvTu1QcJ:scholar.google.com/&output=cite&scirp=16&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D10%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=199HUvTu1QcJ&ei=pmEBZ4mgFb-Ay9YPkfXz2Qs&json=",
        "num_citations": 1286,
        "citedby_url": "/scholar?cites=564620061424738263&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:199HUvTu1QcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://proceedings.mlr.press/v97/lee19d/lee19d.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Dual-aspect self-attention based on transformer for remaining useful life prediction",
            "author": [
                "Z Zhang",
                "W Song",
                "Q Li"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on Instrumentation …",
            "abstract": "In this method, we apply the transformer architecture [24] to  attention mechanism and temporal  convolutional network ( encoder layer uses the multihead self-attention mechanism [24] to"
        },
        "filled": false,
        "gsrank": 18,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9737516/",
        "author_id": [
            "",
            "s8Nz-xoAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:lxb0kGxiflgJ:scholar.google.com/&output=cite&scirp=17&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D10%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lxb0kGxiflgJ&ei=pmEBZ4mgFb-Ay9YPkfXz2Qs&json=",
        "num_citations": 168,
        "citedby_url": "/scholar?cites=6376642340831106711&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:lxb0kGxiflgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/19/4407674/09737516.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "When shift operation meets vision transformer: An extremely simple alternative to attention mechanism",
            "author": [
                "G Wang",
                "Y Zhao",
                "C Tang",
                "C Luo",
                "W Zeng"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the AAAI …",
            "abstract": "We propose to replace the attention layer with a simple shift  backbones are based on the  architecture of Swin Transformer already the simplest spatial modelling module, we argue that"
        },
        "filled": false,
        "gsrank": 19,
        "pub_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20142",
        "author_id": [
            "cKY8e8sAAAAJ",
            "QWemjjQAAAAJ",
            "3ZC8B7MAAAAJ",
            "01iBf38AAAAJ",
            "_cUfvYQAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:WwH7R3WlQswJ:scholar.google.com/&output=cite&scirp=18&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D10%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WwH7R3WlQswJ&ei=pmEBZ4mgFb-Ay9YPkfXz2Qs&json=",
        "num_citations": 58,
        "citedby_url": "/scholar?cites=14718508455337591131&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:WwH7R3WlQswJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20142/19901"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Clinical knowledge-based ECG abnormalities detection using dual-view CNN-Transformer and external attention mechanism",
            "author": [
                "H Li",
                "J Han",
                "H Zhang",
                "X Zhang",
                "Y Si",
                "Y Zhang"
            ],
            "pub_year": "2024",
            "venue": "Computers in Biology …",
            "abstract": "Table 1 lists the details of the module architecture designed  Each encoder layer is composed  of a Multi-Head Attention  external attention-based dual-view CNN-Transformer module to"
        },
        "filled": false,
        "gsrank": 20,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0010482524008369",
        "author_id": [
            "",
            "",
            "",
            "OCXZaZgAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:vYmHf83-XxYJ:scholar.google.com/&output=cite&scirp=19&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D10%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vYmHf83-XxYJ&ei=pmEBZ4mgFb-Ay9YPkfXz2Qs&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=1612287350183266749&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:vYmHf83-XxYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0010482524008369"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Enhancing heart disease prediction using a self-attention-based transformer model",
            "author": [
                "AU Rahman",
                "Y Alsenani",
                "A Zafar",
                "K Ullah",
                "K Rabie"
            ],
            "pub_year": "2024",
            "venue": "Scientific Reports",
            "abstract": "This attention-based model architecture with self-attention  , we modify our model’s local-based  attention mechanism to  with an attention layer is followed by a classification layer. This"
        },
        "filled": false,
        "gsrank": 21,
        "pub_url": "https://www.nature.com/articles/s41598-024-51184-7",
        "author_id": [
            "srwkX04AAAAJ",
            "0lYgMg0AAAAJ",
            "_XrxatIAAAAJ",
            "4SPZxS4AAAAJ",
            "nsKrJq0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:kwHuKJQtxIQJ:scholar.google.com/&output=cite&scirp=20&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D20%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kwHuKJQtxIQJ&ei=cGIBZ5LvOcDBy9YP683bgAE&json=",
        "num_citations": 20,
        "citedby_url": "/scholar?cites=9566821622806872467&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:kwHuKJQtxIQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.nature.com/articles/s41598-024-51184-7.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A multi-step ahead global solar radiation prediction method using an attention-based transformer model with an interpretable mechanism",
            "author": [
                "Y Zhou",
                "Y Li",
                "D Wang",
                "Y Liu"
            ],
            "pub_year": "2023",
            "venue": "International Journal of Hydrogen Energy",
            "abstract": "model, which consists only of the attention mechanism. The  This architecture provides  great flexibility in practical  multi-head attention layer, a multi-head attention layer, and a"
        },
        "filled": false,
        "gsrank": 22,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0360319923000691",
        "author_id": [
            "AOPbkqkAAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:S17KgIKO6McJ:scholar.google.com/&output=cite&scirp=21&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D20%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=S17KgIKO6McJ&ei=cGIBZ5LvOcDBy9YP683bgAE&json=",
        "num_citations": 18,
        "citedby_url": "/scholar?cites=14404920099302170187&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:S17KgIKO6McJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0360319923000691"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Beyond self-attention: External attention using two linear layers for visual tasks",
            "author": [
                "MH Guo",
                "ZN Liu",
                "TJ Mu",
                "SM Hu"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on Pattern …",
            "abstract": "a novel attention mechanism which we call external attention,  attention module named  external attention, which computes  with common CNN-based and transformer-based methods."
        },
        "filled": false,
        "gsrank": 23,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9912362/",
        "author_id": [
            "DnXVAgcAAAAJ",
            "wjPqa9EAAAAJ",
            "V-0oiTYAAAAJ",
            "LDb4tb0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:wKzny3HUDZcJ:scholar.google.com/&output=cite&scirp=22&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D20%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wKzny3HUDZcJ&ei=cGIBZ5LvOcDBy9YP683bgAE&json=",
        "num_citations": 516,
        "citedby_url": "/scholar?cites=10884589459641707712&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:wKzny3HUDZcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/34/4359286/09912362.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "UTNet: a hybrid transformer architecture for medical image segmentation",
            "author": [
                "Y Gao",
                "M Zhou",
                "DN Metaxas"
            ],
            "pub_year": "2021",
            "venue": "… , France, September 27–October 1, 2021 …",
            "abstract": "Our hybrid layer design allows the initialization of Transformer into  the self-attention  module on top of the feature maps from the CNN backbone, we apply the Transformer module to"
        },
        "filled": false,
        "gsrank": 24,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-87199-4_6",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:YUDwOR1iEBAJ:scholar.google.com/&output=cite&scirp=23&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D20%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YUDwOR1iEBAJ&ei=cGIBZ5LvOcDBy9YP683bgAE&json=",
        "num_citations": 492,
        "citedby_url": "/scholar?cites=1157532981899837537&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:YUDwOR1iEBAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2107.00781"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multi-head self-attention transformer for dogecoin price prediction",
            "author": [
                "S Sridhar",
                "S Sanagavarapu"
            ],
            "pub_year": "2021",
            "venue": "2021 14th International …",
            "abstract": "In this paper, a multi-head attention-based transformer  The transformer uses a multi-head  attention mechanism to map  Figure 2 shows the overall architecture of the transformer model."
        },
        "filled": false,
        "gsrank": 25,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9538640/",
        "author_id": [
            "v97f514AAAAJ",
            "FN16ygUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:SvrfG5nmyTwJ:scholar.google.com/&output=cite&scirp=24&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D20%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SvrfG5nmyTwJ&ei=cGIBZ5LvOcDBy9YP683bgAE&json=",
        "num_citations": 43,
        "citedby_url": "/scholar?cites=4380285657866566218&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:SvrfG5nmyTwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9538522/9538525/09538640.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Medical transformer: Gated axial-attention for medical image segmentation",
            "author": [
                "JMJ Valanarasu",
                "P Oza",
                "I Hacihaliloglu"
            ],
            "pub_year": "2021",
            "venue": "Medical image computing …",
            "abstract": "an additional control mechanism in the self-attention module.  -sensitive axial attention  mechanism where we introduce four  architecture MedT performs better than Gated axial attention,"
        },
        "filled": false,
        "gsrank": 26,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-87193-2_4",
        "author_id": [
            "",
            "9dhBHuAAAAAJ",
            "dA7G64kAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:AqKo-gNkpKcJ:scholar.google.com/&output=cite&scirp=25&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D20%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AqKo-gNkpKcJ&ei=cGIBZ5LvOcDBy9YP683bgAE&json=",
        "num_citations": 1137,
        "citedby_url": "/scholar?cites=12079890068767547906&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:AqKo-gNkpKcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2102.10662"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer encoder with multi-modal multi-head attention for continuous affect recognition",
            "author": [
                "H Chen",
                "D Jiang",
                "H Sahli"
            ],
            "pub_year": "2020",
            "venue": "IEEE Transactions on Multimedia",
            "abstract": "features through a multi-head attention mechanism; and finally, (3)  into the LSTM based  encoder-decoder architecture. More  the multi-head attention module used in the transformer-"
        },
        "filled": false,
        "gsrank": 27,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9257201/",
        "author_id": [
            "",
            "Awsue7sAAAAJ",
            "1hUjXaYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:nZwoi9ihoTMJ:scholar.google.com/&output=cite&scirp=26&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D20%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nZwoi9ihoTMJ&ei=cGIBZ5LvOcDBy9YP683bgAE&json=",
        "num_citations": 81,
        "citedby_url": "/scholar?cites=3720432718604442781&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:nZwoi9ihoTMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6046/4456689/09257201.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Tree transformer: Integrating tree structures into self-attention",
            "author": [
                "YS Wang",
                "HY Lee",
                "YN Chen"
            ],
            "pub_year": "2019",
            "venue": "arXiv preprint arXiv:1909.06639",
            "abstract": "Attention” module, which is simply implemented by  same network architecture, but with  different sets of network  of attention heads from each layer of the original Transformer"
        },
        "filled": false,
        "gsrank": 28,
        "pub_url": "https://arxiv.org/abs/1909.06639",
        "author_id": [
            "_sURVUgAAAAJ",
            "DxLO11IAAAAJ",
            "jQLg-_UAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:IORV-54NiTIJ:scholar.google.com/&output=cite&scirp=27&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D20%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IORV-54NiTIJ&ei=cGIBZ5LvOcDBy9YP683bgAE&json=",
        "num_citations": 169,
        "citedby_url": "/scholar?cites=3641456750178460704&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:IORV-54NiTIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/1909.06639"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A survey on vision transformer",
            "author": [
                "K Han",
                "Y Wang",
                "H Chen",
                "X Chen",
                "J Guo"
            ],
            "pub_year": "2022",
            "venue": "IEEE transactions on …",
            "abstract": "proposed transformer based on attention mechanism for  due to the cross-attention module  in the transformer decoder. To  model named TransPose based on Transformer architecture"
        },
        "filled": false,
        "gsrank": 29,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9716741/",
        "author_id": [
            "vThoBVcAAAAJ",
            "isizOkYAAAAJ",
            "wZ9N88gAAAAJ",
            "tuGWUVIAAAAJ",
            "UnAbd4gAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:G7PR8u-tYDsJ:scholar.google.com/&output=cite&scirp=28&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D20%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=G7PR8u-tYDsJ&ei=cGIBZ5LvOcDBy9YP683bgAE&json=",
        "num_citations": 2024,
        "citedby_url": "/scholar?cites=4278610892084589339&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:G7PR8u-tYDsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/34/9970415/09716741.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A survey on visual transformer",
            "author": [
                "K Han",
                "Y Wang",
                "H Chen",
                "X Chen",
                "J Guo",
                "Z Liu"
            ],
            "pub_year": "2020",
            "venue": "arXiv preprint arXiv …",
            "abstract": "[9] first proposed transformer based on attention mechanism for  is similar to the self-attention  layer in the encoder module  model named TransPose based on Transformer architecture"
        },
        "filled": false,
        "gsrank": 30,
        "pub_url": "https://arxiv.org/abs/2012.12556",
        "author_id": [
            "vThoBVcAAAAJ",
            "isizOkYAAAAJ",
            "wZ9N88gAAAAJ",
            "tuGWUVIAAAAJ",
            "UnAbd4gAAAAJ",
            "bihqxP4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Aj10Crv7DScJ:scholar.google.com/&output=cite&scirp=29&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D20%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Aj10Crv7DScJ&ei=cGIBZ5LvOcDBy9YP683bgAE&json=",
        "num_citations": 363,
        "citedby_url": "/scholar?cites=2814182122929274114&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Aj10Crv7DScJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2012.12556"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Vision transformer with deformable attention",
            "author": [
                "Z Xia",
                "X Pan",
                "S Song",
                "LE Li"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the IEEE …",
            "abstract": "propose a novel deformable self-attention module, where the  revisit the attention mechanism  in recent Vision Transformers.  of the network architecture, our model, Deformable Attention"
        },
        "filled": false,
        "gsrank": 31,
        "pub_url": "https://openaccess.thecvf.com/content/CVPR2022/html/Xia_Vision_Transformer_With_Deformable_Attention_CVPR_2022_paper.html?ref=https://githubhelp.com",
        "author_id": [
            "m2M6b58AAAAJ",
            "pIg5Qc4AAAAJ",
            "rw6vWdcAAAAJ",
            "GkMfzy4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:KYwPK25Z3ZsJ:scholar.google.com/&output=cite&scirp=30&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D30%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KYwPK25Z3ZsJ&ei=PGMBZ-jKDayCy9YPseaHkQg&json=",
        "num_citations": 501,
        "citedby_url": "/scholar?cites=11231231375435598889&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:KYwPK25Z3ZsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Xia_Vision_Transformer_With_Deformable_Attention_CVPR_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Neighborhood attention transformer",
            "author": [
                "A Hassani",
                "S Walton",
                "J Li",
                "S Li"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the IEEE …",
            "abstract": "and scalable sliding window attention mechanism for vision. NA is  more vision transformers,  and attentionbased models in  An illustration of the overall network architecture is presented"
        },
        "filled": false,
        "gsrank": 32,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2023/html/Hassani_Neighborhood_Attention_Transformer_CVPR_2023_paper.html",
        "author_id": [
            "Ndu0dUcAAAAJ",
            "he4JY7wAAAAJ",
            "mTHic3EAAAAJ",
            "5qD1sLEAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:j4aKOnoM074J:scholar.google.com/&output=cite&scirp=31&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D30%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=j4aKOnoM074J&ei=PGMBZ-jKDayCy9YPseaHkQg&json=",
        "num_citations": 252,
        "citedby_url": "/scholar?cites=13750347806399956623&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:j4aKOnoM074J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Hassani_Neighborhood_Attention_Transformer_CVPR_2023_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Hardware accelerator for multi-head attention and position-wise feed-forward in the transformer",
            "author": [
                "S Lu",
                "M Wang",
                "S Liang",
                "J Lin"
            ],
            "pub_year": "2020",
            "venue": "2020 IEEE 33rd …",
            "abstract": "of the attention mechanism, the Transformer and Transformer the proposed architecture,  the latency of layer normalization  The Softmax module in the proposed architecture is used to"
        },
        "filled": false,
        "gsrank": 33,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9524802/",
        "author_id": [
            "cvUoCBEAAAAJ",
            "fFweGwIAAAAJ",
            "",
            "fzkrb4UAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:yZoHks70DiwJ:scholar.google.com/&output=cite&scirp=32&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D30%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yZoHks70DiwJ&ei=PGMBZ-jKDayCy9YPseaHkQg&json=",
        "num_citations": 101,
        "citedby_url": "/scholar?cites=3174743955393190601&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:yZoHks70DiwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9524739/9524720/09524802.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer tracking",
            "author": [
                "X Chen",
                "B Yan",
                "J Zhu",
                "D Wang"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the …",
            "abstract": "by Transformer, this work presents a novel attention-based  encoder-decoder architecture  in the original Transformer as it  idea of Transformer and exploit the attention mechanism to"
        },
        "filled": false,
        "gsrank": 34,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2021/html/Chen_Transformer_Tracking_CVPR_2021_paper.html",
        "author_id": [
            "A04HWTIAAAAJ",
            "3f8qn4cAAAAJ",
            "j_gYsS8AAAAJ",
            "nVgPQpoAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:gLJ-dqGPzpQJ:scholar.google.com/&output=cite&scirp=33&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D30%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gLJ-dqGPzpQJ&ei=PGMBZ-jKDayCy9YPseaHkQg&json=",
        "num_citations": 1149,
        "citedby_url": "/scholar?cites=10722665686456251008&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:gLJ-dqGPzpQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Transformer_Tracking_CVPR_2021_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer architecture and attention mechanisms in genome data analysis: a comprehensive review",
            "author": [
                "SR Choi",
                "M Lee"
            ],
            "pub_year": "2023",
            "venue": "Biology",
            "abstract": "the transformer architecture and the attention mechanism in  The model leverages an  attention-based mechanism to learn  An attention mechanism layer is incorporated into each base"
        },
        "filled": false,
        "gsrank": 35,
        "pub_url": "https://www.mdpi.com/2079-7737/12/7/1033",
        "author_id": [
            "",
            "QNtOrigAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:LY1VJ0g70YsJ:scholar.google.com/&output=cite&scirp=34&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D30%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LY1VJ0g70YsJ&ei=PGMBZ-jKDayCy9YPseaHkQg&json=",
        "num_citations": 37,
        "citedby_url": "/scholar?cites=10074899022488112429&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:LY1VJ0g70YsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2079-7737/12/7/1033/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "An abstractive text summarization technique using transformer model with self-attention mechanism",
            "author": [
                "S Kumar",
                "A Solanki"
            ],
            "pub_year": "2023",
            "venue": "Neural Computing and Applications",
            "abstract": "transformer architecture with an attention mechanism and a  Feed-forward layer and the  crucial Self-attention layer, which  network and a knowledge-based hierarchical attention module"
        },
        "filled": false,
        "gsrank": 36,
        "pub_url": "https://link.springer.com/article/10.1007/s00521-023-08687-7",
        "author_id": [
            "g8f-07wAAAAJ",
            "lJUCGWkAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:_m7l34EEWaAJ:scholar.google.com/&output=cite&scirp=35&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D30%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_m7l34EEWaAJ&ei=PGMBZ-jKDayCy9YPseaHkQg&json=",
        "num_citations": 19,
        "citedby_url": "/scholar?cites=11554271274849365758&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:_m7l34EEWaAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s00521-023-08687-7.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer based multi-grained attention network for aspect-based sentiment analysis",
            "author": [
                "J Sun",
                "P Han",
                "Z Cheng",
                "E Wu",
                "W Wang"
            ],
            "pub_year": "2020",
            "venue": "IEEE Access",
            "abstract": "And then we utilize the attention mechanism multiple times to  improvement of multilayer  network architecture fitting and  the proposed model not only employs Transformer module to"
        },
        "filled": false,
        "gsrank": 37,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9265203/",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:eL2nV0Mohy8J:scholar.google.com/&output=cite&scirp=36&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D30%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eL2nV0Mohy8J&ei=PGMBZ-jKDayCy9YPseaHkQg&json=",
        "num_citations": 24,
        "citedby_url": "/scholar?cites=3424750311337082232&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:eL2nV0Mohy8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6287639/6514899/09265203.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer in transformer",
            "author": [
                "K Han",
                "A Xiao",
                "E Wu",
                "J Guo",
                "C Xu"
            ],
            "pub_year": "2021",
            "venue": "Advances in neural …",
            "abstract": "features via the attention mechanism. Basically, the visual  a new architecture, namely,  Transformer iN Transformer (TNT).  In the self-attention module, the inputs X ∈ Rn×d are linearly"
        },
        "filled": false,
        "gsrank": 38,
        "pub_url": "https://proceedings.neurips.cc/paper/2021/hash/854d9fca60b4bd07f9bb215d59ef5561-Abstract.html",
        "author_id": [
            "vThoBVcAAAAJ",
            "Q2J2qK4AAAAJ",
            "",
            "UnAbd4gAAAAJ",
            "-CJ5LkMAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:_9P7mh-CUNIJ:scholar.google.com/&output=cite&scirp=37&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D30%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_9P7mh-CUNIJ&ei=PGMBZ-jKDayCy9YPseaHkQg&json=",
        "num_citations": 1646,
        "citedby_url": "/scholar?cites=15154755818357511167&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:_9P7mh-CUNIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/854d9fca60b4bd07f9bb215d59ef5561-Paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Robust wave-feature adaptive heartbeat classification based on self-attention mechanism using a transformer model",
            "author": [
                "S Hu",
                "W Cai",
                "T Gao",
                "J Zhou"
            ],
            "pub_year": "2021",
            "venue": "Physiological …",
            "abstract": "the feature attention mechanism (FAM) and location attention  model based on the proposed  transformer model architecture,  encoder module of the transformer to perform self-attention"
        },
        "filled": false,
        "gsrank": 39,
        "pub_url": "https://iopscience.iop.org/article/10.1088/1361-6579/ac3e88/meta",
        "author_id": [
            "worq2P0AAAAJ",
            "prBUY1UAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:dMDD_N7JiN8J:scholar.google.com/&output=cite&scirp=38&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D30%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dMDD_N7JiN8J&ei=PGMBZ-jKDayCy9YPseaHkQg&json=",
        "num_citations": 13,
        "citedby_url": "/scholar?cites=16107346026851188852&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:dMDD_N7JiN8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.researchgate.net/profile/Shuaicong-Hu/publication/356681190_Robust_wave-feature_adaptive_heartbeat_classification_based_on_self-attention_mechanism_using_a_transformer_model/links/63744e5437878b3e87b4feb9/Robust-wave-feature-adaptive-heartbeat-classification-based-on-self-attention-mechanism-using-a-transformer-model.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "BViT: Broad attention-based vision transformer",
            "author": [
                "N Li",
                "Y Chen",
                "W Li",
                "Z Ding",
                "D Zhao"
            ],
            "pub_year": "2023",
            "venue": "… on Neural Networks and …",
            "abstract": "attention mechanism that can efficiently extract and utilize the knowledge in each transformer  layer ] searched high-performed transformer-based models through one-shot architecture"
        },
        "filled": false,
        "gsrank": 40,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10113704/",
        "author_id": [
            "_8YxTk0AAAAJ",
            "",
            "",
            "eTPzv2MAAAAJ",
            "RxvYlNQAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:dtIgpvN-EKEJ:scholar.google.com/&output=cite&scirp=39&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D30%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dtIgpvN-EKEJ&ei=PGMBZ-jKDayCy9YPseaHkQg&json=",
        "num_citations": 13,
        "citedby_url": "/scholar?cites=11605915824663089782&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:dtIgpvN-EKEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/5962385/6104215/10113704.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Swin-transformer-enabled YOLOv5 with attention mechanism for small object detection on satellite images",
            "author": [
                "H Gong",
                "T Mu",
                "Q Li",
                "H Dai",
                "C Li",
                "Z He",
                "W Wang",
                "F Han"
            ],
            "pub_year": "2022",
            "venue": "Remote Sensing",
            "abstract": "a new branch in the shallower network layer for detecting small  This architecture has the  flexibility to model at various scales  transformer prediction head, but also another C3 module in"
        },
        "filled": false,
        "gsrank": 41,
        "pub_url": "https://www.mdpi.com/2072-4292/14/12/2861",
        "author_id": [
            "",
            "xNl4gewAAAAJ",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:hbXE-TFUSH4J:scholar.google.com/&output=cite&scirp=40&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D40%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hbXE-TFUSH4J&ei=BmQBZ8j1OqSMy9YPoKv0mAY&json=",
        "num_citations": 133,
        "citedby_url": "/scholar?cites=9099615620722636165&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:hbXE-TFUSH4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2072-4292/14/12/2861/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Refiner: Refining self-attention for vision transformers",
            "author": [
                "D Zhou",
                "Y Shi",
                "B Kang",
                "W Yu",
                "Z Jiang",
                "Y Li"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv …",
            "abstract": "local attention mechanism that combines both strengths of  the baseline model and replace  the self-attention module with  Thus, we design the architecture of Refiner-ViT based on the"
        },
        "filled": false,
        "gsrank": 42,
        "pub_url": "https://arxiv.org/abs/2106.03714",
        "author_id": [
            "DdCAbWwAAAAJ",
            "Okeolr8AAAAJ",
            "NmHgX-wAAAAJ",
            "LYxjt1QAAAAJ",
            "Wo8tMSMAAAAJ",
            "-5juAR0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:ZYIefC4_aXIJ:scholar.google.com/&output=cite&scirp=41&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D40%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZYIefC4_aXIJ&ei=BmQBZ8j1OqSMy9YPoKv0mAY&json=",
        "num_citations": 71,
        "citedby_url": "/scholar?cites=8244190061761823333&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:ZYIefC4_aXIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.researchgate.net/profile/Zhou-Daquan/publication/352209841_Refiner_Refining_Self-attention_for_Vision_Transformers/links/60c22db34585157774c7a935/Refiner-Refining-Self-attention-for-Vision-Transformers.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Deepvit: Towards deeper vision transformer",
            "author": [
                "D Zhou",
                "B Kang",
                "X Jin",
                "L Yang",
                "X Lian",
                "Z Jiang"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv …",
            "abstract": "architecture, we would like to highlight that the self-attention  replace the self-attention module  in ViT with Reattention and  and propose a novel Re-attention mechanism to solve it with"
        },
        "filled": false,
        "gsrank": 43,
        "pub_url": "https://arxiv.org/abs/2103.11886",
        "author_id": [
            "DdCAbWwAAAAJ",
            "NmHgX-wAAAAJ",
            "OEZ816YAAAAJ",
            "XptEO8oAAAAJ",
            "-jNTDU0AAAAJ",
            "Wo8tMSMAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:RG8p5sZv1HYJ:scholar.google.com/&output=cite&scirp=42&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D40%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RG8p5sZv1HYJ&ei=BmQBZ8j1OqSMy9YPoKv0mAY&json=",
        "num_citations": 602,
        "citedby_url": "/scholar?cites=8562591691593838404&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:RG8p5sZv1HYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2103.11886"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Local enhancing transformer with temporal convolutional attention mechanism for bearings remaining useful life prediction",
            "author": [
                "H Peng",
                "B Jiang",
                "Z Mao",
                "S Liu"
            ],
            "pub_year": "2023",
            "venue": "IEEE Transactions on …",
            "abstract": "Recently, the Transformer, a novel network architecture  Transformer by introducing a  standard convolutional layer to  , a TCN attention module combing TCN and SE attention [37] is"
        },
        "filled": false,
        "gsrank": 44,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10177813/",
        "author_id": [
            "",
            "FRXg54EAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:x_qAdUQmODoJ:scholar.google.com/&output=cite&scirp=43&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D40%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=x_qAdUQmODoJ&ei=BmQBZ8j1OqSMy9YPoKv0mAY&json=",
        "num_citations": 18,
        "citedby_url": "/scholar?cites=4195145128366635719&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:x_qAdUQmODoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/19/4407674/10177813.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Global–local self-attention based transformer for speaker verification",
            "author": [
                "F Xie",
                "D Zhang",
                "C Liu"
            ],
            "pub_year": "2022",
            "venue": "Applied Sciences",
            "abstract": "The attention mechanism is at the heart of Transformer’s  , followed by a statistical pooling  layer and two linear layers. In  work used self-attention as the backbone of our architecture. Its"
        },
        "filled": false,
        "gsrank": 45,
        "pub_url": "https://www.mdpi.com/2076-3417/12/19/10154",
        "author_id": [
            "",
            "",
            "OsuH_wgAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:cd6gVY6Y_WIJ:scholar.google.com/&output=cite&scirp=44&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D40%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cd6gVY6Y_WIJ&ei=BmQBZ8j1OqSMy9YPoKv0mAY&json=",
        "num_citations": 8,
        "citedby_url": "/scholar?cites=7133025121914117745&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:cd6gVY6Y_WIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2076-3417/12/19/10154/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Efficient temporal flow Transformer accompanied with multi-head probsparse self-attention mechanism for remaining useful life prognostics",
            "author": [
                "Y Chang",
                "F Li",
                "J Chen",
                "Y Liu",
                "Z Li"
            ],
            "pub_year": "2022",
            "venue": "Reliability Engineering & System Safety",
            "abstract": "As a new architecture, the Transformer completely relies on  The encoder module mainly  undertakes the function of mining  Comparisons of different attention mechanism in various RUL"
        },
        "filled": false,
        "gsrank": 46,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S095183202200326X",
        "author_id": [
            "",
            "",
            "wEGyDvkAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:9hFpOPRYYEsJ:scholar.google.com/&output=cite&scirp=45&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D40%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9hFpOPRYYEsJ&ei=BmQBZ8j1OqSMy9YPoKv0mAY&json=",
        "num_citations": 51,
        "citedby_url": "/scholar?cites=5431438956550492662&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:9hFpOPRYYEsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S095183202200326X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A PV cell defect detector combined with transformer and attention mechanism",
            "author": [
                "D Lang",
                "Z Lv"
            ],
            "pub_year": "2024",
            "venue": "Scientific Reports",
            "abstract": "the YOLO architecture, integrating an attention mechanism and the Transformer module. We   Positioning the CCT in the backbone's final layer minimally increases the model's overall"
        },
        "filled": false,
        "gsrank": 47,
        "pub_url": "https://www.nature.com/articles/s41598-024-72019-5",
        "author_id": [
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:1G7If8eRkscJ:scholar.google.com/&output=cite&scirp=46&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D40%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1G7If8eRkscJ&ei=BmQBZ8j1OqSMy9YPoKv0mAY&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:1G7If8eRkscJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.nature.com/articles/s41598-024-72019-5.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "U-net transformer: Self and cross attention for medical image segmentation",
            "author": [
                "O Petit",
                "N Thome",
                "C Rambour",
                "L Themyr"
            ],
            "pub_year": "2021",
            "venue": "Machine Learning in …",
            "abstract": ", which combines a U-shaped architecture for image  Firstly, a self-attention module  leverages global interactions between  Our cross-attention mechanism shares the high-level"
        },
        "filled": false,
        "gsrank": 48,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-87589-3_28",
        "author_id": [
            "9aMV880AAAAJ",
            "3f3Zq-8AAAAJ",
            "5lGyobkAAAAJ",
            "VRBupZAAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:UzXgfpZ0sk4J:scholar.google.com/&output=cite&scirp=47&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D40%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UzXgfpZ0sk4J&ei=BmQBZ8j1OqSMy9YPoKv0mAY&json=",
        "num_citations": 271,
        "citedby_url": "/scholar?cites=5670723070535415123&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:UzXgfpZ0sk4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2103.06104"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A hybrid transformer model for obstructive sleep apnea detection based on self-attention mechanism using single-lead ECG",
            "author": [
                "S Hu",
                "W Cai",
                "T Gao",
                "M Wang"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on …",
            "abstract": ", a hybrid Transformer model leveraging attention mechanism is  processing module. Figure  1 shows the constructed data.  three parallel two-layer convolutional networks with different"
        },
        "filled": false,
        "gsrank": 49,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9837102/",
        "author_id": [
            "worq2P0AAAAJ",
            "prBUY1UAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:RlhZVBSTGh8J:scholar.google.com/&output=cite&scirp=48&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D40%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RlhZVBSTGh8J&ei=BmQBZ8j1OqSMy9YPoKv0mAY&json=",
        "num_citations": 26,
        "citedby_url": "/scholar?cites=2241265480094013510&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:RlhZVBSTGh8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/19/4407674/09837102.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Evaluation of transformer model and self-attention mechanism in the Yangtze River basin runoff prediction",
            "author": [
                "X Wei",
                "G Wang",
                "B Schmalz",
                "DFT Hagan"
            ],
            "pub_year": "2023",
            "venue": "Journal of Hydrology …",
            "abstract": "The Transformer (TSF) is a newly proposed neural network  Likely, the Attention Mechanism  (AM) is a solution to arrange the  or GRU model, an SA module, and a fully connected layer."
        },
        "filled": false,
        "gsrank": 50,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S2214581823001258",
        "author_id": [
            "",
            "SVYw9dcAAAAJ",
            "IytaXmsAAAAJ",
            "KUfwjYsAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:ySMcblQL9ZEJ:scholar.google.com/&output=cite&scirp=49&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D40%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ySMcblQL9ZEJ&ei=BmQBZ8j1OqSMy9YPoKv0mAY&json=",
        "num_citations": 19,
        "citedby_url": "/scholar?cites=10517324962046157769&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:ySMcblQL9ZEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S2214581823001258"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Dae-former: Dual attention-guided efficient transformer for medical image segmentation",
            "author": [
                "R Azad",
                "R Arimond",
                "EK Aghdam",
                "A Kazerouni"
            ],
            "pub_year": "2023",
            "venue": "… Workshop on PRedictive …",
            "abstract": "U-Net-like pure Transformer architecture, namely, the DAE- -attention module in each skip  connection path. Our contributions are as follows: ❶ a novel efficient dual attention mechanism"
        },
        "filled": false,
        "gsrank": 51,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-031-46005-0_8",
        "author_id": [
            "Qb5ildMAAAAJ",
            "",
            "",
            "aKDCc3MAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:491R8XdU6O0J:scholar.google.com/&output=cite&scirp=50&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D50%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=491R8XdU6O0J&ei=0WQBZ7eGNoHOy9YPxaCm4AE&json=",
        "num_citations": 76,
        "citedby_url": "/scholar?cites=17143044855712308707&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:491R8XdU6O0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2212.13504"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Local-to-global self-attention in vision transformers",
            "author": [
                "J Li",
                "Y Yan",
                "S Liao",
                "X Yang",
                "L Shao"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv:2107.04735",
            "abstract": "1-3), we expand the self-attention module into a multi-path structure,  the detailed architecture  of the proposed LG-Transformer. As  multi-path attention mechanism and demonstrate the"
        },
        "filled": false,
        "gsrank": 52,
        "pub_url": "https://arxiv.org/abs/2107.04735",
        "author_id": [
            "",
            "ZPHMMRkAAAAJ",
            "CnqsHlAAAAAJ",
            "yDEavdMAAAAJ",
            "z84rLjoAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:HWVvUGDaOMkJ:scholar.google.com/&output=cite&scirp=51&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D50%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HWVvUGDaOMkJ&ei=0WQBZ7eGNoHOy9YPxaCm4AE&json=",
        "num_citations": 41,
        "citedby_url": "/scholar?cites=14499579107520505117&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:HWVvUGDaOMkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2107.04735"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Improving transformer-based conversational ASR by inter-sentential attention mechanism",
            "author": [
                "K Wei",
                "P Guo",
                "N Jiang"
            ],
            "pub_year": "2022",
            "venue": "arXiv preprint arXiv:2207.00883",
            "abstract": "end-to-end architecture for conversational speech recog Inspired by [23], we include a residual  attention module in the encoder layer and Attn is the dot-product attention mechanism, as"
        },
        "filled": false,
        "gsrank": 53,
        "pub_url": "https://arxiv.org/abs/2207.00883",
        "author_id": [
            "LAMJHvAAAAAJ",
            "qOPuzPMAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:151I0a_BIDsJ:scholar.google.com/&output=cite&scirp=52&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D50%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=151I0a_BIDsJ&ei=0WQBZ7eGNoHOy9YPxaCm4AE&json=",
        "num_citations": 10,
        "citedby_url": "/scholar?cites=4260618208367123927&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:151I0a_BIDsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2207.00883"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers",
            "author": [
                "H Chefer",
                "S Gur",
                "L Wolf"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the IEEE/CVF …",
            "abstract": "Recall the attention mechanism presented in [40]:  at each layer, we can consider an  aggregated self-attention matrix Rqq as  a self-attention based architecture, and the second model"
        },
        "filled": false,
        "gsrank": 54,
        "pub_url": "http://openaccess.thecvf.com/content/ICCV2021/html/Chefer_Generic_Attention-Model_Explainability_for_Interpreting_Bi-Modal_and_Encoder-Decoder_Transformers_ICCV_2021_paper.html",
        "author_id": [
            "B8sA9JoAAAAJ",
            "uuuU23UAAAAJ",
            "UbFrXTsAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:mGMZ2qL6GTsJ:scholar.google.com/&output=cite&scirp=53&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D50%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mGMZ2qL6GTsJ&ei=0WQBZ7eGNoHOy9YPxaCm4AE&json=",
        "num_citations": 269,
        "citedby_url": "/scholar?cites=4258710500006257560&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:mGMZ2qL6GTsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Chefer_Generic_Attention-Model_Explainability_for_Interpreting_Bi-Modal_and_Encoder-Decoder_Transformers_ICCV_2021_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers",
            "author": [
                "W Wang",
                "F Wei",
                "L Dong",
                "H Bao"
            ],
            "pub_year": "2020",
            "venue": "Advances in Neural …",
            "abstract": "distilling the self-attention module of the last Transformer layer of  of the student is required  to have the same architecture as its  The attention mechanism [3] has been a highly successful"
        },
        "filled": false,
        "gsrank": 55,
        "pub_url": "https://proceedings.neurips.cc/paper/2020/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html",
        "author_id": [
            "45XvCHUAAAAJ",
            "G-V1VpwAAAAJ",
            "wEfQgPgAAAAJ",
            "lXCZGqYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:UvqPmRFnPM4J:scholar.google.com/&output=cite&scirp=54&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D50%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UvqPmRFnPM4J&ei=0WQBZ7eGNoHOy9YPxaCm4AE&json=",
        "num_citations": 1067,
        "citedby_url": "/scholar?cites=14860866195704248914&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:UvqPmRFnPM4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper/2020/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Simplified self-attention for transformer-based end-to-end speech recognition",
            "author": [
                "H Luo",
                "S Zhang",
                "M Lei",
                "L Xie"
            ],
            "pub_year": "2021",
            "venue": "2021 IEEE Spoken Language …",
            "abstract": "a simplified self-attention (SSAN) layer which employs FSMN  In this paper, we focus on  attention-based models, aiming at  For the 1000-hour task, we use the best model architecture on"
        },
        "filled": false,
        "gsrank": 56,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9383581/",
        "author_id": [
            "",
            "BcWMSE4AAAAJ",
            "",
            "Qddov9wAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:wfBDTk4r8DoJ:scholar.google.com/&output=cite&scirp=55&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D50%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wfBDTk4r8DoJ&ei=0WQBZ7eGNoHOy9YPxaCm4AE&json=",
        "num_citations": 30,
        "citedby_url": "/scholar?cites=4246942063930896577&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:wfBDTk4r8DoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9383468/9383452/09383581.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Slide-transformer: Hierarchical vision transformer with local self-attention",
            "author": [
                "X Pan",
                "T Ye",
                "Z Xia",
                "S Song"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the IEEE …",
            "abstract": "human design, which sets the least restrictions on the model architecture design.  attention  mechanism and address its efficiency overhead by proposing a novel Slide Attention module"
        },
        "filled": false,
        "gsrank": 57,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2023/html/Pan_Slide-Transformer_Hierarchical_Vision_Transformer_With_Local_Self-Attention_CVPR_2023_paper.html",
        "author_id": [
            "pIg5Qc4AAAAJ",
            "7X8BCBsAAAAJ",
            "m2M6b58AAAAJ",
            "rw6vWdcAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:LNfrZoIP8egJ:scholar.google.com/&output=cite&scirp=56&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D50%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LNfrZoIP8egJ&ei=0WQBZ7eGNoHOy9YPxaCm4AE&json=",
        "num_citations": 44,
        "citedby_url": "/scholar?cites=16785214338933446444&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:LNfrZoIP8egJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Slide-Transformer_Hierarchical_Vision_Transformer_With_Local_Self-Attention_CVPR_2023_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A novel transformer-based attention network for image dehazing",
            "author": [
                "G Gao",
                "J Cao",
                "C Bao",
                "Q Hao",
                "A Ma",
                "G Li"
            ],
            "pub_year": "2022",
            "venue": "Sensors",
            "abstract": "Using the attention mechanism pays attention to detailed  a module that uses the Transformer  architecture to obtain channel  K ( x ) through a convolutional layer. Table 1 shows the"
        },
        "filled": false,
        "gsrank": 58,
        "pub_url": "https://www.mdpi.com/1424-8220/22/9/3428",
        "author_id": [
            "",
            "zP21dQoAAAAJ",
            "uiGpQIEAAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:aShZomh_sxUJ:scholar.google.com/&output=cite&scirp=57&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D50%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aShZomh_sxUJ&ei=0WQBZ7eGNoHOy9YPxaCm4AE&json=",
        "num_citations": 14,
        "citedby_url": "/scholar?cites=1563733583004772457&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:aShZomh_sxUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/1424-8220/22/9/3428/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "AttentionMGT-DTA: A multi-modal drug-target affinity prediction using graph transformer and attention mechanism",
            "author": [
                "H Wu",
                "J Liu",
                "T Jiang",
                "Q Zou",
                "S Qi",
                "Z Cui",
                "P Tiwari"
            ],
            "pub_year": "2024",
            "venue": "Neural Networks",
            "abstract": "Thus, a cross-attention module was leveraged in our method  Secondly, the architecture of  graph transformer model  define the detailed update process of the l th layer: (6) Q i j k , l = Q k"
        },
        "filled": false,
        "gsrank": 59,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S089360802300641X",
        "author_id": [
            "",
            "w6XeRtAAAAAJ",
            "",
            "RcaUHs4AAAAJ",
            "",
            "",
            "sDnmJ_YAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Zk3yDklCAe8J:scholar.google.com/&output=cite&scirp=58&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D50%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Zk3yDklCAe8J&ei=0WQBZ7eGNoHOy9YPxaCm4AE&json=",
        "num_citations": 24,
        "citedby_url": "/scholar?cites=17222119331592293734&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Zk3yDklCAe8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S089360802300641X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transattunet: Multi-level attention-guided u-net with transformer for medical image segmentation",
            "author": [
                "B Chen",
                "Y Liu",
                "Z Zhang",
                "G Lu"
            ],
            "pub_year": "2023",
            "venue": "IEEE Transactions on …",
            "abstract": "segmentation architecture. Inspired by Transformer, the self-aware attention (SAA) module  with  The multi-head attention mechanism processes each attention head separately and"
        },
        "filled": false,
        "gsrank": 60,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10244199/",
        "author_id": [
            "dE0UAg0AAAAJ",
            "",
            "tpVOb2EAAAAJ",
            "fhwB7UwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:dKUpzZbZbz4J:scholar.google.com/&output=cite&scirp=59&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D50%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dKUpzZbZbz4J&ei=0WQBZ7eGNoHOy9YPxaCm4AE&json=",
        "num_citations": 222,
        "citedby_url": "/scholar?cites=4499053794476795252&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:dKUpzZbZbz4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/7433297/7777658/10244199.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention transformer mechanism and fusion-based deep learning architecture for MRI brain tumor classification system",
            "author": [
                "S Tabatabaei",
                "K Rezaee",
                "M Zhu"
            ],
            "pub_year": "2023",
            "venue": "Biomedical Signal Processing and …",
            "abstract": "model that integrates the Transformer Module (TM) with the  utilizes a normalized dot product  attention mechanism [48]. Input x i in  A fusion layer with a bidirectional fusion architecture"
        },
        "filled": false,
        "gsrank": 61,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1746809423005529",
        "author_id": [
            "nEVteFIAAAAJ",
            "ZBllPqoAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:zTH3FruqXxoJ:scholar.google.com/&output=cite&scirp=60&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D60%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zTH3FruqXxoJ&ei=nWUBZ87uG6yCy9YPseaHkQg&json=",
        "num_citations": 25,
        "citedby_url": "/scholar?cites=1900425288294543821&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:zTH3FruqXxoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1746809423005529"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "FAM: Improving columnar vision transformer with feature attention mechanism",
            "author": [
                "L Huang",
                "X Bai",
                "J Zeng",
                "M Yu",
                "W Pang"
            ],
            "pub_year": "2024",
            "venue": "Computer Vision and …",
            "abstract": ", the multi-head attention sub-layer is utilized to model non- -stem with Feature Attention  Module (FAM) during model training and  that the network architecture significantly influences the"
        },
        "filled": false,
        "gsrank": 62,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1077314224000626",
        "author_id": [
            "",
            "",
            "",
            "",
            "kfFNe0cAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:oKb2wAaScHUJ:scholar.google.com/&output=cite&scirp=61&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D60%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oKb2wAaScHUJ&ei=nWUBZ87uG6yCy9YPseaHkQg&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=8462424257534011040&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:oKb2wAaScHUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1077314224000626"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "MDF-SA-DDI: predicting drug–drug interaction events based on multi-source drug fusion, multi-source feature fusion and transformer self-attention mechanism",
            "author": [
                "S Lin",
                "Y Wang",
                "L Zhang",
                "Y Chu",
                "Y Liu"
            ],
            "pub_year": "2022",
            "venue": "Briefings in …",
            "abstract": "Therefore, we add a self-attention layer before the output layer of the  Multi-head attention  mechanism  and input them to the multi-head attention module, which is also called the"
        },
        "filled": false,
        "gsrank": 63,
        "pub_url": "https://academic.oup.com/bib/article-abstract/23/1/bbab421/6406700",
        "author_id": [
            "",
            "",
            "u_Jy24oAAAAJ",
            "n5-R1ZUAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:SoTMz2qKnSgJ:scholar.google.com/&output=cite&scirp=62&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D60%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SoTMz2qKnSgJ&ei=nWUBZ87uG6yCy9YPseaHkQg&json=",
        "num_citations": 100,
        "citedby_url": "/scholar?cites=2926647524218143818&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:SoTMz2qKnSgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.researchgate.net/profile/Dong-Qing-Wei/publication/355470462_MDF-SA-DDI_Predicting_drug-drug_interaction_events_based_on_multi-source_drug_fusion_multi-source_feature_fusion_and_transformer_self-attention_mechanism/links/6171b7bc750da711ac676895/MDF-SA-DDI-Predicting-drug-drug-interaction-events-based-on-multi-source-drug-fusion-multi-source-feature-fusion-and-transformer-self-attention-mechanism.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Spatial convolutional self-attention-based transformer module for strawberry disease identification under complex background",
            "author": [
                "G Li",
                "L Jiao",
                "P Chen",
                "K Liu",
                "R Wang",
                "S Dong"
            ],
            "pub_year": "2023",
            "venue": "… and Electronics in …",
            "abstract": "of an image, attention mechanism has been introduced to  Block Ⅳ have the same network  architecture with that of Block Ⅱ.  map by using the output layer W o . In summary, the SCSA-"
        },
        "filled": false,
        "gsrank": 64,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0168169923005094",
        "author_id": [
            "",
            "rAXnDAcAAAAJ",
            "9uHIfl4AAAAJ",
            "3-t5KAoAAAAJ",
            "",
            "PR_hEL8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:e8Aamehgk5QJ:scholar.google.com/&output=cite&scirp=63&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D60%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=e8Aamehgk5QJ&ei=nWUBZ87uG6yCy9YPseaHkQg&json=",
        "num_citations": 13,
        "citedby_url": "/scholar?cites=10706007291307147387&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:e8Aamehgk5QJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0168169923005094"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Improved transformer net for hyperspectral image classification",
            "author": [
                "Y Qing",
                "W Liu",
                "L Feng",
                "W Gao"
            ],
            "pub_year": "2021",
            "venue": "Remote Sensing",
            "abstract": "The proposed model uses the spectral attention mechanism  proposes a self-attention-based  transformer (SAT) model for  neural network (CNN) [58] (CNN architecture with five layers of"
        },
        "filled": false,
        "gsrank": 65,
        "pub_url": "https://www.mdpi.com/2072-4292/13/11/2216",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:oKD7evu1oscJ:scholar.google.com/&output=cite&scirp=64&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D60%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oKD7evu1oscJ&ei=nWUBZ87uG6yCy9YPseaHkQg&json=",
        "num_citations": 140,
        "citedby_url": "/scholar?cites=14385260251479515296&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:oKD7evu1oscJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2072-4292/13/11/2216/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention-based interpretability with concept transformers",
            "author": [
                "M Rigotti",
                "C Miksovic",
                "I Giurgiu",
                "T Gschwind"
            ],
            "pub_year": "2021",
            "venue": "International …",
            "abstract": "performance gains of deep neural network models in a host of  architecture, plausibility is  achieved by construction by supervising the attention heads of the cross-attention mechanism"
        },
        "filled": false,
        "gsrank": 66,
        "pub_url": "https://openreview.net/forum?id=kAa9eDS0RdO",
        "author_id": [
            "TmHt7CwAAAAJ",
            "Y_WtyywAAAAJ",
            "2NI-034AAAAJ",
            "qlGYL1QAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:vlbAHlCqmeUJ:scholar.google.com/&output=cite&scirp=65&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D60%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vlbAHlCqmeUJ&ei=nWUBZ87uG6yCy9YPseaHkQg&json=",
        "num_citations": 53,
        "citedby_url": "/scholar?cites=16544441967212254910&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:vlbAHlCqmeUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openreview.net/pdf?id=kAa9eDS0RdO"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Point transformer",
            "author": [
                "N Engel",
                "V Belagiannis",
                "K Dietmayer"
            ],
            "pub_year": "2021",
            "venue": "IEEE access",
            "abstract": "We aim to make use of the attention mechanism to capture the  present attention and introduce  the Transformer architecture  The network is permutation invariant due to a new module"
        },
        "filled": false,
        "gsrank": 67,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9552005/",
        "author_id": [
            "k5JHFKcAAAAJ",
            "4IlWd90AAAAJ",
            "u49itfsAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:t_aDxbptFykJ:scholar.google.com/&output=cite&scirp=66&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D60%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=t_aDxbptFykJ&ei=nWUBZ87uG6yCy9YPseaHkQg&json=",
        "num_citations": 242,
        "citedby_url": "/scholar?cites=2960955928964495031&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:t_aDxbptFykJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6287639/6514899/09552005.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer interpretability beyond attention visualization",
            "author": [
                "H Chefer",
                "S Gur",
                "L Wolf"
            ],
            "pub_year": "2021",
            "venue": "… of the IEEE/CVF conference on …",
            "abstract": "lack of conservation in the attention mechanism due to matrix  The self-attention module  operates on a small sub-space dh  the architecture and propagation of information in the network"
        },
        "filled": false,
        "gsrank": 68,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2021/html/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.html",
        "author_id": [
            "B8sA9JoAAAAJ",
            "uuuU23UAAAAJ",
            "UbFrXTsAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:CcFGhIJiKXkJ:scholar.google.com/&output=cite&scirp=67&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D60%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CcFGhIJiKXkJ&ei=nWUBZ87uG6yCy9YPseaHkQg&json=",
        "num_citations": 720,
        "citedby_url": "/scholar?cites=8730617665338917129&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:CcFGhIJiKXkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Variational attention-based interpretable transformer network for rotary machine fault diagnosis",
            "author": [
                "Y Li",
                "Z Zhou",
                "C Sun",
                "X Chen"
            ],
            "pub_year": "2022",
            "venue": "… on neural networks and …",
            "abstract": "attention mechanism is widely applied to interpreting model  Our proposed model architecture  is comprised of a feature  in the multihead self-attention layer, and we will describe it in"
        },
        "filled": false,
        "gsrank": 69,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9887963/",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:zhHLwu8sGqwJ:scholar.google.com/&output=cite&scirp=68&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D60%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zhHLwu8sGqwJ&ei=nWUBZ87uG6yCy9YPseaHkQg&json=",
        "num_citations": 71,
        "citedby_url": "/scholar?cites=12401273932194976206&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:zhHLwu8sGqwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/5962385/6104215/09887963.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A review on the attention mechanism of deep learning",
            "author": [
                "Z Niu",
                "G Zhong",
                "H Yu"
            ],
            "pub_year": "2021",
            "venue": "Neurocomputing",
            "abstract": "network architecture of attention models, the memory network -forward network (FFN) layer  and multi-head attention layer.  , the spatial transformer module is a dynamic mechanism that"
        },
        "filled": false,
        "gsrank": 70,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S092523122100477X",
        "author_id": [
            "",
            "HqKD-fwAAAAJ",
            "K9bBXkYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:2BmMArIDz08J:scholar.google.com/&output=cite&scirp=69&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D60%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2BmMArIDz08J&ei=nWUBZ87uG6yCy9YPseaHkQg&json=",
        "num_citations": 1943,
        "citedby_url": "/scholar?cites=5750819312257210840&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:2BmMArIDz08J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S092523122100477X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "YOLOV5-CBAM-C3TR: an optimized model based on transformer module and attention mechanism for apple leaf disease detection",
            "author": [
                "M Lv",
                "WH Su"
            ],
            "pub_year": "2024",
            "venue": "Frontiers in Plant Science",
            "abstract": "the classical transformer encoder architecture. Illustrated in  , Multi-head attention, and  feedforward neural network (FFN).  at the last layer of backbone network, the optimized YOLOV5-"
        },
        "filled": false,
        "gsrank": 71,
        "pub_url": "https://www.frontiersin.org/articles/10.3389/fpls.2023.1323301/full",
        "author_id": [
            "",
            "RGAh_rcAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:ofhs03ctd8YJ:scholar.google.com/&output=cite&scirp=70&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D70%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ofhs03ctd8YJ&ei=aGYBZ6vzBrCDy9YPgrj5gAQ&json=",
        "num_citations": 10,
        "citedby_url": "/scholar?cites=14300949134409791649&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:ofhs03ctd8YJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.frontiersin.org/articles/10.3389/fpls.2023.1323301/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Sequential transformer via an outside-in attention for image captioning",
            "author": [
                "Y Wei",
                "C Wu",
                "G Li",
                "H Shi"
            ],
            "pub_year": "2022",
            "venue": "Engineering Applications of Artificial Intelligence",
            "abstract": "generating process, an attention mechanism is used in  to integrate such module into the  Transformer encoder–decoder  replace the attention layer in Sequence Memory Module, while"
        },
        "filled": false,
        "gsrank": 72,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0952197621004073",
        "author_id": [
            "ZD3JBK0AAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:RwNSATF57rMJ:scholar.google.com/&output=cite&scirp=71&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D70%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RwNSATF57rMJ&ei=aGYBZ6vzBrCDy9YPgrj5gAQ&json=",
        "num_citations": 20,
        "citedby_url": "/scholar?cites=12965433628628747079&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:RwNSATF57rMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0952197621004073"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention mechanism in intelligent fault diagnosis of machinery: A review of technique and application",
            "author": [
                "H Lv",
                "J Chen",
                "T Pan",
                "T Zhang",
                "Y Feng",
                "S Liu"
            ],
            "pub_year": "2022",
            "venue": "Measurement",
            "abstract": "Transformer was first applied to NLP and achieved results  -based, Convolution-based  and Self-Attention-based. We will  But the full connected layer has insufficient ability to extract"
        },
        "filled": false,
        "gsrank": 73,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0263224122008077",
        "author_id": [
            "",
            "wEGyDvkAAAAJ",
            "",
            "",
            "",
            "qKOUAFYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Wbu9juxGZPkJ:scholar.google.com/&output=cite&scirp=72&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D70%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Wbu9juxGZPkJ&ei=aGYBZ6vzBrCDy9YPgrj5gAQ&json=",
        "num_citations": 118,
        "citedby_url": "/scholar?cites=17970566394936146777&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Wbu9juxGZPkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0263224122008077"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Efficient content-based sparse attention with routing transformers",
            "author": [
                "A Roy",
                "M Saffar",
                "A Vaswani",
                "D Grangier"
            ],
            "pub_year": "2021",
            "venue": "Transactions of the …",
            "abstract": "with a sparse routing module based on online k-means while  we train a 22 layer Routing  Transformer model with 8 heads  Overall, our work contributes an efficient attention mechanism"
        },
        "filled": false,
        "gsrank": 74,
        "pub_url": "https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00353/97776",
        "author_id": [
            "mCmda68AAAAJ",
            "p1cmEzsAAAAJ",
            "oR9sCGYAAAAJ",
            "CIQEGCYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:y8TAdKAFuqEJ:scholar.google.com/&output=cite&scirp=73&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D70%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=y8TAdKAFuqEJ&ei=aGYBZ6vzBrCDy9YPgrj5gAQ&json=",
        "num_citations": 549,
        "citedby_url": "/scholar?cites=11653633172486276299&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:y8TAdKAFuqEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00353/1923932/tacl_a_00353.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Focal attention for long-range interactions in vision transformers",
            "author": [
                "J Yang",
                "C Li",
                "P Zhang",
                "X Dai",
                "B Xiao"
            ],
            "pub_year": "2021",
            "venue": "Advances in Neural …",
            "abstract": ", we present focal attention, a new attention mechanism that  focal attention into a multi-scale  Transformer architecture, we  that is widely applicable to all attention-based neural network"
        },
        "filled": false,
        "gsrank": 75,
        "pub_url": "https://proceedings.neurips.cc/paper/2021/hash/fc1a36821b02abbd2503fd949bfc9131-Abstract.html",
        "author_id": [
            "Cl9byD8AAAAJ",
            "Zd7WmXUAAAAJ",
            "3VZ_E64AAAAJ",
            "QC8RwcoAAAAJ",
            "t5HZdzoAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:hvLmho5YsXoJ:scholar.google.com/&output=cite&scirp=74&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D70%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hvLmho5YsXoJ&ei=aGYBZ6vzBrCDy9YPgrj5gAQ&json=",
        "num_citations": 143,
        "citedby_url": "/scholar?cites=8840944912676876934&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:hvLmho5YsXoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper/2021/file/fc1a36821b02abbd2503fd949bfc9131-Paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer-based personalized attention mechanism for medical images with clinical records",
            "author": [
                "Y Takagi",
                "N Hashimoto",
                "H Masuda",
                "H Miyoshi"
            ],
            "pub_year": "2023",
            "venue": "Journal of Pathology …",
            "abstract": "attention-based MIL, there is no mechanism to change the  We introduce a variant of the  Transformer architecture that  had a hidden layer with 256 units and an output layer with 3 units"
        },
        "filled": false,
        "gsrank": 76,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S2153353922007854",
        "author_id": [
            "",
            "",
            "",
            "8XBkysUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:o8e0jcq928UJ:scholar.google.com/&output=cite&scirp=75&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D70%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=o8e0jcq928UJ&ei=aGYBZ6vzBrCDy9YPgrj5gAQ&json=",
        "num_citations": 8,
        "citedby_url": "/scholar?cites=14257197723029915555&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:o8e0jcq928UJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/am/pii/S2153353922007854"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A tensorized transformer for language modeling",
            "author": [
                "X Ma",
                "P Zhang",
                "S Zhang",
                "N Duan"
            ],
            "pub_year": "2019",
            "venue": "Advances in neural …",
            "abstract": "Transformer [35] is based solely on the attention mechanism,  AttenTD(·) is the function of  Single-block attention based on  architecture, and replace the standard multi-head attention"
        },
        "filled": false,
        "gsrank": 77,
        "pub_url": "https://proceedings.neurips.cc/paper/2019/hash/dc960c46c38bd16e953d97cdeefdbc68-Abstract.html",
        "author_id": [
            "",
            "tvDb5_cAAAAJ",
            "RsZcMZcAAAAJ",
            "Qaa6OxIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:orxX2kCPKo0J:scholar.google.com/&output=cite&scirp=76&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D70%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=orxX2kCPKo0J&ei=aGYBZ6vzBrCDy9YPgrj5gAQ&json=",
        "num_citations": 180,
        "citedby_url": "/scholar?cites=10172100217073548450&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:orxX2kCPKo0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper/2019/file/dc960c46c38bd16e953d97cdeefdbc68-Paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Crossvit: Cross-attention multi-scale vision transformer for image classification",
            "author": [
                "CFR Chen",
                "Q Fan",
                "R Panda"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the IEEE/CVF …",
            "abstract": "cross-attention module, in which each transformer branch  of our proposed transformer  architecture for learning multi- a cross-attention module of a given xl with layer normalization"
        },
        "filled": false,
        "gsrank": 78,
        "pub_url": "http://openaccess.thecvf.com/content/ICCV2021/html/Chen_CrossViT_Cross-Attention_Multi-Scale_Vision_Transformer_for_Image_Classification_ICCV_2021_paper.html",
        "author_id": [
            "9gqd5cYAAAAJ",
            "kCxHiwUAAAAJ",
            "_ySuu6gAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:iraXEWcuIowJ:scholar.google.com/&output=cite&scirp=77&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D70%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=iraXEWcuIowJ&ei=aGYBZ6vzBrCDy9YPgrj5gAQ&json=",
        "num_citations": 1504,
        "citedby_url": "/scholar?cites=10097684334729737866&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:iraXEWcuIowJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_CrossViT_Cross-Attention_Multi-Scale_Vision_Transformer_for_Image_Classification_ICCV_2021_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Efficient self-attention mechanism and structural distilling model for Alzheimer's disease diagnosis",
            "author": [
                "J Zhu",
                "Y Tan",
                "R Lin",
                "J Miao",
                "X Fan",
                "Y Zhu"
            ],
            "pub_year": "2022",
            "venue": "Computers in Biology …",
            "abstract": "While CNNs were developing, a model called Transformer  the structural distilling layer,  which is a computational module that  of the model was improved by the attention mechanism to"
        },
        "filled": false,
        "gsrank": 79,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0010482522005108",
        "author_id": [
            "E6wgtekAAAAJ",
            "5lK0uNgAAAAJ",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:qJ4RXJwL-yYJ:scholar.google.com/&output=cite&scirp=78&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D70%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qJ4RXJwL-yYJ&ei=aGYBZ6vzBrCDy9YPgrj5gAQ&json=",
        "num_citations": 36,
        "citedby_url": "/scholar?cites=2808851558783098536&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:qJ4RXJwL-yYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0010482522005108"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Molecule attention transformer",
            "author": [
                "Ł Maziarka",
                "T Danel",
                "S Mucha",
                "K Rataj",
                "J Tabor"
            ],
            "pub_year": "2020",
            "venue": "arXiv preprint arXiv …",
            "abstract": "augment the attention mechanism in Transformer using inter- model based on the state-of-the-art  Transformer architecture  Transformer is that it augments the self-attention module using"
        },
        "filled": false,
        "gsrank": 80,
        "pub_url": "https://arxiv.org/abs/2002.08264",
        "author_id": [
            "2dkp8z4AAAAJ",
            "WZq_OCsAAAAJ",
            "",
            "uStCOZMAAAAJ",
            "zSKYziUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:QGq9vLLmA60J:scholar.google.com/&output=cite&scirp=79&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D70%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QGq9vLLmA60J&ei=aGYBZ6vzBrCDy9YPgrj5gAQ&json=",
        "num_citations": 199,
        "citedby_url": "/scholar?cites=12467061848836762176&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:QGq9vLLmA60J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2002.08264"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Per-former: rethinking person re-identification using transformer augmented with self-attention and contextual mapping",
            "author": [
                "N Pervaiz",
                "MM Fraz",
                "M Shahzad"
            ],
            "pub_year": "2023",
            "venue": "The Visual Computer",
            "abstract": "transformer architecture with a lightweight self-attention module,  The spatial transformer  networks module samples an  of SCM module in each layer of the baseline architecture"
        },
        "filled": false,
        "gsrank": 81,
        "pub_url": "https://link.springer.com/article/10.1007/s00371-022-02577-0",
        "author_id": [
            "8q5XnWQAAAAJ",
            "tpfgan0AAAAJ",
            "XKmKkY8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:AHGeeAdobjIJ:scholar.google.com/&output=cite&scirp=80&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D80%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AHGeeAdobjIJ&ei=MmcBZ979OIWoy9YPtZ-o2Q4&json=",
        "num_citations": 15,
        "citedby_url": "/scholar?cites=3633956330632278272&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:AHGeeAdobjIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s00371-022-02577-0.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Long-short transformer: Efficient transformers for language and vision",
            "author": [
                "C Zhu",
                "W Ping",
                "C Xiao",
                "M Shoeybi"
            ],
            "pub_year": "2021",
            "venue": "Advances in neural …",
            "abstract": "suffixes are our long-short term attention based on the official ViL  does not depend on the  particular architecture, and it can be  We design a novel global attention mechanism with linear"
        },
        "filled": false,
        "gsrank": 82,
        "pub_url": "https://proceedings.neurips.cc/paper/2021/hash/9425be43ba92c2b4454ca7bf602efad8-Abstract.html",
        "author_id": [
            "m-om5O8AAAAJ",
            "6gKEYRgAAAAJ",
            "Juoqtj8AAAAJ",
            "62ElavIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:2xzjWEVzlIEJ:scholar.google.com/&output=cite&scirp=81&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D80%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2xzjWEVzlIEJ&ei=MmcBZ979OIWoy9YPtZ-o2Q4&json=",
        "num_citations": 129,
        "citedby_url": "/scholar?cites=9337214669127097563&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:2xzjWEVzlIEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper/2021/file/9425be43ba92c2b4454ca7bf602efad8-Paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer-based deep reverse attention network for multi-sensory human activity recognition",
            "author": [
                "R Pramanik",
                "R Sikdar",
                "R Sarkar"
            ],
            "pub_year": "2023",
            "venue": "Engineering Applications of Artificial …",
            "abstract": "deep reverse transformer-based attention mechanism to guide  We put together a 3-block  CNN architecture, where each block  To be specific, we propose a reverse attention module to"
        },
        "filled": false,
        "gsrank": 83,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0952197623003342",
        "author_id": [
            "HK0s3E0AAAAJ",
            "",
            "bDj0BUEAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:o3cCbXZuhlYJ:scholar.google.com/&output=cite&scirp=82&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D80%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=o3cCbXZuhlYJ&ei=MmcBZ979OIWoy9YPtZ-o2Q4&json=",
        "num_citations": 15,
        "citedby_url": "/scholar?cites=6234792189055104931&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:o3cCbXZuhlYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0952197623003342"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "An improved U-net segmentation model that integrates a dual attention mechanism and a residual network for transformer oil leakage detection",
            "author": [
                "X Li",
                "X Liu",
                "Y Xiao",
                "Y Zhang",
                "X Yang",
                "W Zhang"
            ],
            "pub_year": "2022",
            "venue": "Energies",
            "abstract": "U-Net architecture, we removed the full connection layer in the original ResNet18 network  and  are more consistent with the ground truth than the model with only one attention module."
        },
        "filled": false,
        "gsrank": 84,
        "pub_url": "https://www.mdpi.com/1996-1073/15/12/4238",
        "author_id": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:TNY9-jFe0W8J:scholar.google.com/&output=cite&scirp=83&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D80%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TNY9-jFe0W8J&ei=MmcBZ979OIWoy9YPtZ-o2Q4&json=",
        "num_citations": 10,
        "citedby_url": "/scholar?cites=8057324777087293004&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:TNY9-jFe0W8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/1996-1073/15/12/4238/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A multi-head self-attention transformer-based model for traffic situation prediction in terminal areas",
            "author": [
                "Z Yu",
                "X Shi",
                "Z Zhang"
            ],
            "pub_year": "2023",
            "venue": "IEEE Access",
            "abstract": "Self-attention is a variation of the attention mechanism that  on Transformer's architecture  by adopting other network structures to  The data processing module in the prediction model"
        },
        "filled": false,
        "gsrank": 85,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10044658/",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:ftCrY8-FuCkJ:scholar.google.com/&output=cite&scirp=84&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D80%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ftCrY8-FuCkJ&ei=MmcBZ979OIWoy9YPtZ-o2Q4&json=",
        "num_citations": 5,
        "citedby_url": "/scholar?cites=3006299877046734974&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:ftCrY8-FuCkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6287639/6514899/10044658.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Dual attention transformer network for hyperspectral image classification",
            "author": [
                "Z Shu",
                "Y Wang",
                "Z Yu"
            ],
            "pub_year": "2024",
            "venue": "Engineering Applications of Artificial Intelligence",
            "abstract": "the attention mechanism not only effectively enhances model  1 illustrates the overall  architecture of the DATN model. This  our proposed SSHT module, thereby allowing the network to"
        },
        "filled": false,
        "gsrank": 86,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S095219762301535X",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:09dgsiVQFuYJ:scholar.google.com/&output=cite&scirp=85&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D80%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=09dgsiVQFuYJ&ei=MmcBZ979OIWoy9YPtZ-o2Q4&json=",
        "num_citations": 11,
        "citedby_url": "/scholar?cites=16579527201047762899&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:09dgsiVQFuYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S095219762301535X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multi-head self-attention via vision transformer for zero-shot learning",
            "author": [
                "F Alamri",
                "A Dutta"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv:2108.00045",
            "abstract": "In this work, we propose an attention-based model in the  an attention mechanism adapted  from Vision Transformer to  as attention mechanism is included in these models architecture."
        },
        "filled": false,
        "gsrank": 87,
        "pub_url": "https://arxiv.org/abs/2108.00045",
        "author_id": [
            "0OtURV8AAAAJ",
            "1aKTzmIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:gusJsOx1GBQJ:scholar.google.com/&output=cite&scirp=86&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D80%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gusJsOx1GBQJ&ei=MmcBZ979OIWoy9YPtZ-o2Q4&json=",
        "num_citations": 32,
        "citedby_url": "/scholar?cites=1448036939625786242&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:gusJsOx1GBQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2108.00045"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Spatial attention-based convolutional transformer for bearing remaining useful life prediction",
            "author": [
                "C Chen",
                "T Wang",
                "Y Liu",
                "L Cheng"
            ],
            "pub_year": "2022",
            "venue": "… Science and Technology",
            "abstract": "In the proposed architecture, a gated convolutional unit layer  of a CNN and spatial attention  mechanism is effective in  by a transformer network; (b) the transformer network is able to"
        },
        "filled": false,
        "gsrank": 88,
        "pub_url": "https://iopscience.iop.org/article/10.1088/1361-6501/ac7c5b/meta",
        "author_id": [
            "jNbNmU4AAAAJ",
            "",
            "dFgj9cwAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Z0PKjTtRjNoJ:scholar.google.com/&output=cite&scirp=87&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D80%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Z0PKjTtRjNoJ&ei=MmcBZ979OIWoy9YPtZ-o2Q4&json=",
        "num_citations": 21,
        "citedby_url": "/scholar?cites=15748051313231545191&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Z0PKjTtRjNoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://orca.cardiff.ac.uk/id/eprint/150821/1/Liu%20Y%20-%20Spatial%20-attention-based%20....pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Contextual transformer networks for visual recognition",
            "author": [
                "Y Li",
                "T Yao",
                "Y Pan",
                "T Mei"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on Pattern …",
            "abstract": "a novel Transformer-style module, ie, Contextual Transformer ( way to enhance  Transformer-style architecture by exploiting the  2 through attention mechanism [36]. In particular,"
        },
        "filled": false,
        "gsrank": 89,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9747984/",
        "author_id": [
            "7_1gqKgAAAAJ",
            "7Yc6yssAAAAJ",
            "2RxXFPoAAAAJ",
            "7Yq4wf4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:YLQfR2SB2roJ:scholar.google.com/&output=cite&scirp=88&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D80%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YLQfR2SB2roJ&ei=MmcBZ979OIWoy9YPtZ-o2Q4&json=",
        "num_citations": 493,
        "citedby_url": "/scholar?cites=13464216303667491936&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:YLQfR2SB2roJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/34/4359286/09747984.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Self-attention transformer-based architecture for remaining useful life estimation of complex machines",
            "author": [
                "A Wahid",
                "M Yahya",
                "JG Breslin",
                "MA Intizar"
            ],
            "pub_year": "2023",
            "venue": "Procedia Computer Science",
            "abstract": "• We propose a self-attention based transformer architecture  • We investigate the abstract  feature extraction layer module to  any prior domain knowledge into the attention mechanism."
        },
        "filled": false,
        "gsrank": 90,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1877050922023195",
        "author_id": [
            "32czwSMAAAAJ",
            "OXjalfIAAAAJ",
            "D8lvl64AAAAJ",
            "W91A6_sAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:N2TI1ha_MiIJ:scholar.google.com/&output=cite&scirp=89&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D80%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=N2TI1ha_MiIJ&ei=MmcBZ979OIWoy9YPtZ-o2Q4&json=",
        "num_citations": 8,
        "citedby_url": "/scholar?cites=2464242050938725431&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:N2TI1ha_MiIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1877050922023195/pdf?md5=a295e124211d906422fae0a6b29bd999&pid=1-s2.0-S1877050922023195-main.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "MAXFormer: Enhanced transformer for medical image segmentation with multi-attention and multi-scale features fusion",
            "author": [
                "Z Liang",
                "K Zhao",
                "G Liang",
                "S Li",
                "Y Wu",
                "Y Zhou"
            ],
            "pub_year": "2023",
            "venue": "Knowledge-Based Systems",
            "abstract": "The attention mechanism in the Transformer block is  model architecture. In the following  sections, we will introduce the Max Transformer block and Refined Fused Connection module"
        },
        "filled": false,
        "gsrank": 91,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0950705123007372",
        "author_id": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:D_eVWLYaRwMJ:scholar.google.com/&output=cite&scirp=90&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D90%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=D_eVWLYaRwMJ&ei=_mcBZ_zTBsDBy9YP683bgAE&json=",
        "num_citations": 12,
        "citedby_url": "/scholar?cites=236186875932833551&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:D_eVWLYaRwMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0950705123007372"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Dual-former: Hybrid self-attention transformer for efficient image restoration",
            "author": [
                "S Chen",
                "T Ye",
                "Y Liu",
                "E Chen"
            ],
            "pub_year": "2024",
            "venue": "Digital Signal Processing",
            "abstract": "of convolutions in an overall architecture. With convolution- For the Hybrid Transformer  Block in the latent layer, we  , we also design a local feature extraction module in parallel with"
        },
        "filled": false,
        "gsrank": 92,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1051200424001106",
        "author_id": [
            "EtljKSgAAAAJ",
            "1sGXZ-wAAAAJ",
            "9fjHp-EAAAAJ",
            "hWo1RTsAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:mzbBZxqXgzgJ:scholar.google.com/&output=cite&scirp=91&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D90%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mzbBZxqXgzgJ&ei=_mcBZ_zTBsDBy9YP683bgAE&json=",
        "num_citations": 19,
        "citedby_url": "/scholar?cites=4072264627738719899&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:mzbBZxqXgzgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1051200424001106"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "CAM-VT: A weakly supervised cervical cancer nest image identification approach using conjugated attention mechanism and visual transformer",
            "author": [
                "Z Fan",
                "X Wu",
                "C Li",
                "H Chen",
                "W Liu",
                "Y Zheng"
            ],
            "pub_year": "2023",
            "venue": "Computers in Biology …",
            "abstract": "designs an ensemble learning module to further improve the  architecture, mainly focusing  on three aspects: network depth,  It adds class tokens into ViT Transformer Layer and then"
        },
        "filled": false,
        "gsrank": 93,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0010482523005358",
        "author_id": [
            "",
            "",
            "",
            "4hL_2lkAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:rtRYaXdAPhoJ:scholar.google.com/&output=cite&scirp=92&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D90%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rtRYaXdAPhoJ&ei=_mcBZ_zTBsDBy9YP683bgAE&json=",
        "num_citations": 28,
        "citedby_url": "/scholar?cites=1891019775154902190&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:rtRYaXdAPhoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0010482523005358"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Cswin transformer: A general vision transformer backbone with cross-shaped windows",
            "author": [
                "X Dong",
                "J Bao",
                "D Chen",
                "W Zhang"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the …",
            "abstract": ", the Transformer architecture with full-attention mechanism [12]  operates directly upon V  and acts as a parallel module.  of l-th Transformer block or the precedent convolutional layer of"
        },
        "filled": false,
        "gsrank": 94,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2022/html/Dong_CSWin_Transformer_A_General_Vision_Transformer_Backbone_With_Cross-Shaped_Windows_CVPR_2022_paper.html",
        "author_id": [
            "FscToE0AAAAJ",
            "hjwvkYUAAAAJ",
            "sYKpKqEAAAAJ",
            "eTCfl6cAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:vKiHMRuvfz0J:scholar.google.com/&output=cite&scirp=93&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D90%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vKiHMRuvfz0J&ei=_mcBZ_zTBsDBy9YP683bgAE&json=",
        "num_citations": 1025,
        "citedby_url": "/scholar?cites=4431453089685809340&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:vKiHMRuvfz0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Dong_CSWin_Transformer_A_General_Vision_Transformer_Backbone_With_Cross-Shaped_Windows_CVPR_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "T-gsa: Transformer with gaussian-weighted self-attention for speech enhancement",
            "author": [
                "J Kim",
                "M El-Khamy",
                "J Lee"
            ],
            "pub_year": "2020",
            "venue": "ICASSP 2020-2020 IEEE …",
            "abstract": "The multi-head attention module in our proposed T-GSA is  We proposed a complex  Transformer architecture for speech  the decoder layer of the complex Transformer network. Hu"
        },
        "filled": false,
        "gsrank": 95,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9053591/",
        "author_id": [
            "",
            "qxPC268AAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:szeKoww4XHwJ:scholar.google.com/&output=cite&scirp=94&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D90%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=szeKoww4XHwJ&ei=_mcBZ_zTBsDBy9YP683bgAE&json=",
        "num_citations": 201,
        "citedby_url": "/scholar?cites=8961098985494951859&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:szeKoww4XHwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9040208/9052899/09053591.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Hyperspectral image classification based on graph transformer network and graph attention mechanism",
            "author": [
                "X Zhao",
                "J Niu",
                "C Liu",
                "Y Ding"
            ],
            "pub_year": "2022",
            "venue": "IEEE Geoscience and …",
            "abstract": "to the graph convolution network model. The graph transformer layer (GT-layer) is adopted   But in this paper, with the use of the GT-layer module, which is introduced in III -B, we do not"
        },
        "filled": false,
        "gsrank": 96,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9793640/",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:QT2qFQIX-iYJ:scholar.google.com/&output=cite&scirp=95&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D90%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QT2qFQIX-iYJ&ei=_mcBZ_zTBsDBy9YP683bgAE&json=",
        "num_citations": 14,
        "citedby_url": "/scholar?cites=2808582615339777345&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:QT2qFQIX-iYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/8859/4357975/09793640.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "An attention-based multiscale transformer network for remote sensing image change detection",
            "author": [
                "W Liu",
                "Y Lin",
                "W Liu",
                "Y Yu",
                "J Li"
            ],
            "pub_year": "2023",
            "venue": "ISPRS Journal of Photogrammetry and …",
            "abstract": "Siamese network based on the CNN-transformer architecture  and attention mechanism with  the transformer structure for  multi-layer perception (MLP) to obtain the channel attention M c"
        },
        "filled": false,
        "gsrank": 97,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S092427162300182X",
        "author_id": [
            "0-6tgtAAAAAJ",
            "",
            "",
            "sY1AFdAAAAAJ",
            "7hyYxRkAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:VCcKGEycPlwJ:scholar.google.com/&output=cite&scirp=96&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D90%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VCcKGEycPlwJ&ei=_mcBZ_zTBsDBy9YP683bgAE&json=",
        "num_citations": 43,
        "citedby_url": "/scholar?cites=6646921950680196948&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:VCcKGEycPlwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S092427162300182X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transfg: A transformer architecture for fine-grained recognition",
            "author": [
                "J He",
                "JN Chen",
                "S Liu",
                "A Kortylewski",
                "C Yang"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the …",
            "abstract": "tion Module that can be applied to most of the transformer ar its innate multi-head attention  mechanism. To fully exploit the  change the input to the last Transformer Layer. Suppose the"
        },
        "filled": false,
        "gsrank": 98,
        "pub_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19967",
        "author_id": [
            "NyTPm_zUV_kC",
            "yLYj88sAAAAJ",
            "MsUzsOUAAAAJ",
            "tRLUOBIAAAAJ",
            "wU90N1YAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:v8MQEgWkVwgJ:scholar.google.com/&output=cite&scirp=97&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D90%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=v8MQEgWkVwgJ&ei=_mcBZ_zTBsDBy9YP683bgAE&json=",
        "num_citations": 410,
        "citedby_url": "/scholar?cites=601129416962130879&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:v8MQEgWkVwgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19967/19726"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "CAT-Net: A cross-slice attention transformer model for prostate zonal segmentation in MRI",
            "author": [
                "ALY Hung",
                "H Zheng",
                "Q Miao",
                "SS Raman"
            ],
            "pub_year": "2022",
            "venue": "IEEE transactions on …",
            "abstract": "attention mechanism, which we use in a Transformer module  that can be incorporated within  any network architecture with  attention between slices at the bottom layer of a U-Net with a"
        },
        "filled": false,
        "gsrank": 99,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9910184/",
        "author_id": [
            "xEFJjVoAAAAJ",
            "LZM6kKcAAAAJ",
            "",
            "ZOZnvrYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:5klo4fXkE6EJ:scholar.google.com/&output=cite&scirp=98&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D90%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5klo4fXkE6EJ&ei=_mcBZ_zTBsDBy9YP683bgAE&json=",
        "num_citations": 31,
        "citedby_url": "/scholar?cites=11606872409363728870&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:5klo4fXkE6EJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/42/4359023/09910184.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Star-transformer: a spatio-temporal cross attention transformer for human action recognition",
            "author": [
                "D Ahn",
                "S Kim",
                "H Hong",
                "BC Ko"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the IEEE …",
            "abstract": "We therefore propose a STAR attention mechanism that can  -recognition model based on  the STAR-transformer module.  each layer of the transformer model and computes the token"
        },
        "filled": false,
        "gsrank": 100,
        "pub_url": "https://openaccess.thecvf.com/content/WACV2023/html/Ahn_STAR-Transformer_A_Spatio-Temporal_Cross_Attention_Transformer_for_Human_Action_Recognition_WACV_2023_paper.html",
        "author_id": [
            "SyZb3N8AAAAJ",
            "pUHZ-IcAAAAJ",
            "YJgmuz8AAAAJ",
            "o8ToM1QAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:_lSkSZncQS8J:scholar.google.com/&output=cite&scirp=99&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D90%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_lSkSZncQS8J&ei=_mcBZ_zTBsDBy9YP683bgAE&json=",
        "num_citations": 120,
        "citedby_url": "/scholar?cites=3405245344192419070&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:_lSkSZncQS8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/WACV2023/papers/Ahn_STAR-Transformer_A_Spatio-Temporal_Cross_Attention_Transformer_for_Human_Action_Recognition_WACV_2023_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Sam: Self attention mechanism for scene text recognition based on swin transformer",
            "author": [
                "X Shuai",
                "X Wang",
                "W Wang",
                "X Yuan",
                "X Xu"
            ],
            "pub_year": "2022",
            "venue": "International Conference on …",
            "abstract": ", the attention mechanism is usually combined with RNN structures as a module to predict  the  Eventually, the final results are obtained by connecting the outputs and applying a layer"
        },
        "filled": false,
        "gsrank": 101,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-98358-1_35",
        "author_id": [
            "",
            "",
            "",
            "Cp5JZsoAAAAJ",
            "DtuoAWIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:adopxQyZGNcJ:scholar.google.com/&output=cite&scirp=100&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D100%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=adopxQyZGNcJ&ei=ymgBZ_C2HdKAy9YPm-_p6Ao&json=",
        "num_citations": 5,
        "citedby_url": "/scholar?cites=15499306397722073705&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:adopxQyZGNcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Surface vision transformers: Attention-based modelling applied to cortical analysis",
            "author": [
                "S Dahan",
                "A Fawaz",
                "LZJ Williams"
            ],
            "pub_year": "2022",
            "venue": "… on Medical Imaging …",
            "abstract": "architecture is made of L transformer blocks, each composed of a multi-head self-attention  layer  Corticalflow: A diffeomorphic mesh transformer network for cortical surface reconstruction"
        },
        "filled": false,
        "gsrank": 102,
        "pub_url": "https://proceedings.mlr.press/v172/dahan22a.html",
        "author_id": [
            "JcbJqX0AAAAJ",
            "",
            "iz-TKOgAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:pPzylR_0MaQJ:scholar.google.com/&output=cite&scirp=101&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D100%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=pPzylR_0MaQJ&ei=ymgBZ_C2HdKAy9YPm-_p6Ao&json=",
        "num_citations": 20,
        "citedby_url": "/scholar?cites=11831506112575896740&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:pPzylR_0MaQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.mlr.press/v172/dahan22a/dahan22a.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer based on channel-spatial attention for accurate classification of scenes in remote sensing image",
            "author": [
                "J Guo",
                "N Jia",
                "J Bai"
            ],
            "pub_year": "2022",
            "venue": "Scientific Reports",
            "abstract": "First, transformer is a new encoder-decoder architecture that  Third, the self-attention  mechanism layer in transformer will  attention mechanism called the bottleneck attention module ("
        },
        "filled": false,
        "gsrank": 103,
        "pub_url": "https://www.nature.com/articles/s41598-022-19831-z",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:mCxuGR3uFKcJ:scholar.google.com/&output=cite&scirp=102&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D100%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mCxuGR3uFKcJ&ei=ymgBZ_C2HdKAy9YPm-_p6Ao&json=",
        "num_citations": 29,
        "citedby_url": "/scholar?cites=12039509512616291480&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:mCxuGR3uFKcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.nature.com/articles/s41598-022-19831-z.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Quantifying attention flow in transformers",
            "author": [
                "S Abnar",
                "W Zuidema"
            ],
            "pub_year": "2020",
            "venue": "arXiv preprint arXiv:2005.00928",
            "abstract": "Given the attention module with residual connection, we  , we average the attention at each  layer over all heads.  in any task or architecture that uses self-attention. We should note that"
        },
        "filled": false,
        "gsrank": 104,
        "pub_url": "https://arxiv.org/abs/2005.00928",
        "author_id": [
            "jbxwjgMAAAAJ",
            "MBkG_FYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:2De2Q0AQoxYJ:scholar.google.com/&output=cite&scirp=103&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D100%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2De2Q0AQoxYJ&ei=ymgBZ_C2HdKAy9YPm-_p6Ao&json=",
        "num_citations": 809,
        "citedby_url": "/scholar?cites=1631165358238218200&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:2De2Q0AQoxYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2005.00928"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Adder attention for vision transformer",
            "author": [
                "H Shu",
                "J Wang",
                "H Chen",
                "L Li"
            ],
            "pub_year": "2021",
            "venue": "Advances in Neural …",
            "abstract": "network (AdderNet). We first theoretically analyze the  Transformer— which implement the  multi-head attention module  each layer, since LN is widely used in the transformer architecture"
        },
        "filled": false,
        "gsrank": 105,
        "pub_url": "https://proceedings.neurips.cc/paper/2021/hash/a57e8915461b83adefb011530b711704-Abstract.html",
        "author_id": [
            "h-0HS0YAAAAJ",
            "QjVR3UUAAAAJ",
            "wZ9N88gAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:L-jWQVnMH0UJ:scholar.google.com/&output=cite&scirp=104&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D100%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=L-jWQVnMH0UJ&ei=ymgBZ_C2HdKAy9YPm-_p6Ao&json=",
        "num_citations": 24,
        "citedby_url": "/scholar?cites=4980924396623816751&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:L-jWQVnMH0UJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/a57e8915461b83adefb011530b711704-Paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "On exploring attention-based explanation for transformer models in text classification",
            "author": [
                "S Liu",
                "F Le",
                "S Chakraborty"
            ],
            "pub_year": "2021",
            "venue": "2021 IEEE International …",
            "abstract": "behind this architecture is the attention mechanism, ie, the  across the attention heads  at each layer. In reality, the  Attention-based methods try to understand the network logic"
        },
        "filled": false,
        "gsrank": 106,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9671639/",
        "author_id": [
            "REzrIucAAAAJ",
            "",
            "UIM7nGwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:T8AQFoyFpNMJ:scholar.google.com/&output=cite&scirp=105&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D100%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=T8AQFoyFpNMJ&ei=ymgBZ_C2HdKAy9YPm-_p6Ao&json=",
        "num_citations": 20,
        "citedby_url": "/scholar?cites=15250461074895454287&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:T8AQFoyFpNMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9671263/9671273/09671639.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition",
            "author": [
                "L Dong",
                "S Xu",
                "B Xu"
            ],
            "pub_year": "2018",
            "venue": "2018 IEEE international conference on …",
            "abstract": "we find our proposed 2D-Attention mechanism achieves better  we present the model  architecture of the Speech-Transformer,  we apply 2D-Attention module to our big model, it shows"
        },
        "filled": false,
        "gsrank": 107,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/8462506/",
        "author_id": [
            "CbUs-aAAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:N2oJJpK1vlwJ:scholar.google.com/&output=cite&scirp=106&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D100%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=N2oJJpK1vlwJ&ei=ymgBZ_C2HdKAy9YPm-_p6Ao&json=",
        "num_citations": 1214,
        "citedby_url": "/scholar?cites=6682978536372398647&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:N2oJJpK1vlwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/8450881/8461260/08462506.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "FAMOUS: Flexible Accelerator for the Attention Mechanism of Transformer on UltraScale+ FPGAs",
            "author": [
                "E Kabir",
                "MA Kabir",
                "ARJ Downey",
                "JD Bakos"
            ],
            "pub_year": "2024",
            "venue": "arXiv preprint arXiv …",
            "abstract": "was designed for the multi-head attention (MHA) layer of a transformer neural network (TNN)   In this paper, the architecture supports only the attention module, but it will be expanded to"
        },
        "filled": false,
        "gsrank": 108,
        "pub_url": "https://arxiv.org/abs/2409.14023",
        "author_id": [
            "c1peOmYAAAAJ",
            "-GtU-QMAAAAJ",
            "LgHSdLEAAAAJ",
            "pXa_QqEAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:x2WkTFfkDuUJ:scholar.google.com/&output=cite&scirp=107&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D100%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=x2WkTFfkDuUJ&ei=ymgBZ_C2HdKAy9YPm-_p6Ao&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:x2WkTFfkDuUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2409.14023"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multiscaled multi-head attention-based video transformer network for hand gesture recognition",
            "author": [
                "M Garg",
                "D Ghosh",
                "PM Pradhan"
            ],
            "pub_year": "2023",
            "venue": "IEEE Signal Processing …",
            "abstract": "The attention mechanism decides and identifies the most  One such model which is based  on the attention mechanism  hand gestures based on transformer architecture has been pro"
        },
        "filled": false,
        "gsrank": 109,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10035507/",
        "author_id": [
            "hlPKfaQAAAAJ",
            "",
            "_eIpqasAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:xZ9gujoe4SsJ:scholar.google.com/&output=cite&scirp=108&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D100%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xZ9gujoe4SsJ&ei=ymgBZ_C2HdKAy9YPm-_p6Ao&json=",
        "num_citations": 13,
        "citedby_url": "/scholar?cites=3161841650974629829&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:xZ9gujoe4SsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/97/4358004/10035507.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Deep learning-based identification of maize leaf diseases is improved by an attention mechanism: Self-attention",
            "author": [
                "X Qian",
                "C Zhang",
                "L Chen",
                "K Li"
            ],
            "pub_year": "2022",
            "venue": "Frontiers in Plant Science",
            "abstract": "In this study, we found that transformer and self-attention  The standard model architecture.  Stage 1 extracts information  module of the whole network, and the essential part of it is"
        },
        "filled": false,
        "gsrank": 110,
        "pub_url": "https://www.frontiersin.org/articles/10.3389/fpls.2022.864486/full",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:DzhLUPKNSBUJ:scholar.google.com/&output=cite&scirp=109&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D100%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DzhLUPKNSBUJ&ei=ymgBZ_C2HdKAy9YPm-_p6Ao&json=",
        "num_citations": 36,
        "citedby_url": "/scholar?cites=1533631744988362767&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:DzhLUPKNSBUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2022.864486/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A novel transformer network with a CNN-enhanced cross-attention mechanism for hyperspectral image classification",
            "author": [
                "X Wang",
                "L Sun",
                "C Lu",
                "B Li"
            ],
            "pub_year": "2024",
            "venue": "Remote Sensing",
            "abstract": "from the previous module. Firstly, we apply a 2D convolutional layer with kernel sizes of ( 3   (1) Batch Size: Due to our observation that the performance of the transformer architecture"
        },
        "filled": false,
        "gsrank": 111,
        "pub_url": "https://www.mdpi.com/2072-4292/16/7/1180",
        "author_id": [
            "",
            "9_pbp3kAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:zOJFdb9PrXcJ:scholar.google.com/&output=cite&scirp=110&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D110%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zOJFdb9PrXcJ&ei=lmkBZ7ycE7CDy9YPgrj5gAQ&json=",
        "num_citations": 7,
        "citedby_url": "/scholar?cites=8623636545209230028&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:zOJFdb9PrXcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2072-4292/16/7/1180/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Introducing attention mechanism for eeg signals: Emotion recognition with vision transformers",
            "author": [
                "A Arjun",
                "AS Rajpoot"
            ],
            "pub_year": "2021",
            "venue": "2021 43rd annual …",
            "abstract": "Representations from Transformers (BERT) [14] architecture,  Then a SoftMax layer followed  by an ArgMax layer is applied  multi-headed attention-based mechanism, the model is able"
        },
        "filled": false,
        "gsrank": 112,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9629837/",
        "author_id": [
            "qQnyv64AAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:CXGkEzH-f-UJ:scholar.google.com/&output=cite&scirp=111&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D110%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CXGkEzH-f-UJ&ei=lmkBZ7ycE7CDy9YPgrj5gAQ&json=",
        "num_citations": 34,
        "citedby_url": "/scholar?cites=16537215843464147209&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:CXGkEzH-f-UJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9629355/9629471/09629837.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer-based multivariate time series anomaly detection using inter-variable attention mechanism",
            "author": [
                "H Kang",
                "P Kang"
            ],
            "pub_year": "2024",
            "venue": "Knowledge-Based Systems",
            "abstract": "bring forth an anomaly interpretation module to shed light on  employing the Transformer  [21] architecture. Although  × d model is the hidden representation of the l th layer. Lastly"
        },
        "filled": false,
        "gsrank": 113,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0950705124001424",
        "author_id": [
            "czpq99MAAAAJ",
            "I2pcWZIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:6x1BRIA3gBwJ:scholar.google.com/&output=cite&scirp=112&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D110%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6x1BRIA3gBwJ&ei=lmkBZ7ycE7CDy9YPgrj5gAQ&json=",
        "num_citations": 10,
        "citedby_url": "/scholar?cites=2053702454121405931&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:6x1BRIA3gBwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0950705124001424"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "… -YOLOv5: An automated damage detection model based on DenseNet and Swin-Transformer prediction head-enabled YOLOv5 with attention mechanism",
            "author": [
                "AM Roy",
                "J Bhaduri"
            ],
            "pub_year": "2023",
            "venue": "Advanced Engineering Informatics",
            "abstract": "Along a similar line, deep neural network (DNN) architecture has  5-(a), the network structure  of the CBAM module consists of  attention map from multi-layer perceptron (MLP). The CAM"
        },
        "filled": false,
        "gsrank": 114,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1474034623001350",
        "author_id": [
            "xB6ltXYAAAAJ",
            "6we-a3oAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:X2RzK7HVFawJ:scholar.google.com/&output=cite&scirp=113&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D110%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=X2RzK7HVFawJ&ei=lmkBZ7ycE7CDy9YPgrj5gAQ&json=",
        "num_citations": 118,
        "citedby_url": "/scholar?cites=12400052105949439071&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:X2RzK7HVFawJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1474034623001350"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Boxer: Box-attention for 2d and 3d transformers",
            "author": [
                "DK Nguyen",
                "J Ju",
                "O Booij"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the …",
            "abstract": "a simple attention mechanism, we call Box-Attention. It  inductive bias in the network  architecture delivers a strong  inductive bias in the transformer’s attention module leads to a better"
        },
        "filled": false,
        "gsrank": 115,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2022/html/Nguyen_BoxeR_Box-Attention_for_2D_and_3D_Transformers_CVPR_2022_paper.html",
        "author_id": [
            "welhhBIAAAAJ",
            "NwbkoewAAAAJ",
            "INz-U3AAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:XWe8_cdzDRkJ:scholar.google.com/&output=cite&scirp=114&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D110%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XWe8_cdzDRkJ&ei=lmkBZ7ycE7CDy9YPgrj5gAQ&json=",
        "num_citations": 40,
        "citedby_url": "/scholar?cites=1805226328438105949&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:XWe8_cdzDRkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Nguyen_BoxeR_Box-Attention_for_2D_and_3D_Transformers_CVPR_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Cat: Cross attention in vision transformer",
            "author": [
                "H Lin",
                "X Cheng",
                "X Wu",
                "D Shen"
            ],
            "pub_year": "2022",
            "venue": "2022 IEEE international …",
            "abstract": "attention mechanism in Transformer termed Cross Attention,  1) and other global attentionbased  methods. The FLOPscpga is  In all architecture we make, the architecture with the best"
        },
        "filled": false,
        "gsrank": 116,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9859720/",
        "author_id": [
            "kFn8mf4AAAAJ",
            "",
            "FMkeZOUAAAAJ",
            "4kh7ApkAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:frOz2U-0TFkJ:scholar.google.com/&output=cite&scirp=115&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D110%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=frOz2U-0TFkJ&ei=lmkBZ7ycE7CDy9YPgrj5gAQ&json=",
        "num_citations": 126,
        "citedby_url": "/scholar?cites=6434716222653444990&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:frOz2U-0TFkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9859562/9858923/09859720.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multi-Head-Self-Attention based YOLOv5X-transformer for multi-scale object detection",
            "author": [
                "P Vasanthi",
                "L Mohan"
            ],
            "pub_year": "2024",
            "venue": "Multimedia Tools and Applications",
            "abstract": "excitation attention mechanism in the YOLOv3 model. In this  last C3 layer is replaced with  a transformer block followed by  -Transformer model used a transformer architecture, it used"
        },
        "filled": false,
        "gsrank": 117,
        "pub_url": "https://link.springer.com/article/10.1007/s11042-023-15773-4",
        "author_id": [
            "z6hQDvkAAAAJ",
            "f3cOyN8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:fvcUniNtOIkJ:scholar.google.com/&output=cite&scirp=116&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D110%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fvcUniNtOIkJ&ei=lmkBZ7ycE7CDy9YPgrj5gAQ&json=",
        "num_citations": 11,
        "citedby_url": "/scholar?cites=9887772981635381118&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:fvcUniNtOIkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s11042-023-15773-4.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multimodal channel-wise attention transformer inspired by multisensory integration mechanisms of the brain",
            "author": [
                "Q Shi",
                "J Fan",
                "Z Wang",
                "Z Zhang"
            ],
            "pub_year": "2022",
            "venue": "Pattern Recognition",
            "abstract": "with the top-down attention mechanism in the brain, we  MCAT architecture, which consists  of N layers; each layer  and layer normalization concluded in the multi-head attention module,"
        },
        "filled": false,
        "gsrank": 118,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0031320322003181",
        "author_id": [
            "Np7YBkYAAAAJ",
            "AfK4UcUAAAAJ",
            "",
            "qxWfV6cAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:KsNuM587sxEJ:scholar.google.com/&output=cite&scirp=117&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D110%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KsNuM587sxEJ&ei=lmkBZ7ycE7CDy9YPgrj5gAQ&json=",
        "num_citations": 18,
        "citedby_url": "/scholar?cites=1275428674424718122&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:KsNuM587sxEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0031320322003181"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Miti-detr: Object detection based on transformers with mitigatory self-attention convergence",
            "author": [
                "W Ma",
                "T Zhang",
                "G Wang"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv:2112.13310",
            "abstract": "to network depth, we propose a transformer architecture with  composite module of multi-head  self-attention network and  problem by attention mechanism and transformer network itself."
        },
        "filled": false,
        "gsrank": 119,
        "pub_url": "https://arxiv.org/abs/2112.13310",
        "author_id": [
            "u0TjHR8AAAAJ",
            "7Ooy4ZEAAAAJ",
            "I_5aoAwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:8-3_dm9yc_UJ:scholar.google.com/&output=cite&scirp=118&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D110%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8-3_dm9yc_UJ&ei=lmkBZ7ycE7CDy9YPgrj5gAQ&json=",
        "num_citations": 15,
        "citedby_url": "/scholar?cites=17686605984677490163&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:8-3_dm9yc_UJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2112.13310"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Twins transformer: Cross-attention based two-branch transformer network for rotating bearing fault diagnosis",
            "author": [
                "J Li",
                "Y Bao",
                "WX Liu",
                "PX Ji",
                "LK Wang",
                "Z Wang"
            ],
            "pub_year": "2023",
            "venue": "Measurement",
            "abstract": "The Transformer architecture is already working well in various  relies on the attention  mechanism to build the model. Therefore,  The first sub-layer is the two-branches Twins Attention"
        },
        "filled": false,
        "gsrank": 120,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0263224123012514",
        "author_id": [
            "",
            "nzTav1wAAAAJ",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:WkoIpnpWkl4J:scholar.google.com/&output=cite&scirp=119&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D110%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WkoIpnpWkl4J&ei=lmkBZ7ycE7CDy9YPgrj5gAQ&json=",
        "num_citations": 10,
        "citedby_url": "/scholar?cites=6814604270936541786&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:WkoIpnpWkl4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0263224123012514"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Face-based age estimation using improved Swin Transformer with attention-based convolution",
            "author": [
                "C Shi",
                "S Zhao",
                "K Zhang",
                "Y Wang",
                "L Liang"
            ],
            "pub_year": "2023",
            "venue": "Frontiers in Neuroscience",
            "abstract": "We also reviewed the attention mechanism and Transformer,  The architecture of  attention-based convolution is shown in  layer X was used as the input of the attention mechanism."
        },
        "filled": false,
        "gsrank": 121,
        "pub_url": "https://www.frontiersin.org/articles/10.3389/fnins.2023.1136934/full",
        "author_id": [
            "xpG6UVcAAAAJ",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:zWvSPKBFhTkJ:scholar.google.com/&output=cite&scirp=120&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D120%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zWvSPKBFhTkJ&ei=YWoBZ8zZErCDy9YPgrj5gAQ&json=",
        "num_citations": 7,
        "citedby_url": "/scholar?cites=4144795586581916621&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:zWvSPKBFhTkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.frontiersin.org/articles/10.3389/fnins.2023.1136934/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention over self-attention: Intention-aware re-ranking with dynamic transformer encoders for recommendation",
            "author": [
                "Z Lin",
                "S Zang",
                "R Wang",
                "Z Sun"
            ],
            "pub_year": "2022",
            "venue": "… on Knowledge and …",
            "abstract": "A fundamental assumption of the transformer architecture is  merit of our dynamic self-attention  module. By assembling the  then the co-attention mechanism assigns higher matching"
        },
        "filled": false,
        "gsrank": 122,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9906456/",
        "author_id": [
            "i04oBhIAAAAJ",
            "uRb-bJoAAAAJ",
            "JEVpgE8AAAAJ",
            "kJy0fd8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:_8DKGf0ks9UJ:scholar.google.com/&output=cite&scirp=121&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D120%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_8DKGf0ks9UJ&ei=YWoBZ8zZErCDy9YPgrj5gAQ&json=",
        "num_citations": 11,
        "citedby_url": "/scholar?cites=15398692220387901695&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:_8DKGf0ks9UJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/69/4358933/09906456.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "An integrated multi-head dual sparse self-attention network for remaining useful life prediction",
            "author": [
                "J Zhang",
                "X Li",
                "J Tian",
                "H Luo",
                "S Yin"
            ],
            "pub_year": "2023",
            "venue": "Reliability Engineering & System Safety",
            "abstract": "As for the single-head attention mechanism, it is assumed that  The Transformer is a multi-layer  encoder–decoder model,  designs the encoder module of the Transformer model. After"
        },
        "filled": false,
        "gsrank": 123,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S095183202300011X",
        "author_id": [
            "M78xFhcAAAAJ",
            "HBxeAp8AAAAJ",
            "JlCKpx8AAAAJ",
            "DNDtXD0AAAAJ",
            "v-Xddq0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Jq777VAiufAJ:scholar.google.com/&output=cite&scirp=122&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D120%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Jq777VAiufAJ&ei=YWoBZ8zZErCDy9YPgrj5gAQ&json=",
        "num_citations": 86,
        "citedby_url": "/scholar?cites=17345933170779598374&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Jq777VAiufAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S095183202300011X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Fine-grained citation count prediction via a transformer-based model with among-attention mechanism",
            "author": [
                "S Huang",
                "Y Huang",
                "Y Bu",
                "W Lu",
                "J Qian"
            ],
            "pub_year": "2022",
            "venue": "Information Processing & …",
            "abstract": "architecture of the transformer.  After that, the Scaled Dot-Product Attention module calculated  the inner  blocks of the encoder layer of the transformers. Actually, it can also be used"
        },
        "filled": false,
        "gsrank": 124,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0306457321002776",
        "author_id": [
            "",
            "9pic8GcAAAAJ",
            "bSUm6ikAAAAJ",
            "mRdnCQ4AAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:lNpxwYZY9b0J:scholar.google.com/&output=cite&scirp=123&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D120%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lNpxwYZY9b0J&ei=YWoBZ8zZErCDy9YPgrj5gAQ&json=",
        "num_citations": 20,
        "citedby_url": "/scholar?cites=13687943978256816788&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:lNpxwYZY9b0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0306457321002776"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Patchformer: An efficient point transformer with patch attention",
            "author": [
                "C Zhang",
                "H Wan",
                "X Shen",
                "Z Wu"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the IEEE …",
            "abstract": "Benefitting from SA module, Transformer is capable of modeling  complexity of the attention  mechanism and limitations, we first  : replace PAT with EdgeConv layer in our architecture."
        },
        "filled": false,
        "gsrank": 125,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_PatchFormer_An_Efficient_Point_Transformer_With_Patch_Attention_CVPR_2022_paper.html",
        "author_id": [
            "_aXGI4kAAAAJ",
            "a87nZj4AAAAJ",
            "vimZwbIAAAAJ",
            "4SDMVSUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:MoTfg1lWzPcJ:scholar.google.com/&output=cite&scirp=124&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D120%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MoTfg1lWzPcJ&ei=YWoBZ8zZErCDy9YPgrj5gAQ&json=",
        "num_citations": 76,
        "citedby_url": "/scholar?cites=17855741565081715762&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:MoTfg1lWzPcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_PatchFormer_An_Efficient_Point_Transformer_With_Patch_Attention_CVPR_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Swin transformer assisted prior attention network for medical image segmentation",
            "author": [
                "Z Liao",
                "K Xu",
                "N Fan"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the 8th International Conference on …",
            "abstract": "to apply pure Transformer-based U-shaped architecture to  Swin Transformer module  is consisted of LayerNorm layer,  details by applying window-based attention mechanism."
        },
        "filled": false,
        "gsrank": 126,
        "pub_url": "https://dl.acm.org/doi/abs/10.1145/3532213.3532287",
        "author_id": [
            "",
            "",
            "8wUGxGoAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:ucvs0DStfrMJ:scholar.google.com/&output=cite&scirp=125&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D120%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ucvs0DStfrMJ&ei=YWoBZ8zZErCDy9YPgrj5gAQ&json=",
        "num_citations": 18,
        "citedby_url": "/scholar?cites=12933965622209727417&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:ucvs0DStfrMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://dl.acm.org/doi/pdf/10.1145/3532213.3532287"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "LPT-Net: A Line-Pad Transformer Network for efficiency coal gangue segmentation with linear multi-head self-attention mechanism",
            "author": [
                "T Ye",
                "H Chen",
                "H Ren",
                "Z Zheng",
                "Z Zhao"
            ],
            "pub_year": "2024",
            "venue": "Measurement",
            "abstract": "of the second layer and MSA1 for MSA2 of the fourth layer.  LPT-Net, based on the Transformer  architecture, could benefit  We build L-MSA module based on linear attention mechanism"
        },
        "filled": false,
        "gsrank": 127,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S026322412301607X",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:DtLmkR73mJgJ:scholar.google.com/&output=cite&scirp=126&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D120%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DtLmkR73mJgJ&ei=YWoBZ8zZErCDy9YPgrj5gAQ&json=",
        "num_citations": 2,
        "citedby_url": "/scholar?cites=10995810200893968910&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:DtLmkR73mJgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S026322412301607X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Couplformer: Rethinking vision transformer with coupling attention",
            "author": [
                "H Lan",
                "X Wang",
                "H Shen",
                "P Liang"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the …",
            "abstract": "Recent efforts have shown that Transformer and attention-based models have become   model’s overall architecture to reach a better result. We believe our coupled attention mechanism"
        },
        "filled": false,
        "gsrank": 128,
        "pub_url": "https://openaccess.thecvf.com/content/WACV2023/html/Lan_Couplformer_Rethinking_Vision_Transformer_With_Coupling_Attention_WACV_2023_paper.html",
        "author_id": [
            "",
            "hqUb6B4AAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:cQE05pmiZJMJ:scholar.google.com/&output=cite&scirp=127&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D120%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cQE05pmiZJMJ&ei=YWoBZ8zZErCDy9YPgrj5gAQ&json=",
        "num_citations": 5,
        "citedby_url": "/scholar?cites=10620792603122336113&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:cQE05pmiZJMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/WACV2023/papers/Lan_Couplformer_Rethinking_Vision_Transformer_With_Coupling_Attention_WACV_2023_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Knowing what it is: semantic-enhanced dual attention transformer",
            "author": [
                "Y Ma",
                "J Ji",
                "X Sun",
                "Y Zhou",
                "Y Wu"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on …",
            "abstract": "a novel attention module termed Channel-wise Attention  In contrast, we propose a novel  attention mechanism to  -and interlayer global representation in transformer network,” in"
        },
        "filled": false,
        "gsrank": 129,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9749944/",
        "author_id": [
            "KIDY5pUAAAAJ",
            "xp_rICcAAAAJ",
            "KPMK3B4AAAAJ",
            "w3_2ep0AAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:mYzqSfl2FbgJ:scholar.google.com/&output=cite&scirp=128&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D120%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mYzqSfl2FbgJ&ei=YWoBZ8zZErCDy9YPgrj5gAQ&json=",
        "num_citations": 20,
        "citedby_url": "/scholar?cites=13264639090548706457&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:mYzqSfl2FbgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6046/4456689/09749944.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Exploiting multi-scale parallel self-attention and local variation via dual-branch transformer-CNN structure for face super-resolution",
            "author": [
                "J Shi",
                "Y Wang",
                "Z Yu",
                "G Li",
                "X Hong"
            ],
            "pub_year": "2023",
            "venue": "IEEE Transactions on …",
            "abstract": "dual-branch module which consists of Transformer and CNN  blocks and introduced  attention mechanism to refine the  most important module in the whole network architecture. It"
        },
        "filled": false,
        "gsrank": 130,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10207832/",
        "author_id": [
            "N2ftCz4AAAAJ",
            "",
            "ziHejLwAAAAJ",
            "",
            "x3X-qysAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:R5g3f_RHnZgJ:scholar.google.com/&output=cite&scirp=129&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D120%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=R5g3f_RHnZgJ&ei=YWoBZ8zZErCDy9YPgrj5gAQ&json=",
        "num_citations": 27,
        "citedby_url": "/scholar?cites=10997024980540561479&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:R5g3f_RHnZgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6046/4456689/10207832.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Remote sensing image change detection based on deep multi-scale multi-attention Siamese transformer network",
            "author": [
                "M Zhang",
                "Z Liu",
                "J Feng",
                "L Liu",
                "L Jiao"
            ],
            "pub_year": "2023",
            "venue": "Remote Sensing",
            "abstract": "change information, the attention mechanism is added in  attention module combining a  convolution and self-attention module  and the self-attention together in the same architecture. It"
        },
        "filled": false,
        "gsrank": 131,
        "pub_url": "https://www.mdpi.com/2072-4292/15/3/842",
        "author_id": [
            "",
            "",
            "G0DT_oYAAAAJ",
            "",
            "FZbrL2YAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:NghKZnmOT28J:scholar.google.com/&output=cite&scirp=130&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D130%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NghKZnmOT28J&ei=K2sBZ7_DNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 33,
        "citedby_url": "/scholar?cites=8020786113428457526&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:NghKZnmOT28J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2072-4292/15/3/842/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Local multi-head channel self-attention for facial expression recognition",
            "author": [
                "R Pecoraro",
                "V Basile",
                "V Bono"
            ],
            "pub_year": "2022",
            "venue": "Information",
            "abstract": "multi-Head Channel self-attention, a novel self-attention module  alone spatial self-attention  architecture in which the transformer’ to improve the attention mechanism of the Transformer."
        },
        "filled": false,
        "gsrank": 132,
        "pub_url": "https://www.mdpi.com/2078-2489/13/9/419",
        "author_id": [
            "",
            "Q03ktAIAAAAJ",
            "e13kYqoAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:oRc5n3hEp8UJ:scholar.google.com/&output=cite&scirp=131&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D130%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oRc5n3hEp8UJ&ei=K2sBZ7_DNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 68,
        "citedby_url": "/scholar?cites=14242427631440566177&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:oRc5n3hEp8UJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2078-2489/13/9/419/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "ELSA: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks",
            "author": [
                "TJ Ham",
                "Y Lee",
                "SH Seo",
                "S Kim",
                "H Choi"
            ],
            "pub_year": "2021",
            "venue": "… Architecture (ISCA)",
            "abstract": "of the attention mechanism is the self-attention mechanism,  layer in Transformer-style models  with the self-attention [48], [ to the attention computation module. The candidate selection"
        },
        "filled": false,
        "gsrank": 133,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9499860/",
        "author_id": [
            "q4qNHBsAAAAJ",
            "W_joQJkAAAAJ",
            "y91TSdYAAAAJ",
            "4a4nuY8AAAAJ",
            "pBjWEm8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:vgUrjY0B3WoJ:scholar.google.com/&output=cite&scirp=132&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D130%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vgUrjY0B3WoJ&ei=K2sBZ7_DNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 114,
        "citedby_url": "/scholar?cites=7700312645343839678&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:vgUrjY0B3WoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9499716/9499722/09499860.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multi-gate attention network for image captioning",
            "author": [
                "W Jiang",
                "X Li",
                "H Hu",
                "Q Lu",
                "B Liu"
            ],
            "pub_year": "2021",
            "venue": "IEEE Access",
            "abstract": "-layernorm transformer to simplify the transformer architecture  network, [1] first integrates  the spatial attention mechanism  a SG module to consider the intra-object attention distribution"
        },
        "filled": false,
        "gsrank": 134,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9382255/",
        "author_id": [
            "",
            "",
            "gieROVcAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:6MFTic4KUhkJ:scholar.google.com/&output=cite&scirp=133&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D130%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6MFTic4KUhkJ&ei=K2sBZ7_DNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 20,
        "citedby_url": "/scholar?cites=1824532681221980648&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:6MFTic4KUhkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6287639/6514899/09382255.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Easy attention: A simple self-attention mechanism for transformers",
            "author": [
                "M Sanchis-Agudo",
                "Y Wang",
                "K Duraisamy"
            ],
            "pub_year": "2023",
            "venue": "arXiv preprint arXiv …",
            "abstract": "systems, we propose a novel attention mechanism called easy attention. Due to the fact that   sparse multi-head easy attention as the easy attention module on the proposed architecture,"
        },
        "filled": false,
        "gsrank": 135,
        "pub_url": "https://arxiv.org/abs/2308.12874",
        "author_id": [
            "egkPCv4AAAAJ",
            "n0DYwYgAAAAJ",
            "8F2oaQwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:EbME9ZWDKacJ:scholar.google.com/&output=cite&scirp=134&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D130%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EbME9ZWDKacJ&ei=K2sBZ7_DNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 7,
        "citedby_url": "/scholar?cites=12045303358463193873&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:EbME9ZWDKacJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2308.12874"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Vehicle trajectory prediction based on intention-aware non-autoregressive transformer with multi-attention learning for Internet of Vehicles",
            "author": [
                "X Chen",
                "H Zhang",
                "F Zhao",
                "Y Cai"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on …",
            "abstract": ", we propose a multiple attention mechanism capturing the  overall network architecture of  our proposed iNATran model  we propose a social attention learning (SAL) module to learn the"
        },
        "filled": false,
        "gsrank": 136,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9832594/",
        "author_id": [
            "F2sBN_oAAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:g9J1iZ1jzeQJ:scholar.google.com/&output=cite&scirp=135&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D130%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=g9J1iZ1jzeQJ&ei=K2sBZ7_DNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 71,
        "citedby_url": "/scholar?cites=16486943339140469379&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:g9J1iZ1jzeQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/19/4407674/09832594.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Hyperspectral image classification based on multibranch attention transformer networks",
            "author": [
                "J Bai",
                "Z Wen",
                "Z Xiao",
                "F Ye",
                "Y Zhu"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on …",
            "abstract": "The representative spatial attention mechanism methods include spatial transformer  3)  Transformer Classification: The architecture of a singlelayer transformer encoder module in this"
        },
        "filled": false,
        "gsrank": 137,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9851412/",
        "author_id": [
            "",
            "",
            "8SVEEHYAAAAJ",
            "",
            "AhYP-Q4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:SWwX7uoeLtQJ:scholar.google.com/&output=cite&scirp=136&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D130%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SWwX7uoeLtQJ&ei=K2sBZ7_DNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 47,
        "citedby_url": "/scholar?cites=15289191779335105609&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:SWwX7uoeLtQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/36/4358825/09851412.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Remote sensing image change detection transformer network based on dual-feature mixed attention",
            "author": [
                "X Song",
                "Z Hua",
                "J Li"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on Geoscience and …",
            "abstract": "convolutional networks, attention mechanism and transformer -feature mixed attention based  transformer network (DMATNet mulit-head self-attention based transformer encoder layer. In"
        },
        "filled": false,
        "gsrank": 138,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9905616/",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:7o4dzXhWSX0J:scholar.google.com/&output=cite&scirp=137&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D130%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7o4dzXhWSX0J&ei=K2sBZ7_DNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 34,
        "citedby_url": "/scholar?cites=9027842004878200558&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:7o4dzXhWSX0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/36/4358825/09905616.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "NA-segformer: A multi-level transformer model based on neighborhood attention for colonoscopic polyp segmentation",
            "author": [
                "D Liu",
                "C Lu",
                "H Sun",
                "S Gao"
            ],
            "pub_year": "2024",
            "venue": "Scientific Reports",
            "abstract": "a patch merging module with a neighbor attention mechanism based  Each encoder layer in  the Segmenter captures global  Transformer-based multi-level encoder-decoder architecture"
        },
        "filled": false,
        "gsrank": 139,
        "pub_url": "https://www.nature.com/articles/s41598-024-74123-y",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:qRSKG-k9trYJ:scholar.google.com/&output=cite&scirp=138&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D130%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qRSKG-k9trYJ&ei=K2sBZ7_DNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:qRSKG-k9trYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.nature.com/articles/s41598-024-74123-y.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention based transformer for student answers assessment",
            "author": [
                "N Ait Khayi"
            ],
            "pub_year": "2020",
            "venue": "Proceedings of the Thirty-Third International FLAIRS …",
            "abstract": "proposed model uses a multiYhead attention mechanism that  based model architecture is  different from our proposed model  layer of the transformer. In all experiments, we trained our"
        },
        "filled": false,
        "gsrank": 140,
        "pub_url": "https://par.nsf.gov/biblio/10188357",
        "author_id": [
            "qG0WCT4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:4tqSD9eADV4J:scholar.google.com/&output=cite&scirp=139&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D130%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4tqSD9eADV4J&ei=K2sBZ7_DNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 3,
        "citedby_url": "/scholar?cites=6777214675430071010&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:4tqSD9eADV4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://par.nsf.gov/servlets/purl/10188357"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "EAPT: efficient attention pyramid transformer for image processing",
            "author": [
                "X Lin",
                "S Sun",
                "W Huang",
                "B Sheng",
                "P Li"
            ],
            "pub_year": "2021",
            "venue": "IEEE Transactions on …",
            "abstract": "of the i-th layer to obtain the windows of the (i + 1)-th layer.  En-DeC module, which is based  on an encode-decode architecture  However, such an attention mechanism is not capable of"
        },
        "filled": false,
        "gsrank": 141,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9580642/",
        "author_id": [
            "",
            "nRNw6Z0AAAAJ",
            "",
            "QlGJBvkAAAAJ",
            "P56nZgUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:pLOMOTmUKpYJ:scholar.google.com/&output=cite&scirp=140&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D140%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=pLOMOTmUKpYJ&ei=9msBZ6z2LoWoy9YPtZ-o2Q4&json=",
        "num_citations": 193,
        "citedby_url": "/scholar?cites=10820624028210607012&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:pLOMOTmUKpYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6046/4456689/09580642.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "HT-Net: hierarchical context-attention transformer network for medical ct image segmentation",
            "author": [
                "M Ma",
                "H Xia",
                "Y Tan",
                "H Li",
                "S Song"
            ],
            "pub_year": "2022",
            "venue": "Applied Intelligence",
            "abstract": ", avoiding the poor training effect of transformer-based architecture in the dataset of medical   attention mechanism of transformers, we build a HCA module following the PAA module"
        },
        "filled": false,
        "gsrank": 142,
        "pub_url": "https://link.springer.com/article/10.1007/s10489-021-03010-0",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:OeFiQ_6dWFAJ:scholar.google.com/&output=cite&scirp=141&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D140%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OeFiQ_6dWFAJ&ei=9msBZ6z2LoWoy9YPtZ-o2Q4&json=",
        "num_citations": 34,
        "citedby_url": "/scholar?cites=5789551036362580281&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:OeFiQ_6dWFAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s10489-021-03010-0.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Deep learning attention mechanism in medical image analysis: Basics and beyonds",
            "author": [
                "X Li",
                "M Li",
                "P Yan",
                "G Li",
                "Y Jiang",
                "H Luo"
            ],
            "pub_year": "2023",
            "venue": "… Journal of Network …",
            "abstract": "The attention mechanism is usually used as a module of deep  of channel attention, hybrid  attention, and transformer in medical  [105] proposed a hybrid transformer architecture termed"
        },
        "filled": false,
        "gsrank": 143,
        "pub_url": "https://www.sciltp.com/journals/ijndi/article/view/173",
        "author_id": [
            "HBxeAp8AAAAJ",
            "",
            "",
            "",
            "7xxeOfcAAAAJ",
            "DNDtXD0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:0YTFmWwMbiQJ:scholar.google.com/&output=cite&scirp=142&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D140%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0YTFmWwMbiQJ&ei=9msBZ6z2LoWoy9YPtZ-o2Q4&json=",
        "num_citations": 88,
        "citedby_url": "/scholar?cites=2625049293379437777&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:0YTFmWwMbiQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciltp.com/journals/ijndi/article/view/173"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "MATNet: A combining multi-attention and transformer network for hyperspectral image classification",
            "author": [
                "B Zhang",
                "Y Chen",
                "Y Rong",
                "S Xiong"
            ],
            "pub_year": "2023",
            "venue": "IEEE Transactions on …",
            "abstract": "brandnew architecture uses the self-attention mechanism to  The spatial attention mechanism  module considers the  stacks the transformer layer, we use the cross-layer connection to"
        },
        "filled": false,
        "gsrank": 144,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10064266/",
        "author_id": [
            "ALuJTwMAAAAJ",
            "l0zg1_MAAAAJ",
            "Sv5DluIAAAAJ",
            "wWqRC4IAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:oE8UkLFGWv0J:scholar.google.com/&output=cite&scirp=143&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D140%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oE8UkLFGWv0J&ei=9msBZ6z2LoWoy9YPtZ-o2Q4&json=",
        "num_citations": 45,
        "citedby_url": "/scholar?cites=18255981767940132768&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:oE8UkLFGWv0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/36/4358825/10064266.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Self-and-mixed attention decoder with deep acoustic structure for transformer-based lvcsr",
            "author": [
                "X Zhou",
                "G Lee",
                "E Yılmaz",
                "Y Long",
                "J Liang"
            ],
            "pub_year": "2020",
            "venue": "arXiv preprint arXiv …",
            "abstract": "We also design a mixed attention mechanism that learns the  Multi-head attention is the  core module of the Transformer  Figure 1 shows an encoder-decoder architecture for ASR. We"
        },
        "filled": false,
        "gsrank": 145,
        "pub_url": "https://arxiv.org/abs/2006.10407",
        "author_id": [
            "",
            "m5AuWP4AAAAJ",
            "s8mECnwAAAAJ",
            "dR3syukAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:MPgvI7a9Jy0J:scholar.google.com/&output=cite&scirp=144&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D140%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MPgvI7a9Jy0J&ei=9msBZ6z2LoWoy9YPtZ-o2Q4&json=",
        "num_citations": 11,
        "citedby_url": "/scholar?cites=3253777845770516528&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:MPgvI7a9Jy0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2006.10407"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention swin u-net: Cross-contextual attention mechanism for skin lesion segmentation",
            "author": [
                "EK Aghdam",
                "R Azad",
                "M Zarvani"
            ],
            "pub_year": "2023",
            "venue": "2023 IEEE 20th …",
            "abstract": ", recently a Transformer based U-Net architecture that  incorporating the attention mechanism  into the Transformer skip  U-Net model [10], we propose Att-SwinU-Net, an attention-based"
        },
        "filled": false,
        "gsrank": 146,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10230337/",
        "author_id": [
            "",
            "Qb5ildMAAAAJ",
            "DbjoEg0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:jexRLyyih14J:scholar.google.com/&output=cite&scirp=145&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D140%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jexRLyyih14J&ei=9msBZ6z2LoWoy9YPtZ-o2Q4&json=",
        "num_citations": 42,
        "citedby_url": "/scholar?cites=6811591272077323405&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:jexRLyyih14J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/10230311/10230322/10230337.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Omninet: Omnidirectional representations from transformers",
            "author": [
                "Y Tay",
                "M Dehghani",
                "V Aribandi"
            ],
            "pub_year": "2021",
            "venue": "International …",
            "abstract": "a brief background for the Transformer architecture. The  the core attention mechanism  here. Our choice of the efficient  layer to the pooling operation of the Omnidirectional module."
        },
        "filled": false,
        "gsrank": 147,
        "pub_url": "https://proceedings.mlr.press/v139/tay21b.html",
        "author_id": [
            "VBclY_cAAAAJ",
            "MiHOX3QAAAAJ",
            "P1sLApYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:FkA6DOofFu0J:scholar.google.com/&output=cite&scirp=146&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D140%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FkA6DOofFu0J&ei=9msBZ6z2LoWoy9YPtZ-o2Q4&json=",
        "num_citations": 34,
        "citedby_url": "/scholar?cites=17083877326564507670&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:FkA6DOofFu0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://proceedings.mlr.press/v139/tay21b/tay21b.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Fully cross-attention transformer for guided depth super-resolution",
            "author": [
                "I Ariav",
                "I Cohen"
            ],
            "pub_year": "2023",
            "venue": "Sensors",
            "abstract": "a transformer-based architecture with a novel guidance  using a CAGM module, which  leverages transformers to fuse and  use a cross-attention mechanism instead of self-attention. We"
        },
        "filled": false,
        "gsrank": 148,
        "pub_url": "https://www.mdpi.com/1424-8220/23/5/2723",
        "author_id": [
            "",
            "ZkQc3WMAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:giCLIdZj_2EJ:scholar.google.com/&output=cite&scirp=147&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D140%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=giCLIdZj_2EJ&ei=9msBZ6z2LoWoy9YPtZ-o2Q4&json=",
        "num_citations": 10,
        "citedby_url": "/scholar?cites=7061472512077144194&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:giCLIdZj_2EJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/1424-8220/23/5/2723/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Serialized multi-layer multi-head attention for neural speaker embedding",
            "author": [
                "H Zhu",
                "KA Lee",
                "H Li"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv:2107.06493",
            "abstract": "Inspired by the Transformer network, our proposed method utilizes the hierarchical  proposed  serialized attention mechanism, we use d = 256 and dk = 128 in self-attention module, and"
        },
        "filled": false,
        "gsrank": 149,
        "pub_url": "https://arxiv.org/abs/2107.06493",
        "author_id": [
            "fx2R-wYAAAAJ",
            "SZegiA4AAAAJ",
            "z8_x7C8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:bE_3QFUrFuEJ:scholar.google.com/&output=cite&scirp=148&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D140%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bE_3QFUrFuEJ&ei=9msBZ6z2LoWoy9YPtZ-o2Q4&json=",
        "num_citations": 20,
        "citedby_url": "/scholar?cites=16219198753183584108&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:bE_3QFUrFuEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2107.06493"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A Multi-Scale Cross-Fusion Medical Image Segmentation Network Based on Dual-Attention Mechanism Transformer",
            "author": [
                "J Cui",
                "L Wang",
                "S Jiang"
            ],
            "pub_year": "2023",
            "venue": "Applied Sciences",
            "abstract": "network structure, and this network model uses our proposed DAT for coding and decoding  images. Our DAT module takes the form of a two-layer attention architecture as the network in"
        },
        "filled": false,
        "gsrank": 150,
        "pub_url": "https://www.mdpi.com/2076-3417/13/19/10881",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:csjyHRiU44QJ:scholar.google.com/&output=cite&scirp=149&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D140%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=csjyHRiU44QJ&ei=9msBZ6z2LoWoy9YPtZ-o2Q4&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=9575660064022382706&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:csjyHRiU44QJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2076-3417/13/19/10881/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "The devil is in the details: Window-based attention for image compression",
            "author": [
                "R Zou",
                "C Song",
                "Z Zhang"
            ],
            "pub_year": "2022",
            "venue": "… of the IEEE/CVF conference on …",
            "abstract": "the neural networks with the attention mechanism for  a flexible attention module  combined with neural networks to  the attention mechanism in the transformer are incompatible."
        },
        "filled": false,
        "gsrank": 151,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zou_The_Devil_Is_in_the_Details_Window-Based_Attention_for_Image_CVPR_2022_paper.html",
        "author_id": [
            "",
            "k-tibewAAAAJ",
            "qxWfV6cAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:m4GWRRY90ooJ:scholar.google.com/&output=cite&scirp=150&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D150%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=m4GWRRY90ooJ&ei=wWwBZ4fwHKyCy9YPseaHkQg&json=",
        "num_citations": 162,
        "citedby_url": "/scholar?cites=10003124888209359259&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:m4GWRRY90ooJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Zou_The_Devil_Is_in_the_Details_Window-Based_Attention_for_Image_CVPR_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Improving image captioning by leveraging intra-and inter-layer global representation in transformer network",
            "author": [
                "J Ji",
                "Y Luo",
                "X Sun",
                "F Chen",
                "G Luo",
                "Y Wu"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the AAAI …",
            "abstract": "Transformer architecture, ie, Global Enhanced Transformer ( fed into the multi-head self-attention  module in each layer. By this  the baseline model by adopting the GEA module, which"
        },
        "filled": false,
        "gsrank": 152,
        "pub_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16258",
        "author_id": [
            "xp_rICcAAAAJ",
            "xj0KzfEAAAAJ",
            "KPMK3B4AAAAJ",
            "sP-7Y0EAAAAJ",
            "EyZqU9gAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:S_OljLnw_YgJ:scholar.google.com/&output=cite&scirp=151&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D150%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=S_OljLnw_YgJ&ei=wWwBZ4fwHKyCy9YPseaHkQg&json=",
        "num_citations": 170,
        "citedby_url": "/scholar?cites=9871310637985297227&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:S_OljLnw_YgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ojs.aaai.org/index.php/AAAI/article/view/16258/16065"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Plg-vit: Vision transformer with parallel local and global self-attention",
            "author": [
                "N Ebert",
                "D Stricker",
                "O Wasenmüller"
            ],
            "pub_year": "2023",
            "venue": "Sensors",
            "abstract": "This attention mechanism allows the modeling of  Instead, we replaced self-attention with  a pyramid-pooling module along  Model architecture of our PLG-SA block. The PLG-ViT block"
        },
        "filled": false,
        "gsrank": 153,
        "pub_url": "https://www.mdpi.com/1424-8220/23/7/3447",
        "author_id": [
            "CfFwm1sAAAAJ",
            "ImhXfxgAAAAJ",
            "GkHxKY8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:GohFTowiGWcJ:scholar.google.com/&output=cite&scirp=152&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D150%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GohFTowiGWcJ&ei=wWwBZ4fwHKyCy9YPseaHkQg&json=",
        "num_citations": 10,
        "citedby_url": "/scholar?cites=7429007046328289306&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:GohFTowiGWcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/1424-8220/23/7/3447/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Modeling air quality PM2. 5 forecasting using deep sparse attention-based transformer networks",
            "author": [
                "Z Zhang",
                "S Zhang"
            ],
            "pub_year": "2023",
            "venue": "International Journal of Environmental Science and …",
            "abstract": "In our STN, a multi-head sparse attention mechanism is  masked sparse attention block,  a multi-head attention layer,  LSTMs are a special kind of recurrent architecture used for"
        },
        "filled": false,
        "gsrank": 154,
        "pub_url": "https://link.springer.com/article/10.1007/s13762-023-04900-1",
        "author_id": [
            "",
            "fv-L-64AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:4WkzQ_yqwagJ:scholar.google.com/&output=cite&scirp=153&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D150%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4WkzQ_yqwagJ&ei=wWwBZ4fwHKyCy9YPseaHkQg&json=",
        "num_citations": 14,
        "citedby_url": "/scholar?cites=12160188469312973281&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:4WkzQ_yqwagJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s13762-023-04900-1.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Wild terrestrial animal re-identification based on an improved locally aware transformer with a cross-attention mechanism",
            "author": [
                "Z Zheng",
                "Y Zhao",
                "A Li",
                "Q Yu"
            ],
            "pub_year": "2022",
            "venue": "Animals",
            "abstract": "a transformer network structure with a cross-attention block ( We replace the self-attention  module of the LA transformer with  model consists of three parts, namely, the embedding layer ["
        },
        "filled": false,
        "gsrank": 155,
        "pub_url": "https://www.mdpi.com/2076-2615/12/24/3503",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:0-LYUQz_dwMJ:scholar.google.com/&output=cite&scirp=154&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D150%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0-LYUQz_dwMJ&ei=wWwBZ4fwHKyCy9YPseaHkQg&json=",
        "num_citations": 10,
        "citedby_url": "/scholar?cites=249948732720210643&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:0-LYUQz_dwMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2076-2615/12/24/3503/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Rethinking attention mechanism in time series classification",
            "author": [
                "B Zhao",
                "H Xing",
                "X Wang",
                "F Song",
                "Z Xiao"
            ],
            "pub_year": "2023",
            "venue": "Information Sciences",
            "abstract": "of the Transformer for TSC, namely flexible multi-head linear  the existing Transformer-based  models, our architecture has  CNN-based or attention-based neural networks. However,"
        },
        "filled": false,
        "gsrank": 156,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0020025523000968",
        "author_id": [
            "",
            "8zciJhwAAAAJ",
            "",
            "V7IeVIcAAAAJ",
            "AIIwLaUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:DZ6G9OQRwGgJ:scholar.google.com/&output=cite&scirp=155&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D150%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DZ6G9OQRwGgJ&ei=wWwBZ4fwHKyCy9YPseaHkQg&json=",
        "num_citations": 21,
        "citedby_url": "/scholar?cites=7548052650525629965&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:DZ6G9OQRwGgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0020025523000968"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention calibration for transformer-based sequential recommendation",
            "author": [
                "P Zhou",
                "Q Ye",
                "Y Xie",
                "J Gao",
                "S Wang",
                "JB Kim"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the …",
            "abstract": "highest attention weight learned by the self-attention module  tations of the attention  mechanism: Locker [12] contends that the  to offer a succinct overview of the transformer"
        },
        "filled": false,
        "gsrank": 157,
        "pub_url": "https://dl.acm.org/doi/abs/10.1145/3583780.3614785",
        "author_id": [
            "3dx8O1AAAAAJ",
            "6MFWQPsAAAAJ",
            "XB8oP_gAAAAJ",
            "gCjrTIwAAAAJ",
            "BQ0mBRIAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:J7yyXFU8-m4J:scholar.google.com/&output=cite&scirp=156&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D150%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=J7yyXFU8-m4J&ei=wWwBZ4fwHKyCy9YPseaHkQg&json=",
        "num_citations": 12,
        "citedby_url": "/scholar?cites=7996770425674841127&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:J7yyXFU8-m4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://dl.acm.org/doi/pdf/10.1145/3583780.3614785"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "On the usefulness of self-attention for automatic speech recognition with transformers",
            "author": [
                "S Zhang",
                "E Loweimi",
                "P Bell"
            ],
            "pub_year": "2021",
            "venue": "2021 IEEE Spoken …",
            "abstract": "For attention-based RNN models, the attention mechanism  train a baseline model with a  12-layer self-attention encoder.  designing novel network architecture based on our findings."
        },
        "filled": false,
        "gsrank": 158,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9383521/",
        "author_id": [
            "ZK1dMoYAAAAJ",
            "ke3k7XEAAAAJ",
            "fRf1FbAAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:lv_zUMk5v8kJ:scholar.google.com/&output=cite&scirp=157&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D150%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lv_zUMk5v8kJ&ei=wWwBZ4fwHKyCy9YPseaHkQg&json=",
        "num_citations": 38,
        "citedby_url": "/scholar?cites=14537401658984628118&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:lv_zUMk5v8kJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9383468/9383452/09383521.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Research and implementation of Chinese couplet generation system with attention-based transformer mechanism",
            "author": [
                "Y Wang",
                "J Zhang",
                "B Zhang",
                "Q Jin"
            ],
            "pub_year": "2021",
            "venue": "IEEE Transactions on …",
            "abstract": "of the last layer in the Decoder back to the first layer of the  network is 1024. The number  of headers in the multihead  the complete attention mechanism-based Transformer model to"
        },
        "filled": false,
        "gsrank": 159,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9411906/",
        "author_id": [
            "WAPquN8AAAAJ",
            "",
            "",
            "6viv2j0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:_Kiu1AWUVVcJ:scholar.google.com/&output=cite&scirp=158&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D150%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_Kiu1AWUVVcJ&ei=wWwBZ4fwHKyCy9YPseaHkQg&json=",
        "num_citations": 9,
        "citedby_url": "/scholar?cites=6293098807084099836&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:_Kiu1AWUVVcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6570650/6780646/09411906.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Point transformer v2: Grouped vector attention and partition-based pooling",
            "author": [
                "X Wu",
                "Y Lao",
                "L Jiang",
                "X Liu"
            ],
            "pub_year": "2022",
            "venue": "Advances in Neural …",
            "abstract": "Transformer [1], and propose several novel architecture designs for the attention and pooling  module,  With grouped vector attention restricting the capacity of the attention mechanism,"
        },
        "filled": false,
        "gsrank": 160,
        "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/d78ece6613953f46501b958b7bb4582f-Abstract-Conference.html",
        "author_id": [
            "Np1dTpQAAAAJ",
            "2w9VSWIAAAAJ",
            "5cIodxsAAAAJ",
            "4YL23GMAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:kNbyJcMLyiUJ:scholar.google.com/&output=cite&scirp=159&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D150%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kNbyJcMLyiUJ&ei=wWwBZ4fwHKyCy9YPseaHkQg&json=",
        "num_citations": 247,
        "citedby_url": "/scholar?cites=2723001857482086032&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:kNbyJcMLyiUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/d78ece6613953f46501b958b7bb4582f-Paper-Conference.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Pay attention to mlps",
            "author": [
                "H Liu",
                "Z Dai",
                "D So",
                "QV Le"
            ],
            "pub_year": "2021",
            "venue": "Advances in neural information …",
            "abstract": "For BERT, our model achieves parity with Transformers on  On one hand, the attention  mechanism [18] introduces the  A typical tiny attention module in our experiments has only a"
        },
        "filled": false,
        "gsrank": 161,
        "pub_url": "https://proceedings.neurips.cc/paper/2021/hash/4cc05b35c2f937c5bd9e7d41d3686fff-Abstract.html",
        "author_id": [
            "IMkVH_8AAAAJ",
            "",
            "NmDY9BMAAAAJ",
            "vfT6-XIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:JLKk4MDd5ncJ:scholar.google.com/&output=cite&scirp=160&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D160%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JLKk4MDd5ncJ&ei=i20BZ7LeH6yCy9YPseaHkQg&json=",
        "num_citations": 587,
        "citedby_url": "/scholar?cites=8639836755629224484&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:JLKk4MDd5ncJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper/2021/file/4cc05b35c2f937c5bd9e7d41d3686fff-Paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A CBAM based multiscale transformer fusion approach for remote sensing image change detection",
            "author": [
                "W Wang",
                "X Tan",
                "P Zhang"
            ],
            "pub_year": "2022",
            "venue": "IEEE Journal of Selected …",
            "abstract": "attention mechanism on the basis of the convolutional neural  transformer with the  convolutional block attention module ( going through the transformer encoder layer. Split N into N"
        },
        "filled": false,
        "gsrank": 162,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9855775/",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:1FmSHrrQ1DEJ:scholar.google.com/&output=cite&scirp=161&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D160%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1FmSHrrQ1DEJ&ei=i20BZ7LeH6yCy9YPseaHkQg&json=",
        "num_citations": 80,
        "citedby_url": "/scholar?cites=3590724300716530132&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:1FmSHrrQ1DEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/4609443/9656571/09855775.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Video swin transformer",
            "author": [
                "Z Liu",
                "J Ning",
                "Y Cao",
                "Y Wei",
                "Z Zhang"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the …",
            "abstract": "in the standard Transformer layer with the 3D shifted window based multihead self-attention  module ( We present a pure-transformer architecture for video recognition that is based on"
        },
        "filled": false,
        "gsrank": 163,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2022/html/Liu_Video_Swin_Transformer_CVPR_2022_paper.html",
        "author_id": [
            "9DbprTIAAAAJ",
            "hW0AexsAAAAJ",
            "iRUO1ckAAAAJ",
            "xwudKb4AAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:1bBd9n0g81AJ:scholar.google.com/&output=cite&scirp=162&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D160%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1bBd9n0g81AJ&ei=i20BZ7LeH6yCy9YPseaHkQg&json=",
        "num_citations": 1711,
        "citedby_url": "/scholar?cites=5833041667751260373&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:1bBd9n0g81AJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Video_Swin_Transformer_CVPR_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Music transformer",
            "author": [
                "CZA Huang",
                "A Vaswani",
                "J Uszkoreit",
                "N Shazeer"
            ],
            "pub_year": "2018",
            "venue": "arXiv preprint arXiv …",
            "abstract": "the Transformer with our relative attention mechanism on two  , hence we simply adopt their  architecture. Table 3 shows that  attention-based models is that we can visualize its attention"
        },
        "filled": false,
        "gsrank": 164,
        "pub_url": "https://arxiv.org/abs/1809.04281",
        "author_id": [
            "NRz_EVgAAAAJ",
            "oR9sCGYAAAAJ",
            "mOG0bwsAAAAJ",
            "wsGvgA8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Mi0sBQhVHq4J:scholar.google.com/&output=cite&scirp=163&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D160%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Mi0sBQhVHq4J&ei=i20BZ7LeH6yCy9YPseaHkQg&json=",
        "num_citations": 920,
        "citedby_url": "/scholar?cites=12546559104835661106&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Mi0sBQhVHq4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/1809.04281.pdf?trk=public_post_comment-text"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Localvit: Bringing locality to vision transformers",
            "author": [
                "Y Li",
                "K Zhang",
                "J Cao",
                "R Timofte",
                "L Van Gool"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv …",
            "abstract": "the sequence from the self-attention module must be rearranged  The new transformer  architecture combines a self-attention  within aa sequence owing to their attention mechanism [42"
        },
        "filled": false,
        "gsrank": 165,
        "pub_url": "https://arxiv.org/abs/2104.05707",
        "author_id": [
            "IFLsTGsAAAAJ",
            "0RycFIIAAAAJ",
            "IFYbb7oAAAAJ",
            "u3MwH5kAAAAJ",
            "TwMib_QAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:HPN6eCOoSj0J:scholar.google.com/&output=cite&scirp=164&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D160%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HPN6eCOoSj0J&ei=i20BZ7LeH6yCy9YPseaHkQg&json=",
        "num_citations": 516,
        "citedby_url": "/scholar?cites=4416527254888837916&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:HPN6eCOoSj0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2104.05707"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Structural rotor fault diagnosis using attention-based sensor fusion and transformers",
            "author": [
                "AG Nath",
                "SS Udmale",
                "D Raghuwanshi"
            ],
            "pub_year": "2021",
            "venue": "IEEE Sensors …",
            "abstract": "AM is found in the DL architecture called transformers [27], [28] -wise feed-forward layer with  residual connections and layer  of both multi-head attention mechanism and RNNs. General"
        },
        "filled": false,
        "gsrank": 166,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9625031/",
        "author_id": [
            "",
            "JmPR5vQAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:XegaIv5hJvUJ:scholar.google.com/&output=cite&scirp=165&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D160%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XegaIv5hJvUJ&ei=i20BZ7LeH6yCy9YPseaHkQg&json=",
        "num_citations": 29,
        "citedby_url": "/scholar?cites=17664914332529125469&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:XegaIv5hJvUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/7361/4427201/09625031.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "LncLocFormer: a Transformer-based deep learning model for multi-label lncRNA subcellular localization prediction by using localization-specific attention mechanism",
            "author": [
                "M Zeng",
                "Y Wu",
                "Y Li",
                "R Yin",
                "C Lu",
                "J Duan",
                "M Li"
            ],
            "pub_year": "2023",
            "venue": "Bioinformatics",
            "abstract": "We know that Recurrent Neural Network (RNN) is a  attention scores from the previous  self-attention layer. Instead of using a single attention in the traditional Transformer architecture,"
        },
        "filled": false,
        "gsrank": 167,
        "pub_url": "https://academic.oup.com/bioinformatics/article-abstract/39/12/btad752/7477673",
        "author_id": [
            "Q6b80i8AAAAJ",
            "B8BR0tAAAAAJ",
            "1bV4Do0AAAAJ",
            "MZigJDUAAAAJ",
            "wicacBwAAAAJ",
            "rF_NZFAAAAAJ",
            "w47WJE4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:dMwoe64QmmQJ:scholar.google.com/&output=cite&scirp=166&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D160%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=dMwoe64QmmQJ&ei=i20BZ7LeH6yCy9YPseaHkQg&json=",
        "num_citations": 11,
        "citedby_url": "/scholar?cites=7249124891782859892&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:dMwoe64QmmQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://academic.oup.com/bioinformatics/article-pdf/39/12/btad752/54850429/btad752.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Centroid transformers: Learning to abstract with attention",
            "author": [
                "L Wu",
                "X Liu",
                "Q Liu"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv:2102.08606",
            "abstract": "and use it to motivate the design of our new module.  the design of a new transformer  architecture, and our goal is not  to draw connection between attention mechanism and learning to"
        },
        "filled": false,
        "gsrank": 168,
        "pub_url": "https://arxiv.org/abs/2102.08606",
        "author_id": [
            "PCDSl2sAAAAJ",
            "VOTVE0UAAAAJ",
            "XEx1fZkAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:NpH-4MkrmkkJ:scholar.google.com/&output=cite&scirp=167&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D160%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NpH-4MkrmkkJ&ei=i20BZ7LeH6yCy9YPseaHkQg&json=",
        "num_citations": 31,
        "citedby_url": "/scholar?cites=5303599657245380918&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:NpH-4MkrmkkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2102.08606"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Neural speech synthesis with transformer network",
            "author": [
                "N Li",
                "S Liu",
                "Y Liu",
                "S Zhao",
                "M Liu"
            ],
            "pub_year": "2019",
            "venue": "… of the AAAI conference on artificial …",
            "abstract": "and adapt the multi-head attention mechanism to replace the  introduce the architecture of  our Transformer TTS model, and  Attention-based models for speech recognition. In Advances"
        },
        "filled": false,
        "gsrank": 169,
        "pub_url": "https://ojs.aaai.org/index.php/AAAI/article/view/4642",
        "author_id": [
            "Jhm80SAAAAAJ",
            "kpZq6QwAAAAJ",
            "dIJFz4UAAAAJ",
            "689bIIwAAAAJ",
            "6mNya-wAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:NGmJknn8-WoJ:scholar.google.com/&output=cite&scirp=168&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D160%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NGmJknn8-WoJ&ei=i20BZ7LeH6yCy9YPseaHkQg&json=",
        "num_citations": 843,
        "citedby_url": "/scholar?cites=7708469836301035828&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:NGmJknn8-WoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ojs.aaai.org/index.php/AAAI/article/download/4642/4520"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Recent advances in vision transformer: A survey and outlook of recent work",
            "author": [
                "K Islam"
            ],
            "pub_year": "2022",
            "venue": "arXiv preprint arXiv:2203.01536",
            "abstract": "This transformer is solely based on attention mechanism,  a reattention module, which  regenerated the attention map  volumetric transformer and proposed a unique architecture UNETR"
        },
        "filled": false,
        "gsrank": 170,
        "pub_url": "https://arxiv.org/abs/2203.01536",
        "author_id": [
            "SFClxNEAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:fDGluX2YaI0J:scholar.google.com/&output=cite&scirp=169&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D160%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fDGluX2YaI0J&ei=i20BZ7LeH6yCy9YPseaHkQg&json=",
        "num_citations": 52,
        "citedby_url": "/scholar?cites=10189561822678692220&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:fDGluX2YaI0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Efficient long-range attention network for image super-resolution",
            "author": [
                "X Zhang",
                "H Zeng",
                "S Guo",
                "L Zhang"
            ],
            "pub_year": "2022",
            "venue": "European conference on computer …",
            "abstract": "module, which is further accelerated by using a shared  The existing transformer-based  models such as SwinIR [29]  Finally, by employing the proposed shared attention mechanism, we"
        },
        "filled": false,
        "gsrank": 171,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-031-19790-1_39",
        "author_id": [
            "q76RnqIAAAAJ",
            "Dcew3BIAAAAJ",
            "5hsEmuQAAAAJ",
            "tAK5l1IAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:oZeNmAQIFDAJ:scholar.google.com/&output=cite&scirp=170&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D170%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oZeNmAQIFDAJ&ei=V24BZ9j8AdKAy9YPm-_p6Ao&json=",
        "num_citations": 311,
        "citedby_url": "/scholar?cites=3464402829187061665&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:oZeNmAQIFDAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2203.06697"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Towards robust diagnosis of COVID-19 using vision self-attention transformer",
            "author": [
                "F Mehboob",
                "A Rauf",
                "R Jiang",
                "AKJ Saudagar"
            ],
            "pub_year": "2022",
            "venue": "Scientific Reports",
            "abstract": "CNN architecture 11 . To enhance the local related features, self-attention module performs   are significantly important relevance to machine translation using attention mechanism."
        },
        "filled": false,
        "gsrank": 172,
        "pub_url": "https://www.nature.com/articles/s41598-022-13039-x",
        "author_id": [
            "",
            "RUI3KLIAAAAJ",
            "NuyoNc4AAAAJ",
            "qwgH8dUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:C0Vs7ReBiUUJ:scholar.google.com/&output=cite&scirp=171&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D170%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=C0Vs7ReBiUUJ&ei=V24BZ9j8AdKAy9YPm-_p6Ao&json=",
        "num_citations": 12,
        "citedby_url": "/scholar?cites=5010678000193914123&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:C0Vs7ReBiUUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.nature.com/articles/s41598-022-13039-x.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention-based transformer-BiGRU for question classification",
            "author": [
                "D Han",
                "T Tohti",
                "A Hamdulla"
            ],
            "pub_year": "2022",
            "venue": "Information",
            "abstract": "Our purpose of adding the Attention mechanism is to highlight  The experimental results show  that the model proposed in  , add an attention mechanism behind the two-layer network. On"
        },
        "filled": false,
        "gsrank": 173,
        "pub_url": "https://www.mdpi.com/2078-2489/13/5/214",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Y8NQKMQOFY4J:scholar.google.com/&output=cite&scirp=172&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D170%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Y8NQKMQOFY4J&ei=V24BZ9j8AdKAy9YPm-_p6Ao&json=",
        "num_citations": 7,
        "citedby_url": "/scholar?cites=10238105563549451107&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Y8NQKMQOFY4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2078-2489/13/5/214/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Couplformer: Rethinking vision transformer with coupling attention map",
            "author": [
                "H Lan",
                "X Wang",
                "X Wei"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv:2112.05425",
            "abstract": "memory economy attention mechanism named Couplformer,  Nowadays, Transformer and  attention-based models have  our architecture, we replace the original multi-head attention"
        },
        "filled": false,
        "gsrank": 174,
        "pub_url": "https://arxiv.org/abs/2112.05425",
        "author_id": [
            "qGaEMvoAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:fCdhtF1tNVkJ:scholar.google.com/&output=cite&scirp=173&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D170%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fCdhtF1tNVkJ&ei=V24BZ9j8AdKAy9YPm-_p6Ao&json=",
        "num_citations": 3,
        "citedby_url": "/scholar?cites=6428164292366903164&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:fCdhtF1tNVkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2112.05425"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Maformer: A transformer network with multi-scale attention fusion for visual recognition",
            "author": [
                "H Sun",
                "Y Wang",
                "X Wang",
                "B Zhang",
                "Y Xin",
                "B Zhang"
            ],
            "pub_year": "2024",
            "venue": "Neurocomputing",
            "abstract": "but effective module to explore the full potential of transformers for  Some existing transformers  explore the hybrid architecture to  attention mechanism, and X l denotes the output of l th"
        },
        "filled": false,
        "gsrank": 175,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S092523122400599X",
        "author_id": [
            "oWy4VD4AAAAJ",
            "",
            "",
            "",
            "",
            "ImJz6MsAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:M_0aqEcK01YJ:scholar.google.com/&output=cite&scirp=174&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D170%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=M_0aqEcK01YJ&ei=V24BZ9j8AdKAy9YPm-_p6Ao&json=",
        "num_citations": 7,
        "citedby_url": "/scholar?cites=6256355610227047731&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:M_0aqEcK01YJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S092523122400599X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention-aligned transformer for image captioning",
            "author": [
                "Z Fei"
            ],
            "pub_year": "2022",
            "venue": "proceedings of the AAAI Conference on Artificial …",
            "abstract": "Such attention module learns the attended features that  Attention mechanism plays an  essential role in image captioning, which  Figure 3: Architecture of the A2 Transformer. Mask pertur"
        },
        "filled": false,
        "gsrank": 176,
        "pub_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19940",
        "author_id": [
            "_43YnBcAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:lWw6vSjKPvgJ:scholar.google.com/&output=cite&scirp=175&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D170%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lWw6vSjKPvgJ&ei=V24BZ9j8AdKAy9YPm-_p6Ao&json=",
        "num_citations": 37,
        "citedby_url": "/scholar?cites=17887957046284414101&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:lWw6vSjKPvgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ojs.aaai.org/index.php/AAAI/article/download/19940/19699"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Sentiment classification based on part-of-speech and self-attention mechanism",
            "author": [
                "K Cheng",
                "Y Yue",
                "Z Song"
            ],
            "pub_year": "2020",
            "venue": "IEEE Access",
            "abstract": "Currently, various attention-based neural networks have achieved successes   attention mechanism, we design a new Part-of-Speech based Transformer Attention Network("
        },
        "filled": false,
        "gsrank": 177,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/8962060/",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:BdsZp0fVIHkJ:scholar.google.com/&output=cite&scirp=176&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D170%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BdsZp0fVIHkJ&ei=V24BZ9j8AdKAy9YPm-_p6Ao&json=",
        "num_citations": 52,
        "citedby_url": "/scholar?cites=8728210581566905093&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:BdsZp0fVIHkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6287639/6514899/08962060.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Improved transformer for high-resolution gans",
            "author": [
                "L Zhao",
                "Z Zhang",
                "T Chen"
            ],
            "pub_year": "2021",
            "venue": "Advances in Neural …",
            "abstract": "a cross-attention module performing attention between the  attention mechanism. We follow  the decoder form of Nested  architecture of these stages as multi-axis Nested Transformer"
        },
        "filled": false,
        "gsrank": 178,
        "pub_url": "https://proceedings.neurips.cc/paper/2021/hash/98dce83da57b0395e163467c9dae521b-Abstract.html",
        "author_id": [
            "YTyBTmgAAAAJ",
            "lGrbH60AAAAJ",
            "KoXUMbsAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:c7zbCYsbVcAJ:scholar.google.com/&output=cite&scirp=177&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D170%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=c7zbCYsbVcAJ&ei=V24BZ9j8AdKAy9YPm-_p6Ao&json=",
        "num_citations": 98,
        "citedby_url": "/scholar?cites=13859013712282369139&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:c7zbCYsbVcAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper/2021/file/98dce83da57b0395e163467c9dae521b-Paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "CATNet: Cascaded attention transformer network for marine species image classification",
            "author": [
                "W Zhang",
                "G Chen",
                "P Zhuang",
                "W Zhao"
            ],
            "pub_year": "2024",
            "venue": "Expert Systems with …",
            "abstract": "transformer module, our STM employs the parallel learning strategy of the multi-head attention  mechanism  of the multi-head attention module normalized by the pass layer when l = 1 . b"
        },
        "filled": false,
        "gsrank": 179,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0957417424017998",
        "author_id": [
            "B__yUccAAAAJ",
            "eXhp9C4AAAAJ",
            "K8y4I0AAAAAJ",
            "cdhmBxAAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:8VUifDiKDk4J:scholar.google.com/&output=cite&scirp=178&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D170%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8VUifDiKDk4J&ei=V24BZ9j8AdKAy9YPm-_p6Ao&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=5624584959837754865&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:8VUifDiKDk4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0957417424017998"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer technology in molecular science",
            "author": [
                "J Jiang",
                "L Ke",
                "L Chen",
                "B Dou",
                "Y Zhu"
            ],
            "pub_year": "2024",
            "venue": "Wiley …",
            "abstract": "Additionally, the attention mechanism within transformers facilitates the  decoder architecture,  the model comprises three key components, as illustrated in Figure 9: an Attention module"
        },
        "filled": false,
        "gsrank": 180,
        "pub_url": "https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wcms.1725",
        "author_id": [
            "W1M1PEUAAAAJ",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Xz07P3hskf0J:scholar.google.com/&output=cite&scirp=179&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D170%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Xz07P3hskf0J&ei=V24BZ9j8AdKAy9YPm-_p6Ao&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=18271504426931535199&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Xz07P3hskf0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://wires.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/wcms.1725"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Sparse self-attention transformer for image inpainting",
            "author": [
                "W Huang",
                "Y Deng",
                "S Hui",
                "Y Wu",
                "S Zhou",
                "J Wang"
            ],
            "pub_year": "2024",
            "venue": "Pattern Recognition",
            "abstract": "In Section 3, we present the network architecture of Spa- Spa-attention mechanism, which  is incorporated into the  In the Spa-attention module of the k th transformer block, we first put"
        },
        "filled": false,
        "gsrank": 181,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0031320323005952",
        "author_id": [
            "",
            "sYgv6FYAAAAJ",
            "37G3slYAAAAJ",
            "",
            "2Drvv44AAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:6Kiu2ld_u4cJ:scholar.google.com/&output=cite&scirp=180&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D180%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6Kiu2ld_u4cJ&ei=Im8BZ5vFCayCy9YPseaHkQg&json=",
        "num_citations": 34,
        "citedby_url": "/scholar?cites=9780551031072925928&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:6Kiu2ld_u4cJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0031320323005952"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Brain network transformer",
            "author": [
                "X Kan",
                "W Dai",
                "H Cui",
                "Z Zhang"
            ],
            "pub_year": "2022",
            "venue": "Advances in Neural …",
            "abstract": "Transformer-based designs to graph representation learning. GAT [57] firstly adapts the  attention mechanism to graph neural networks  a two-layer Multi-Head Self-Attention Module and"
        },
        "filled": false,
        "gsrank": 182,
        "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/a408234a9b80604a9cf6ca518e474550-Abstract-Conference.html",
        "author_id": [
            "SiD9eLgAAAAJ",
            "N1x7v90AAAAJ",
            "r0Vh6GEAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:zVUMda6XIpYJ:scholar.google.com/&output=cite&scirp=181&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D180%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zVUMda6XIpYJ&ei=Im8BZ5vFCayCy9YPseaHkQg&json=",
        "num_citations": 93,
        "citedby_url": "/scholar?cites=10818376030441199053&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:zVUMda6XIpYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/a408234a9b80604a9cf6ca518e474550-Paper-Conference.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Crossformer++: A versatile vision transformer hinging on cross-scale attention",
            "author": [
                "W Wang",
                "W Chen",
                "Q Qiu",
                "L Chen",
                "B Wu"
            ],
            "pub_year": "2023",
            "venue": "… on Pattern Analysis …",
            "abstract": "of the powerful attention mechanism. Early vision  layer after the linear transformation layer  of the self-attention module. It  Anyway, most works that focus on transformer architecture"
        },
        "filled": false,
        "gsrank": 183,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10366193/",
        "author_id": [
            "rcxOjikAAAAJ",
            "ZamhRxMAAAAJ",
            "wrP3avMAAAAJ",
            "-gtmMpIAAAAJ",
            "AqDe35sAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:84U9W8kEeXoJ:scholar.google.com/&output=cite&scirp=182&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D180%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=84U9W8kEeXoJ&ei=Im8BZ5vFCayCy9YPseaHkQg&json=",
        "num_citations": 261,
        "citedby_url": "/scholar?cites=8825090207674893811&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:84U9W8kEeXoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/34/4359286/10366193.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention-based generative adversarial network in medical imaging: A narrative review",
            "author": [
                "J Zhao",
                "X Hou",
                "M Pan",
                "H Zhang"
            ],
            "pub_year": "2022",
            "venue": "Computers in Biology and Medicine",
            "abstract": "that Transformer-based GAN will be a promising model in  “generative adversarial network”  and “attention mechanism”, and  The architecture of the Transformer makes few assumptions"
        },
        "filled": false,
        "gsrank": 184,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0010482522006837",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:TSjwbU7sWGsJ:scholar.google.com/&output=cite&scirp=183&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D180%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TSjwbU7sWGsJ&ei=Im8BZ5vFCayCy9YPseaHkQg&json=",
        "num_citations": 37,
        "citedby_url": "/scholar?cites=7735192181604886605&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:TSjwbU7sWGsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0010482522006837"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Global self-attention as a replacement for graph convolution",
            "author": [
                "MS Hussain",
                "MJ Zaki",
                "D Subramanian"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the 28th ACM …",
            "abstract": "In relation to our work, we discuss self-attention based GNN models, where the attention   dynamically formed by the attention mechanism. However, the basic transformer does not have"
        },
        "filled": false,
        "gsrank": 185,
        "pub_url": "https://dl.acm.org/doi/abs/10.1145/3534678.3539296",
        "author_id": [
            "hc97XqQAAAAJ",
            "UmwJklEAAAAJ",
            "j54RzcEAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:ywz2hNvwnDQJ:scholar.google.com/&output=cite&scirp=184&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D180%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ywz2hNvwnDQJ&ei=Im8BZ5vFCayCy9YPseaHkQg&json=",
        "num_citations": 108,
        "citedby_url": "/scholar?cites=3791169811958336715&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:ywz2hNvwnDQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://dl.acm.org/doi/pdf/10.1145/3534678.3539296"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "An effective video transformer with synchronized spatiotemporal and spatial self-attention for action recognition",
            "author": [
                "S Alfasly",
                "CK Chui",
                "Q Jiang",
                "J Lu"
            ],
            "pub_year": "2022",
            "venue": "… on Neural Networks and …",
            "abstract": "spatial attention by another spatial self-attention module in  It expands the model architecture  by adapting six axes,  layer) that is added on top of the temporal selfattention module."
        },
        "filled": false,
        "gsrank": 186,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9834306/",
        "author_id": [
            "BTFNIwYAAAAJ",
            "",
            "BJ1pft0AAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:rzUhbjj3fcMJ:scholar.google.com/&output=cite&scirp=185&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D180%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rzUhbjj3fcMJ&ei=Im8BZ5vFCayCy9YPseaHkQg&json=",
        "num_citations": 33,
        "citedby_url": "/scholar?cites=14086687031222678959&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:rzUhbjj3fcMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/5962385/6104215/09834306.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention is all you need in speech separation",
            "author": [
                "C Subakan",
                "M Ravanelli",
                "S Cornell"
            ],
            "pub_year": "2021",
            "venue": "ICASSP 2021-2021 …",
            "abstract": "Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent  computations with a multi-head attention mechanism a fully attention-based mechanism. By"
        },
        "filled": false,
        "gsrank": 187,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9413901/",
        "author_id": [
            "zXzV-0UAAAAJ",
            "-6Pj3IYAAAAJ",
            "A3lfL0QAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:BwvwD2whQ64J:scholar.google.com/&output=cite&scirp=186&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D180%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BwvwD2whQ64J&ei=Im8BZ5vFCayCy9YPseaHkQg&json=",
        "num_citations": 535,
        "citedby_url": "/scholar?cites=12556916934046649095&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:BwvwD2whQ64J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9413349/9413350/09413901.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Lawin transformer: Improving semantic segmentation transformer with multi-scale representations via large window attention",
            "author": [
                "H Yan",
                "C Zhang",
                "M Wu"
            ],
            "pub_year": "2022",
            "venue": "arXiv preprint arXiv:2201.01615",
            "abstract": "a novel window attention mechanism named large window  R, a SPP module evolves into  a large window attention spatial  window attention and describe the architecture of LawinASPP"
        },
        "filled": false,
        "gsrank": 188,
        "pub_url": "https://arxiv.org/abs/2201.01615",
        "author_id": [
            "tqJSNHMAAAAJ",
            "mBTL7K8AAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:hfqwbn2uTn0J:scholar.google.com/&output=cite&scirp=187&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D180%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hfqwbn2uTn0J&ei=Im8BZ5vFCayCy9YPseaHkQg&json=",
        "num_citations": 83,
        "citedby_url": "/scholar?cites=9029346156675660421&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:hfqwbn2uTn0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2201.01615"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Ps-transformer: Learning sparse photometric stereo network using self-attention mechanism",
            "author": [
                "S Ikehata"
            ],
            "pub_year": "2022",
            "venue": "arXiv preprint arXiv:2211.11386",
            "abstract": "PMA (Pooling by Multihead Attention) module [17] which applies multi-head attention on a   the architecture. This illustrates that how the attention mechanism from the transformer models"
        },
        "filled": false,
        "gsrank": 189,
        "pub_url": "https://arxiv.org/abs/2211.11386",
        "author_id": [
            "Poc22dcAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:m6ZhdztcUXkJ:scholar.google.com/&output=cite&scirp=188&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D180%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=m6ZhdztcUXkJ&ei=Im8BZ5vFCayCy9YPseaHkQg&json=",
        "num_citations": 23,
        "citedby_url": "/scholar?cites=8741869762178557595&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:m6ZhdztcUXkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2211.11386"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer in convolutional neural networks",
            "author": [
                "Y Liu",
                "G Sun",
                "Y Qiu",
                "L Zhang"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv …",
            "abstract": "Motivated by this, we introduce a novel architecture  this module is less powerful for “local-invariant\"  vision data. Combining the H-MHSA module with a more potent convolutional layer"
        },
        "filled": false,
        "gsrank": 190,
        "pub_url": "https://homes.esat.kuleuven.be/~konijn/publications/2021/Liu2.pdf",
        "author_id": [
            "UB3doCoAAAAJ",
            "qd8Blw0AAAAJ",
            "",
            "61LOyWUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Xhupwy48-swJ:scholar.google.com/&output=cite&scirp=189&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D180%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Xhupwy48-swJ&ei=Im8BZ5vFCayCy9YPseaHkQg&json=",
        "num_citations": 80,
        "citedby_url": "/scholar?cites=14770184099463764830&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Xhupwy48-swJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://homes.esat.kuleuven.be/~konijn/publications/2021/Liu2.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Correspondence attention transformer: A context-sensitive network for two-view correspondence learning",
            "author": [
                "J Ma",
                "Y Wang",
                "A Fan",
                "G Xiao"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on …",
            "abstract": "attention mechanism in our correspondence attention block,  architecture of our  correspondence attention transformer is  propagation of the covariance attention module, we need"
        },
        "filled": false,
        "gsrank": 191,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9741369/",
        "author_id": [
            "73trMQkAAAAJ",
            "8Cf_c1QAAAAJ",
            "S1RneAuEVGcC",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:tdT2skb43YUJ:scholar.google.com/&output=cite&scirp=190&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D190%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tdT2skb43YUJ&ei=7W8BZ-2DDqSMy9YPoKv0mAY&json=",
        "num_citations": 11,
        "citedby_url": "/scholar?cites=9646138959431390389&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:tdT2skb43YUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6046/4456689/09741369.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Breast Cancer Image Classification Using External Attention Multilayer Perceptron-Based Transformer",
            "author": [
                "S Barua",
                "MS Islam"
            ],
            "pub_year": "2024",
            "venue": "2024 3rd International Conference on …",
            "abstract": "attention mechanism called External Attention Multi-Layer  crucial component of the  Transformer architecture which divides the  an external attention-based transformer method for"
        },
        "filled": false,
        "gsrank": 192,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10561857/",
        "author_id": [
            "",
            "27mo5eEAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:-TiF1Z9S01IJ:scholar.google.com/&output=cite&scirp=191&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D190%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-TiF1Z9S01IJ&ei=7W8BZ-2DDqSMy9YPoKv0mAY&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=5968204777631594745&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:-TiF1Z9S01IJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel8/10561631/10561632/10561857.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Tgc-yolov5: An enhanced yolov5 drone detection model based on transformer, gam & ca attention mechanism",
            "author": [
                "Y Zhao",
                "Z Ju",
                "T Sun",
                "F Dong",
                "J Li",
                "R Yang",
                "Q Fu",
                "C Lian"
            ],
            "pub_year": "2023",
            "venue": "Drones",
            "abstract": ": First, the Transformer encoder module is incorporated into  , all with the same model  architecture but different widths and  eighth layer of the original Yolov5s version with a Transformer"
        },
        "filled": false,
        "gsrank": 193,
        "pub_url": "https://www.mdpi.com/2504-446X/7/7/446",
        "author_id": [
            "UozbSy4AAAAJ",
            "",
            "",
            "",
            "",
            "",
            "",
            "66qtEeQAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:a9XT0IGnE2cJ:scholar.google.com/&output=cite&scirp=192&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D190%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=a9XT0IGnE2cJ&ei=7W8BZ-2DDqSMy9YPoKv0mAY&json=",
        "num_citations": 14,
        "citedby_url": "/scholar?cites=7427464386460243307&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:a9XT0IGnE2cJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2504-446X/7/7/446/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer with Hybrid Attention Mechanism for Stereo Endoscopic Video Super Resolution",
            "author": [
                "T Zhang",
                "J Yang"
            ],
            "pub_year": "2023",
            "venue": "Symmetry",
            "abstract": "Specifically, the Swin transformer architecture is utilized in  hybrid attention block that combines  the parallel attention module  feed-forward layer in the transformer model spatially aligns"
        },
        "filled": false,
        "gsrank": 194,
        "pub_url": "https://www.mdpi.com/2073-8994/15/10/1947",
        "author_id": [
            "",
            "tmx7tu8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:fDFz7WoFV94J:scholar.google.com/&output=cite&scirp=193&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D190%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fDFz7WoFV94J&ei=7W8BZ-2DDqSMy9YPoKv0mAY&json=",
        "num_citations": 4,
        "citedby_url": "/scholar?cites=16021280156202250620&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:fDFz7WoFV94J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2073-8994/15/10/1947/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer-based multi-attention hybrid networks for skin lesion segmentation",
            "author": [
                "Z Dong",
                "J Li",
                "Z Hua"
            ],
            "pub_year": "2024",
            "venue": "Expert Systems with Applications",
            "abstract": "Gated External Attention module — DGEA module, which  overview of the overall network  architecture TMAHU-Net  propose a novel attention mechanism that utilizes multiscale"
        },
        "filled": false,
        "gsrank": 195,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0957417423035182",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:BBI7cb_wKgMJ:scholar.google.com/&output=cite&scirp=194&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D190%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BBI7cb_wKgMJ&ei=7W8BZ-2DDqSMy9YPoKv0mAY&json=",
        "num_citations": 5,
        "citedby_url": "/scholar?cites=228259436164747780&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:BBI7cb_wKgMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0957417423035182"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Learning contextual transformer network for image inpainting",
            "author": [
                "Y Deng",
                "S Hui",
                "S Zhou",
                "D Meng",
                "J Wang"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the 29th ACM …",
            "abstract": "a multi-scale multi-head attention module to better model the affinity  • We propose a novel  transformer architecture network for  a multi-headed attention mechanism on the same scale,"
        },
        "filled": false,
        "gsrank": 196,
        "pub_url": "https://dl.acm.org/doi/abs/10.1145/3474085.3475426",
        "author_id": [
            "sYgv6FYAAAAJ",
            "37G3slYAAAAJ",
            "2Drvv44AAAAJ",
            "an6w-64AAAAJ",
            "Dk7JgNcAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Sp0NheWoxPcJ:scholar.google.com/&output=cite&scirp=195&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D190%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Sp0NheWoxPcJ&ei=7W8BZ-2DDqSMy9YPoKv0mAY&json=",
        "num_citations": 18,
        "citedby_url": "/scholar?cites=17853580526536727882&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Sp0NheWoxPcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://dl.acm.org/doi/pdf/10.1145/3474085.3475426"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Relation transformer network",
            "author": [
                "R Koner",
                "S Shit",
                "V Tresp"
            ],
            "pub_year": "2020",
            "venue": "arXiv preprint arXiv:2004.06193",
            "abstract": "the attention mechanism of the transformer. An overview of the proposed Relation Transformer  architecture is  We incorporate self-attention module in the encoder of our transformer that"
        },
        "filled": false,
        "gsrank": 197,
        "pub_url": "https://arxiv.org/abs/2004.06193",
        "author_id": [
            "yuOQZtcAAAAJ",
            "",
            "xIJHTUwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:gMvr10VnYacJ:scholar.google.com/&output=cite&scirp=196&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D190%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gMvr10VnYacJ&ei=7W8BZ-2DDqSMy9YPoKv0mAY&json=",
        "num_citations": 27,
        "citedby_url": "/scholar?cites=12061034826747857792&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:gMvr10VnYacJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2004.06193"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Algorithm-hardware co-design of attention mechanism on FPGA devices",
            "author": [
                "X Zhang",
                "Y Wu",
                "P Zhou",
                "X Tang",
                "J Hu"
            ],
            "pub_year": "2021",
            "venue": "ACM Transactions on Embedded …",
            "abstract": "model architecture behind the attention mechanism. To  can compress Transformer (an  attention mechanism based model)  The Transformer is the first self-attention based model. GPT"
        },
        "filled": false,
        "gsrank": 198,
        "pub_url": "https://dl.acm.org/doi/abs/10.1145/3477002",
        "author_id": [
            "J6SP0wMAAAAJ",
            "73k09jEAAAAJ",
            "px_jwFgAAAAJ",
            "jmjRUi4AAAAJ",
            "OcWo8CYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:OKwUhObNIOQJ:scholar.google.com/&output=cite&scirp=197&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D190%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OKwUhObNIOQJ&ei=7W8BZ-2DDqSMy9YPoKv0mAY&json=",
        "num_citations": 48,
        "citedby_url": "/scholar?cites=16438365029844429880&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:OKwUhObNIOQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://dl.acm.org/doi/pdf/10.1145/3477002"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A transformer-based method of multienergy load forecasting in integrated energy system",
            "author": [
                "C Wang",
                "Y Wang",
                "Z Ding",
                "T Zheng"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on …",
            "abstract": ") The attention mechanism is adopted in the proposed model  FORECASTING ARCHITECTURE  In this paper, we proposed  In the multi-head attention module of the encoding layer and"
        },
        "filled": false,
        "gsrank": 199,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9756020/",
        "author_id": [
            "i94R3b8AAAAJ",
            "K5op9CoAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:3w_Mfac0L8kJ:scholar.google.com/&output=cite&scirp=198&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D190%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3w_Mfac0L8kJ&ei=7W8BZ-2DDqSMy9YPoKv0mAY&json=",
        "num_citations": 109,
        "citedby_url": "/scholar?cites=14496863619503624159&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:3w_Mfac0L8kJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/5165411/5446437/09756020.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Fault detection of complicated processes based on an enhanced transformer network with graph attention mechanism",
            "author": [
                "Y Cao",
                "X Tang",
                "X Deng",
                "P Wang"
            ],
            "pub_year": "2024",
            "venue": "Process Safety and Environmental …",
            "abstract": "This architecture is distinguished by its multiple layers, each  layer is equipped with a  multi-head self-attention mechanism includes a multi-head attention mechanism module for"
        },
        "filled": false,
        "gsrank": 200,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0957582024003641",
        "author_id": [
            "",
            "",
            "B57_3boAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:aeoFg9A8JagJ:scholar.google.com/&output=cite&scirp=199&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D190%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aeoFg9A8JagJ&ei=7W8BZ-2DDqSMy9YPoKv0mAY&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=12116157238759254633&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:aeoFg9A8JagJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0957582024003641"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multi-dimension Transformer with Attention-based Filtering for Medical Image Segmentation",
            "author": [
                "W Wang",
                "X Xiao",
                "M Liu",
                "Q Tian",
                "X Huang",
                "Q Lan"
            ],
            "pub_year": "2024",
            "venue": "arXiv preprint arXiv …",
            "abstract": "a convolutional layer within the Transformer to enhance its  to introduce locality into the  Transformer architecture. And an  factor that modulates the attention mechanism’s sensitivity."
        },
        "filled": false,
        "gsrank": 201,
        "pub_url": "https://arxiv.org/abs/2405.12328",
        "author_id": [
            "",
            "",
            "",
            "",
            "",
            "hz8cb2AAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:kPMYQb8mZLUJ:scholar.google.com/&output=cite&scirp=200&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D200%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kPMYQb8mZLUJ&ei=uHABZ-7FAfbYy9YPrPDH6Aw&json=",
        "num_citations": 2,
        "citedby_url": "/scholar?cites=13070614621408785296&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:kPMYQb8mZLUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2405.12328"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Design of a modified transformer architecture based on relative position coding",
            "author": [
                "W Zheng",
                "G Gong",
                "J Tian",
                "S Lu",
                "R Wang",
                "Z Yin"
            ],
            "pub_year": "2023",
            "venue": "International Journal of …",
            "abstract": "transformer module, the calculation formula of self-attention is  position embedding layer.  The performance of the modified  This study uses transformer-based attention mechanism, so"
        },
        "filled": false,
        "gsrank": 202,
        "pub_url": "https://link.springer.com/article/10.1007/s44196-023-00345-z",
        "author_id": [
            "0pRRjPgAAAAJ",
            "",
            "E7JsI0YAAAAJ",
            "SPIm-_UAAAAJ",
            "TS90Fh0AAAAJ",
            "doM8Z-4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:43xVgMm1hdgJ:scholar.google.com/&output=cite&scirp=201&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D200%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=43xVgMm1hdgJ&ei=uHABZ-7FAfbYy9YPrPDH6Aw&json=",
        "num_citations": 55,
        "citedby_url": "/scholar?cites=15602076361141091555&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:43xVgMm1hdgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s44196-023-00345-z.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "CrossFuse: A novel cross attention mechanism based infrared and visible image fusion approach",
            "author": [
                "H Li",
                "XJ Wu"
            ],
            "pub_year": "2024",
            "venue": "Information Fusion",
            "abstract": "To overcome the limitations of current transformer-based  By injecting the CAM into the  transformer architecture, the  novel attention-based loss function is proposed to train our network."
        },
        "filled": false,
        "gsrank": 203,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1566253523004633",
        "author_id": [
            "YemtYUoAAAAJ",
            "5IST34sAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:CseT3lCFi_kJ:scholar.google.com/&output=cite&scirp=202&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D200%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CseT3lCFi_kJ&ei=uHABZ-7FAfbYy9YPrPDH6Aw&json=",
        "num_citations": 30,
        "citedby_url": "/scholar?cites=17981612519584941834&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:CseT3lCFi_kJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1566253523004633"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transnorm: Transformer provides a strong spatial normalization mechanism for a deep segmentation model",
            "author": [
                "R Azad",
                "MT Al-Antary",
                "M Heidari",
                "D Merhof"
            ],
            "pub_year": "2022",
            "venue": "IEEe Access",
            "abstract": "architecture that provides a strong 57 Spatial Normalization  fed into a 149 Transformer  encoder followed by an MLP layer to  attention mechanism based on 204 the Transformer module"
        },
        "filled": false,
        "gsrank": 204,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9908565/",
        "author_id": [
            "Qb5ildMAAAAJ",
            "ZWFq_B0AAAAJ",
            "mir8D5UAAAAJ",
            "0c0rMr0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:_-r-sRc1TSMJ:scholar.google.com/&output=cite&scirp=203&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D200%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_-r-sRc1TSMJ&ei=uHABZ-7FAfbYy9YPrPDH6Aw&json=",
        "num_citations": 69,
        "citedby_url": "/scholar?cites=2543747740420991743&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:_-r-sRc1TSMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6287639/6514899/09908565.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "SwiniPASSR: Swin transformer based parallax attention network for stereo image super-resolution",
            "author": [
                "K Jin",
                "Z Wei",
                "A Yang",
                "S Guo",
                "M Gao"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the …",
            "abstract": ": we firstly elaborate the network architecture and the impact of the conversion layer, then  we  To construct parallax attention mechanism (PAM), we introduce biPAM module and its"
        },
        "filled": false,
        "gsrank": 205,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Jin_SwiniPASSR_Swin_Transformer_Based_Parallax_Attention_Network_for_Stereo_Image_CVPRW_2022_paper.html",
        "author_id": [
            "M77BCKEAAAAJ",
            "",
            "",
            "GQLrk8MAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:YngrV4GpzXwJ:scholar.google.com/&output=cite&scirp=204&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D200%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YngrV4GpzXwJ&ei=uHABZ-7FAfbYy9YPrPDH6Aw&json=",
        "num_citations": 24,
        "citedby_url": "/scholar?cites=8993030403907090530&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:YngrV4GpzXwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Jin_SwiniPASSR_Swin_Transformer_Based_Parallax_Attention_Network_for_Stereo_Image_CVPRW_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attcat: Explaining transformers via attentive class activation tokens",
            "author": [
                "Y Qiang",
                "D Pan",
                "C Li",
                "X Li",
                "R Jang"
            ],
            "pub_year": "2022",
            "venue": "Advances in neural …",
            "abstract": "Figure 1: An illustration of Transformer architecture. The left  Each layer consists of a  self-attention module and a skip  and (b) a feed-forward network module, coupled with layer"
        },
        "filled": false,
        "gsrank": 206,
        "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/20e45668fefa793bd9f2edf19be12c4b-Abstract-Conference.html",
        "author_id": [
            "8ADcg38AAAAJ",
            "JeHpgjoAAAAJ",
            "GeL7DtsAAAAJ",
            "KkPdvB8AAAAJ",
            "h9Nfyf0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:e9GJb2CviwcJ:scholar.google.com/&output=cite&scirp=205&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D200%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=e9GJb2CviwcJ&ei=uHABZ-7FAfbYy9YPrPDH6Aw&json=",
        "num_citations": 39,
        "citedby_url": "/scholar?cites=543721008751300987&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:e9GJb2CviwcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/20e45668fefa793bd9f2edf19be12c4b-Paper-Conference.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Xmorpher: Full transformer for deformable medical image registration via cross attention",
            "author": [
                "J Shi",
                "Y He",
                "Y Kong",
                "JL Coatrieux",
                "H Shu"
            ],
            "pub_year": "2022",
            "venue": "… Conference on Medical …",
            "abstract": "(c) Our cross-attention-based fusion with inner-network  a new attention mechanism, Cross  Attention Transformer (CAT) block : 1) A X-shape transformer architecture with dual parallel U-"
        },
        "filled": false,
        "gsrank": 207,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-031-16446-0_21",
        "author_id": [
            "",
            "WYzBMTUAAAAJ",
            "0eq6vFAAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:vAQqgLnDVSAJ:scholar.google.com/&output=cite&scirp=206&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D200%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vAQqgLnDVSAJ&ei=uHABZ-7FAfbYy9YPrPDH6Aw&json=",
        "num_citations": 71,
        "citedby_url": "/scholar?cites=2329983583720703164&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:vAQqgLnDVSAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2206.07349"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A VGG attention vision transformer network for benign and malignant classification of breast ultrasound images",
            "author": [
                "X Qu",
                "H Lu",
                "W Tang",
                "S Wang",
                "D Zheng",
                "Y Hou"
            ],
            "pub_year": "2022",
            "venue": "Medical …",
            "abstract": ", we employed squeeze-and-excitation (SE) module to enhance  VGGA module followed by  a fully connected (FC) layer for  indicated that self-attention modules of our transformer could"
        },
        "filled": false,
        "gsrank": 208,
        "pub_url": "https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.15852",
        "author_id": [
            "Aq4WRcwAAAAJ",
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:9xB-mcUgNkwJ:scholar.google.com/&output=cite&scirp=207&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D200%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9xB-mcUgNkwJ&ei=uHABZ-7FAfbYy9YPrPDH6Aw&json=",
        "num_citations": 30,
        "citedby_url": "/scholar?cites=5491612828680720631&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:9xB-mcUgNkwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "RPConvformer: A novel Transformer-based deep neural networks for traffic flow prediction",
            "author": [
                "Y Wen",
                "P Xu",
                "Z Li",
                "W Xu",
                "X Wang"
            ],
            "pub_year": "2023",
            "venue": "Expert Systems with Applications",
            "abstract": "Therefore, we propose a multi-head attention mechanism  is passed to the multi-head  attention mechanism layer. Next,  rectangle is a multi-head attention mechanism module, and the"
        },
        "filled": false,
        "gsrank": 209,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S095741742300088X",
        "author_id": [
            "RM4K94oAAAAJ",
            "",
            "gD1HxJMAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:TEEGneV47jwJ:scholar.google.com/&output=cite&scirp=208&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D200%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TEEGneV47jwJ&ei=uHABZ-7FAfbYy9YPrPDH6Aw&json=",
        "num_citations": 27,
        "citedby_url": "/scholar?cites=4390579614310089036&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:TEEGneV47jwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S095741742300088X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Contrans: Improving transformer with convolutional attention for medical image segmentation",
            "author": [
                "A Lin",
                "J Xu",
                "J Li",
                "G Lu"
            ],
            "pub_year": "2022",
            "venue": "… Conference on Medical Image Computing and …",
            "abstract": "module including depthwise convolution, channel attention,  , and leverage the attention  mechanism to reduce feature redundancy  In this paper, we propose a novel architecture termed"
        },
        "filled": false,
        "gsrank": 210,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-031-16443-9_29",
        "author_id": [
            "yz-PwdUAAAAJ",
            "",
            "",
            "fhwB7UwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Ehb8yF6DGFoJ:scholar.google.com/&output=cite&scirp=209&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D200%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ehb8yF6DGFoJ&ei=uHABZ-7FAfbYy9YPrPDH6Aw&json=",
        "num_citations": 19,
        "citedby_url": "/scholar?cites=6492083305976698386&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Ehb8yF6DGFoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer tracking with cyclic shifting window attention",
            "author": [
                "Z Song",
                "J Yu",
                "YPP Chen"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the IEEE …",
            "abstract": "We propose a novel transformer architecture with multi-scale  we consider the transformer  as a feature matching module to  transformer naively and do touch the attention mechanism"
        },
        "filled": false,
        "gsrank": 211,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2022/html/Song_Transformer_Tracking_With_Cyclic_Shifting_Window_Attention_CVPR_2022_paper.html",
        "author_id": [
            "1qnuOZsAAAAJ",
            "_UjqBfcAAAAJ",
            "Vt5edEkAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:OXEy5N0Vu7AJ:scholar.google.com/&output=cite&scirp=210&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D210%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OXEy5N0Vu7AJ&ei=g3EBZ8rjBoHOy9YPxaCm4AE&json=",
        "num_citations": 142,
        "citedby_url": "/scholar?cites=12734796414080676153&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:OXEy5N0Vu7AJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Song_Transformer_Tracking_With_Cyclic_Shifting_Window_Attention_CVPR_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Improved Pest-YOLO: Real-time pest detection based on efficient channel attention mechanism and transformer encoder",
            "author": [
                "Z Tang",
                "J Lu",
                "Z Chen",
                "F Qi",
                "L Zhang"
            ],
            "pub_year": "2023",
            "venue": "Ecological Informatics",
            "abstract": "(CNN) architecture to enhance its capability to capture global  the ECA module into the  ResNet-50-D network. This addition  Instead, we introduce a simple linear layer to transform the"
        },
        "filled": false,
        "gsrank": 212,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1574954123003692",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:JHq_Abg_wUAJ:scholar.google.com/&output=cite&scirp=211&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D210%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JHq_Abg_wUAJ&ei=g3EBZ8rjBoHOy9YPxaCm4AE&json=",
        "num_citations": 15,
        "citedby_url": "/scholar?cites=4666080748468402724&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:JHq_Abg_wUAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1574954123003692"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Adapted transformer network for news recommendation",
            "author": [
                "J Huang",
                "Z Han",
                "H Xu",
                "H Liu"
            ],
            "pub_year": "2022",
            "venue": "Neurocomputing",
            "abstract": "We elaborate the model architecture with adapted transformer interaction  Besides the  adapted transformer interaction module, we also propose two additional attention mechanism to"
        },
        "filled": false,
        "gsrank": 213,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0925231221015356",
        "author_id": [
            "YHbWSOMAAAAJ",
            "",
            "nSLXDo8AAAAJ",
            "lcuYUwoAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:uZYn2NgUsJgJ:scholar.google.com/&output=cite&scirp=212&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D210%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uZYn2NgUsJgJ&ei=g3EBZ8rjBoHOy9YPxaCm4AE&json=",
        "num_citations": 28,
        "citedby_url": "/scholar?cites=11002316811238086329&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:uZYn2NgUsJgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0925231221015356"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Cloze-driven pretraining of self-attention networks",
            "author": [
                "A Baevski",
                "S Edunov",
                "Y Liu",
                "L Zettlemoyer"
            ],
            "pub_year": "2019",
            "venue": "arXiv preprint arXiv …",
            "abstract": "Our bi-directional transformer architecture predicts every  that our model also works well with  an ELMo module on NER  (2017) we apply layer normalization before the self-attention and"
        },
        "filled": false,
        "gsrank": 214,
        "pub_url": "https://arxiv.org/abs/1903.07785",
        "author_id": [
            "i7sxIX8AAAAJ",
            "5w7uYrIAAAAJ",
            "H9buyroAAAAJ",
            "UjpbO6IAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Xid_4_A-M1MJ:scholar.google.com/&output=cite&scirp=213&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D210%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Xid_4_A-M1MJ&ei=g3EBZ8rjBoHOy9YPxaCm4AE&json=",
        "num_citations": 270,
        "citedby_url": "/scholar?cites=5995204733290096478&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Xid_4_A-M1MJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/1903.07785"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "EMTCAL: Efficient multiscale transformer and cross-level attention learning for remote sensing scene classification",
            "author": [
                "X Tang",
                "M Li",
                "J Ma",
                "X Zhang",
                "F Liu"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on …",
            "abstract": "First, we use a multi-layer feature extraction module (MFEM) to  the scaled dotproduct attention  mechanism, its formulation is,  transformer architecture (ie, ViT [44]) and several improved"
        },
        "filled": false,
        "gsrank": 215,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9844016/",
        "author_id": [
            "lmskeBMAAAAJ",
            "",
            "",
            "G6AdRfwAAAAJ",
            "qrQkfxYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:HG5LmNAssZcJ:scholar.google.com/&output=cite&scirp=214&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D210%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HG5LmNAssZcJ&ei=g3EBZ8rjBoHOy9YPxaCm4AE&json=",
        "num_citations": 80,
        "citedby_url": "/scholar?cites=10930567045024804380&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:HG5LmNAssZcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/36/4358825/09844016.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Lightweight vision transformer with cross feature attention",
            "author": [
                "Y Zhao",
                "H Tang",
                "Y Jiang",
                "Q Wu"
            ],
            "pub_year": "2022",
            "venue": "arXiv preprint arXiv:2207.07268",
            "abstract": "strategies to find the optimal architecture under restrictions like  in transformer lies in the  self-attention layer. In the original  of attention module, called cross feature attention (XFA). Fol"
        },
        "filled": false,
        "gsrank": 216,
        "pub_url": "https://arxiv.org/abs/2207.07268",
        "author_id": [
            "rZYsysUAAAAJ",
            "LiiZikgAAAAJ",
            "q5HKTxwAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:BVdeuj9lSugJ:scholar.google.com/&output=cite&scirp=215&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D210%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BVdeuj9lSugJ&ei=g3EBZ8rjBoHOy9YPxaCm4AE&json=",
        "num_citations": 10,
        "citedby_url": "/scholar?cites=16738302289459959557&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:BVdeuj9lSugJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2207.07268"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Trtr: Visual tracking with transformer",
            "author": [
                "M Zhao",
                "K Okada",
                "M Inaba"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv:2105.03817",
            "abstract": "based on Transformer architecture and its attention mechanism to  model, we use 8 heads  for multi-head attention module (M = 8), and set the channel dimension of FFN hidden layer"
        },
        "filled": false,
        "gsrank": 217,
        "pub_url": "https://arxiv.org/abs/2105.03817",
        "author_id": [
            "4P-VLNAAAAAJ",
            "456Oe4YAAAAJ",
            "iwOGCwwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:n_QLNMN2xhsJ:scholar.google.com/&output=cite&scirp=216&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D210%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=n_QLNMN2xhsJ&ei=g3EBZ8rjBoHOy9YPxaCm4AE&json=",
        "num_citations": 112,
        "citedby_url": "/scholar?cites=2001417665176663199&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:n_QLNMN2xhsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2105.03817"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Interpretable CNN-multilevel attention transformer for rapid recognition of pneumonia from chest X-ray images",
            "author": [
                "S Chen",
                "S Ren",
                "G Wang",
                "M Huang"
            ],
            "pub_year": "2023",
            "venue": "IEEE Journal of …",
            "abstract": "low computational complexity attention mechanism, which  Given that this attention module  contains N heads internally,  and input to the subsequent Transformer architecture, where ”×3”"
        },
        "filled": false,
        "gsrank": 218,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10050021/",
        "author_id": [
            "9R3tsmUAAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:NydzOvOP-3MJ:scholar.google.com/&output=cite&scirp=217&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D210%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NydzOvOP-3MJ&ei=g3EBZ8rjBoHOy9YPxaCm4AE&json=",
        "num_citations": 20,
        "citedby_url": "/scholar?cites=8357431808336537399&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:NydzOvOP-3MJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6221020/6363502/10050021.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Set transformer",
            "author": [
                "J Lee",
                "Y Lee",
                "J Kim",
                "AR Kosiorek",
                "S Choi"
            ],
            "pub_year": "2019",
            "venue": "… on Machine Learning",
            "abstract": "attention-based neural network module, the Set Transformer,  Set Transformers, our novel  neural network architecture for  the multihead attention mechanism used in Transformer. We"
        },
        "filled": false,
        "gsrank": 219,
        "pub_url": "https://www.robots.ox.ac.uk/~mobile/Papers/ICLR2019_lee.pdf",
        "author_id": [
            "",
            "BAAZ_ysAAAAJ",
            "KXNUYWgAAAAJ",
            "i7eVfzwAAAAJ",
            "_3O0RcUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:PenrYy95X4AJ:scholar.google.com/&output=cite&scirp=218&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D210%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PenrYy95X4AJ&ei=g3EBZ8rjBoHOy9YPxaCm4AE&json=",
        "num_citations": 29,
        "citedby_url": "/scholar?cites=9250245404089116989&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:PenrYy95X4AJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.robots.ox.ac.uk/~mobile/Papers/ICLR2019_lee.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Reference-based image super-resolution with deformable attention transformer",
            "author": [
                "J Cao",
                "J Liang",
                "K Zhang",
                "Y Li",
                "Y Zhang",
                "W Wang"
            ],
            "pub_year": "2022",
            "venue": "European conference on …",
            "abstract": "-based deformable attention module for correspondence  Different from existing attention  mechanism [39], our attention is  With the help of our architecture, the proposed RDA module is"
        },
        "filled": false,
        "gsrank": 220,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-031-19797-0_19",
        "author_id": [
            "IFYbb7oAAAAJ",
            "3-Hz9BgAAAAJ",
            "0RycFIIAAAAJ",
            "IFLsTGsAAAAJ",
            "ORmLjWoAAAAJ",
            "CqAQQkgAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:p8EtYzQcCN8J:scholar.google.com/&output=cite&scirp=219&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D210%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=p8EtYzQcCN8J&ei=g3EBZ8rjBoHOy9YPxaCm4AE&json=",
        "num_citations": 62,
        "citedby_url": "/scholar?cites=16071126281599435175&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:p8EtYzQcCN8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2207.11938"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "An attention mechanism based convolutional LSTM network for video action recognition",
            "author": [
                "H Ge",
                "Z Yan",
                "W Yu",
                "L Sun"
            ],
            "pub_year": "2019",
            "venue": "Multimedia Tools and Applications",
            "abstract": "transformer proposed in [13], we propose a novel attention based  convolutional LSTM  module following by a softmax layer is  network and keep the basic architecture as our baseline."
        },
        "filled": false,
        "gsrank": 221,
        "pub_url": "https://link.springer.com/article/10.1007/s11042-019-7404-z",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:hs1KI9QJGCMJ:scholar.google.com/&output=cite&scirp=220&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D220%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hs1KI9QJGCMJ&ei=TXIBZ7-YNaSMy9YPoKv0mAY&json=",
        "num_citations": 71,
        "citedby_url": "/scholar?cites=2528781997498355078&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:hs1KI9QJGCMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s11042-019-7404-z.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Se (3)-transformers: 3d roto-translation equivariant attention networks",
            "author": [
                "F Fuchs",
                "D Worrall",
                "V Fischer"
            ],
            "pub_year": "2020",
            "venue": "Advances in neural …",
            "abstract": "(3)-Transformer, a variant of the self-attention module for 3D  We have presented an  attention-based neural architecture  a mathematically motivated attention mechanism which can"
        },
        "filled": false,
        "gsrank": 222,
        "pub_url": "https://proceedings.neurips.cc/paper/2020/hash/15231a7ce4ba789d13b722cc5c955834-Abstract.html",
        "author_id": [
            "nqQdwhsAAAAJ",
            "613GPbQAAAAJ",
            "r6JrCToAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:0ZhsN60ivWIJ:scholar.google.com/&output=cite&scirp=221&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D220%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0ZhsN60ivWIJ&ei=TXIBZ7-YNaSMy9YPoKv0mAY&json=",
        "num_citations": 658,
        "citedby_url": "/scholar?cites=7114881113669802193&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:0ZhsN60ivWIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper_files/paper/2020/file/15231a7ce4ba789d13b722cc5c955834-Paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Automatic identifier of socket for electrical vehicles using SWIN-transformer and SimAM attention mechanism-based EVS YOLO",
            "author": [
                "VC Mahaadevan",
                "R Narayanamoorthi",
                "R Gono"
            ],
            "pub_year": "2023",
            "venue": "IEEE …",
            "abstract": "attention map produced by a two-layer perceptron. CBAM [29]  Swin module in the backbone  of the YOLOv5S architecture.  transformer module into the backbone part of the network,"
        },
        "filled": false,
        "gsrank": 223,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10268929/",
        "author_id": [
            "_kriW0cAAAAJ",
            "TOhCQU4AAAAJ",
            "FSzR6MAAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:ikWkCjAKM6cJ:scholar.google.com/&output=cite&scirp=222&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D220%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ikWkCjAKM6cJ&ei=TXIBZ7-YNaSMy9YPoKv0mAY&json=",
        "num_citations": 8,
        "citedby_url": "/scholar?cites=12047984629599454602&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:ikWkCjAKM6cJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6287639/6514899/10268929.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Improved transformer with multi-head dense collaboration",
            "author": [
                "H Wang",
                "X Shen",
                "M Tu",
                "Y Zhuang"
            ],
            "pub_year": "2022",
            "venue": "IEEE/ACM Transactions …",
            "abstract": "CLC to the Transformerbased encoder-decoder architecture.  module of Transformer, we  adopt the standard Transformer [5 vanilla Transformer without the residual attention mechanism."
        },
        "filled": false,
        "gsrank": 224,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9870037/",
        "author_id": [
            "DeuwxT8AAAAJ",
            "vDpk7pkAAAAJ",
            "2xomuKsAAAAJ",
            "17ERuiwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:0FLaSk9tDQAJ:scholar.google.com/&output=cite&scirp=223&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D220%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0FLaSk9tDQAJ&ei=TXIBZ7-YNaSMy9YPoKv0mAY&json=",
        "num_citations": 7,
        "citedby_url": "/scholar?cites=3779362022904528&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:0FLaSk9tDQAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6570655/9657755/09870037.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Mult: An end-to-end multitask learning transformer",
            "author": [
                "D Bhattacharjee",
                "T Zhang"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the …",
            "abstract": "Detailed overview of our MulT architecture. Our MulT  tasks via a shared attention mechanism  (shown in the bottom left), which we introduce in this work. The encoder module (in green)"
        },
        "filled": false,
        "gsrank": 225,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2022/html/Bhattacharjee_MulT_An_End-to-End_Multitask_Learning_Transformer_CVPR_2022_paper.html",
        "author_id": [
            "F3YYEmMAAAAJ",
            "LurWtuYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:5graO4toaeEJ:scholar.google.com/&output=cite&scirp=224&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D220%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5graO4toaeEJ&ei=TXIBZ7-YNaSMy9YPoKv0mAY&json=",
        "num_citations": 82,
        "citedby_url": "/scholar?cites=16242628478302292710&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:5graO4toaeEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Bhattacharjee_MulT_An_End-to-End_Multitask_Learning_Transformer_CVPR_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Dynamic multi-headed self-attention and multiscale enhancement vision transformer for object detection",
            "author": [
                "S Fang",
                "X Lu",
                "Y Huang",
                "G Sun",
                "X Liu"
            ],
            "pub_year": "2024",
            "venue": "Multimedia Tools and Applications",
            "abstract": "redesigned module, even in a single layer, the network can  Table 1 Layer wise architecture  of DGANet-ViT. Each stage  -headed self-attention mechanism, which enabled the model to"
        },
        "filled": false,
        "gsrank": 226,
        "pub_url": "https://link.springer.com/article/10.1007/s11042-024-18234-8",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:O6ZZd_hfi4gJ:scholar.google.com/&output=cite&scirp=225&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D220%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=O6ZZd_hfi4gJ&ei=TXIBZ7-YNaSMy9YPoKv0mAY&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:O6ZZd_hfi4gJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s11042-024-18234-8.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Self-attention based molecule representation for predicting drug-target interaction",
            "author": [
                "B Shin",
                "S Park",
                "K Kang",
                "JC Ho"
            ],
            "pub_year": "2019",
            "venue": "Machine learning for …",
            "abstract": "a new deep DTI model, Molecule Transformer DTI (MT-DTI),  We first describe the proposed  MT-DTI model architecture ( Each Transformer block consists of a self-attention layer and"
        },
        "filled": false,
        "gsrank": 227,
        "pub_url": "http://proceedings.mlr.press/v106/shin19a.html?ref=https://githubhelp.com",
        "author_id": [
            "j9nUzZAAAAAJ",
            "mpqxShAAAAAJ",
            "QvL9WLwAAAAJ",
            "DrUBb5sAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:R1ZEieDhd0cJ:scholar.google.com/&output=cite&scirp=226&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D220%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=R1ZEieDhd0cJ&ei=TXIBZ7-YNaSMy9YPoKv0mAY&json=",
        "num_citations": 171,
        "citedby_url": "/scholar?cites=5149833053413332551&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:R1ZEieDhd0cJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://proceedings.mlr.press/v106/shin19a/shin19a.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Comparative study of Transformer and LSTM Network with attention mechanism on Image Captioning",
            "author": [
                "P Dandwate",
                "C Shahane",
                "V Jagtap"
            ],
            "pub_year": "2023",
            "venue": "… Conference on Information …",
            "abstract": "is comparable to that of RNNs, but its repeating module  hidden state and passing it through  a fully connected layer. The  captions using a base transformer architecture and achieved a"
        },
        "filled": false,
        "gsrank": 228,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-981-99-3761-5_47",
        "author_id": [
            "",
            "",
            "cj8DdQgAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:p7CvBNAWlM8J:scholar.google.com/&output=cite&scirp=227&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D220%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=p7CvBNAWlM8J&ei=TXIBZ7-YNaSMy9YPoKv0mAY&json=",
        "num_citations": 6,
        "citedby_url": "/scholar?cites=14957605345091891367&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:p7CvBNAWlM8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2303.02648"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Monaural Multi-Talker Speech Recognition with Attention Mechanism and Gated Convolutional Networks.",
            "author": [
                "X Chang",
                "Y Qian",
                "D Yu"
            ],
            "pub_year": "2018",
            "venue": "INTERSPEECH",
            "abstract": "model architecture that incorporates the attention  new model has an encoding transformer  that is a 3-layer BLSTM and a predictor module that is a 3-layer LSTM with 384 cells per layer."
        },
        "filled": false,
        "gsrank": 229,
        "pub_url": "https://ai.tencent.com/ailab/media/publications/MonauralMulti-TalkerSpeechRecognitionwithAttentionMechanismand_GatedConvolutionalNetworks._pdf.pdf",
        "author_id": [
            "cIl2jpMAAAAJ",
            "guG9lxgAAAAJ",
            "tMY31_gAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:EVDyxtzQEx4J:scholar.google.com/&output=cite&scirp=228&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D220%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EVDyxtzQEx4J&ei=TXIBZ7-YNaSMy9YPoKv0mAY&json=",
        "num_citations": 26,
        "citedby_url": "/scholar?cites=2167305492344492049&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:EVDyxtzQEx4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ai.tencent.com/ailab/media/publications/MonauralMulti-TalkerSpeechRecognitionwithAttentionMechanismand_GatedConvolutionalNetworks._pdf.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A semi-supervised approach for the integration of multi-omics data based on transformer multi-head self-attention mechanism and graph convolutional networks",
            "author": [
                "J Wang",
                "N Liao",
                "X Du",
                "Q Chen",
                "B Wei"
            ],
            "pub_year": "2024",
            "venue": "BMC genomics",
            "abstract": "Transformer architecture comprises an input layer,  number of attention heads in the multi-head  attention mechanism.  In this approach, we employed the Transformer encoding module"
        },
        "filled": false,
        "gsrank": 230,
        "pub_url": "https://link.springer.com/article/10.1186/s12864-024-09985-7",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:-4Mi7yvKvFoJ:scholar.google.com/&output=cite&scirp=229&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D220%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-4Mi7yvKvFoJ&ei=TXIBZ7-YNaSMy9YPoKv0mAY&json=",
        "num_citations": 9,
        "citedby_url": "/scholar?cites=6538323049079538683&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:-4Mi7yvKvFoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1186/s12864-024-09985-7.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Language modeling with deep transformers",
            "author": [
                "K Irie",
                "A Zeyer",
                "R Schlüter",
                "H Ney"
            ],
            "pub_year": "2019",
            "venue": "arXiv preprint arXiv:1905.04226",
            "abstract": "subword-level models to attention based encoder-decoder models  the decoder component  of the Transformer architecture [1].  We observe that the first layer of the model with positional"
        },
        "filled": false,
        "gsrank": 231,
        "pub_url": "https://arxiv.org/abs/1905.04226",
        "author_id": [
            "-gZ-BdwAAAAJ",
            "qrh5CBEAAAAJ",
            "JmuAC9oAAAAJ",
            "6C8rf-0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:O-J0luhH8lMJ:scholar.google.com/&output=cite&scirp=230&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D230%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=O-J0luhH8lMJ&ei=GXMBZ93GBvbYy9YPrPDH6Aw&json=",
        "num_citations": 212,
        "citedby_url": "/scholar?cites=6048976313794224699&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:O-J0luhH8lMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/1905.04226"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A transformer-based gan for anomaly detection",
            "author": [
                "C Yang",
                "S Lan",
                "W Huang",
                "W Wang",
                "G Liu"
            ],
            "pub_year": "2022",
            "venue": "… Neural Networks",
            "abstract": "a novel Transformer-based architecture for anomaly  In the self-attention module, different  heads can pay attention to different pixels and areas in the image via the attention mechanism"
        },
        "filled": false,
        "gsrank": 232,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-031-15931-2_29",
        "author_id": [
            "",
            "",
            "",
            "JQFnV5IAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:ECp4QTEjLYEJ:scholar.google.com/&output=cite&scirp=231&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D230%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ECp4QTEjLYEJ&ei=GXMBZ93GBvbYy9YPrPDH6Aw&json=",
        "num_citations": 11,
        "citedby_url": "/scholar?cites=9308134699303447056&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:ECp4QTEjLYEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://personalpages.surrey.ac.uk/w.wang/papers/Yangetal_ICANN2022.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A Siamese network based on multiple attention and multilayer transformers for change detection",
            "author": [
                "W Tang",
                "K Wu",
                "Y Zhang",
                "Y Zhan"
            ],
            "pub_year": "2023",
            "venue": "IEEE Transactions on …",
            "abstract": "attention mechanism or a spatial attention mechanism [40] the attention mechanism to  encoder–decoder architecture  extraction module are fed into a four-layer transformer encoder to"
        },
        "filled": false,
        "gsrank": 233,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10288523/",
        "author_id": [
            "",
            "hDBNNtUAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:nJctUZrPvE4J:scholar.google.com/&output=cite&scirp=232&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D230%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nJctUZrPvE4J&ei=GXMBZ93GBvbYy9YPrPDH6Aw&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=5673637892273837980&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:nJctUZrPvE4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/36/4358825/10288523.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Monotonic multihead attention",
            "author": [
                "X Ma",
                "J Pino",
                "J Cross",
                "L Puzon",
                "J Gu"
            ],
            "pub_year": "2019",
            "venue": "arXiv preprint arXiv:1909.12406",
            "abstract": "attention mechanism, monotonic multihead attention, which enables the Transformer model   An important feature of the Transformer is the use of a separate multihead attention module"
        },
        "filled": false,
        "gsrank": 234,
        "pub_url": "https://arxiv.org/abs/1909.12406",
        "author_id": [
            "al5bfIwAAAAJ",
            "weU_-4IAAAAJ",
            "Oef7pDkAAAAJ",
            "",
            "cB1mFBsAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Cqulgi4qud0J:scholar.google.com/&output=cite&scirp=233&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D230%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Cqulgi4qud0J&ei=GXMBZ93GBvbYy9YPrPDH6Aw&json=",
        "num_citations": 141,
        "citedby_url": "/scholar?cites=15976847532322302730&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Cqulgi4qud0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/1909.12406"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Modeling transformer architecture with attention layer for human activity recognition",
            "author": [
                "G Pareek",
                "S Nigam",
                "R Singh"
            ],
            "pub_year": "2024",
            "venue": "Neural Computing and Applications",
            "abstract": "introduced a temporal attention mechanism that discovers an  via the video action transformer  network. Video action  Additionally, we utilize BERT, an attention-based mechanism, to"
        },
        "filled": false,
        "gsrank": 235,
        "pub_url": "https://link.springer.com/article/10.1007/s00521-023-09362-7",
        "author_id": [
            "",
            "O3nD2M4AAAAJ",
            "7ykYHg8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:PpkXuWsjlMAJ:scholar.google.com/&output=cite&scirp=234&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D230%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=PpkXuWsjlMAJ&ei=GXMBZ93GBvbYy9YPrPDH6Aw&json=",
        "num_citations": 2,
        "citedby_url": "/scholar?cites=13876755297409145150&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:PpkXuWsjlMAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s00521-023-09362-7.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Extracting long‐term spatiotemporal characteristics of traffic flow using attention‐based convolutional transformer",
            "author": [
                "AR Sattarzadeh",
                "PN Pathirana"
            ],
            "pub_year": "2023",
            "venue": "IET Intelligent …",
            "abstract": "to the self-attention layer within the encoder transformer  that incorporates modifications to  the attention mechanism to better  the overall architecture of the spatiotemporal transformers."
        },
        "filled": false,
        "gsrank": 236,
        "pub_url": "https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/itr2.12468",
        "author_id": [
            "",
            "OeQ3qm0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:zWKaZY-Y_VcJ:scholar.google.com/&output=cite&scirp=235&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D230%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=zWKaZY-Y_VcJ&ei=GXMBZ93GBvbYy9YPrPDH6Aw&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=6340391592059888333&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:zWKaZY-Y_VcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/itr2.12468"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Aspect-context level information extraction via transformer based interactive attention mechanism for sentiment classification",
            "author": [
                "S Nayab",
                "MK Hanif",
                "R Talib",
                "MU Sarwar"
            ],
            "pub_year": "2023",
            "venue": "IEEE Access",
            "abstract": "the network architecture containing the attention mechanism is  pre-trained interactiveattention-based  modeling method for  -layer Aspect-Context Interactive Attention (MultiACIA) model"
        },
        "filled": false,
        "gsrank": 237,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10132475/",
        "author_id": [
            "gsLBbOUAAAAJ",
            "Eudse8EAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:peL_AkI__Y8J:scholar.google.com/&output=cite&scirp=236&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D230%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=peL_AkI__Y8J&ei=GXMBZ93GBvbYy9YPrPDH6Aw&json=",
        "num_citations": 3,
        "citedby_url": "/scholar?cites=10375518669282206373&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:peL_AkI__Y8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6287639/6514899/10132475.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "EEG emotion recognition using attention-based convolutional transformer neural network",
            "author": [
                "L Gong",
                "M Li",
                "T Zhang",
                "W Chen"
            ],
            "pub_year": "2023",
            "venue": "Biomedical Signal Processing and Control",
            "abstract": "of all time slices, and feed them into the transformer-based temporal encoding layer to use   spatial and spectral attention mechanism. Next, we use the convolution module to extract the"
        },
        "filled": false,
        "gsrank": 238,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1746809423002689",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Ft-twLDZrk8J:scholar.google.com/&output=cite&scirp=237&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D230%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ft-twLDZrk8J&ei=GXMBZ93GBvbYy9YPrPDH6Aw&json=",
        "num_citations": 42,
        "citedby_url": "/scholar?cites=5741765928114052886&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Ft-twLDZrk8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1746809423002689"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer-based multi-level attention integration network for video saliency prediction",
            "author": [
                "R Tan",
                "M Sun",
                "Y Liang"
            ],
            "pub_year": "2024",
            "venue": "Multimedia Tools and Applications",
            "abstract": "In addition, we add the Transformer module to mine the  introduce the explicit circular  attention mechanism proposed by [ the MLIA and Transformer architecture for establishing long-"
        },
        "filled": false,
        "gsrank": 239,
        "pub_url": "https://link.springer.com/article/10.1007/s11042-024-19404-4",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:H7xJw40zpS8J:scholar.google.com/&output=cite&scirp=238&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D230%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=H7xJw40zpS8J&ei=GXMBZ93GBvbYy9YPrPDH6Aw&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=3433206974899665951&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:H7xJw40zpS8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s11042-024-19404-4.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Spatial-temporal self-attention transformer networks for battery state of charge estimation",
            "author": [
                "D Shi",
                "J Zhao",
                "Z Wang",
                "H Zhao",
                "J Wang",
                "Y Lian"
            ],
            "pub_year": "2023",
            "venue": "Electronics",
            "abstract": "a specialized Transformer-based network architecture, called  The attention-based  Transformer model [43], which is  The attention mechanism is a fundamental component of the"
        },
        "filled": false,
        "gsrank": 240,
        "pub_url": "https://www.mdpi.com/2079-9292/12/12/2598",
        "author_id": [
            "",
            "mg3NO30AAAAJ",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:9ovwCyTucbIJ:scholar.google.com/&output=cite&scirp=239&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D230%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9ovwCyTucbIJ&ei=GXMBZ93GBvbYy9YPrPDH6Aw&json=",
        "num_citations": 20,
        "citedby_url": "/scholar?cites=12858320249706023926&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:9ovwCyTucbIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2079-9292/12/12/2598/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "MCWS-transformers: towards an efficient modeling of protein sequences via multi context-window based scaled self-attention",
            "author": [
                "A Ranjan",
                "MS Fahad"
            ],
            "pub_year": "2022",
            "venue": "IEEE/ACM …",
            "abstract": "The use of the transformer architecture is well-established for plain  transformer module [23]:  (i) Window-based self-attention —  Model A classifier (ie, a multi-layer perceptron network) is"
        },
        "filled": false,
        "gsrank": 241,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9772392/",
        "author_id": [
            "AzOmyncAAAAJ",
            "QCmalJwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:FBtM_rPSk-UJ:scholar.google.com/&output=cite&scirp=240&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D240%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FBtM_rPSk-UJ&ei=6XMBZ-ShNoHOy9YPxaCm4AE&json=",
        "num_citations": 4,
        "citedby_url": "/scholar?cites=16542797526769343252&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:FBtM_rPSk-UJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/8857/4359833/09772392.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transcam: Transformer attention-based cam refinement for weakly supervised semantic segmentation",
            "author": [
                "R Li",
                "Z Mai",
                "Z Zhang",
                "J Jang",
                "S Sanner"
            ],
            "pub_year": "2023",
            "venue": "Journal of Visual Communication …",
            "abstract": "of the multi-head self-attention module in the l th transformer  a linear layer to the class token  at the end of the transformer  that leverages the transformer architecture for the WSSS task."
        },
        "filled": false,
        "gsrank": 242,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1047320323000500",
        "author_id": [
            "m7hRl4YAAAAJ",
            "FT3oT6EAAAAJ",
            "wp3JEzkAAAAJ",
            "-DJPQqgAAAAJ",
            "kB8UPNIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:fFcWQkh8y3sJ:scholar.google.com/&output=cite&scirp=241&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D240%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fFcWQkh8y3sJ&ei=6XMBZ-ShNoHOy9YPxaCm4AE&json=",
        "num_citations": 53,
        "citedby_url": "/scholar?cites=8920360136725649276&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:fFcWQkh8y3sJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1047320323000500"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Co-scale conv-attentional image transformers",
            "author": [
                "W Xu",
                "Y Xu",
                "T Chang",
                "Z Tu"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the IEEE/CVF …",
            "abstract": "across different scales in our co-scale module provides  Here, we develop our factorized  attention mechanism following  CoaT architecture, namely serial and parallel blocks, in order to"
        },
        "filled": false,
        "gsrank": 243,
        "pub_url": "http://openaccess.thecvf.com/content/ICCV2021/html/Xu_Co-Scale_Conv-Attentional_Image_Transformers_ICCV_2021_paper.html",
        "author_id": [
            "6y8YsE8AAAAJ",
            "Qs44LF4AAAAJ",
            "zkDuqfwAAAAJ",
            "9oz-dvgAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Aw0lBddHCC4J:scholar.google.com/&output=cite&scirp=242&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D240%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Aw0lBddHCC4J&ei=6XMBZ-ShNoHOy9YPxaCm4AE&json=",
        "num_citations": 389,
        "citedby_url": "/scholar?cites=3316980114388225283&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Aw0lBddHCC4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Co-Scale_Conv-Attentional_Image_Transformers_ICCV_2021_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformers meet visual learning understanding: A comprehensive review",
            "author": [
                "Y Yang",
                "L Jiao",
                "X Liu",
                "F Liu",
                "S Yang",
                "Z Feng"
            ],
            "pub_year": "2022",
            "venue": "arXiv preprint arXiv …",
            "abstract": "First, the attention mechanism is reviewed, which plays an  visual Transformer model and  the principle of each module  -decoder architecture is also adopted in the Transformer model. It"
        },
        "filled": false,
        "gsrank": 244,
        "pub_url": "https://arxiv.org/abs/2203.12944",
        "author_id": [
            "ujb5xXQAAAAJ",
            "FZbrL2YAAAAJ",
            "_09bkMgAAAAJ",
            "qrQkfxYAAAAJ",
            "TimAUN0AAAAJ",
            "F6-ByvMAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:bE_fVEogRgIJ:scholar.google.com/&output=cite&scirp=243&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D240%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bE_fVEogRgIJ&ei=6XMBZ-ShNoHOy9YPxaCm4AE&json=",
        "num_citations": 34,
        "citedby_url": "/scholar?cites=163853940069191532&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:bE_fVEogRgIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2203.12944"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Hyperspectral Image Classification Using Attention-only Spatial-Spectral Network Based on Transformer",
            "author": [
                "W Liao",
                "F Wang",
                "H Zhao"
            ],
            "pub_year": "2024",
            "venue": "IEEE Access",
            "abstract": "new network architecture called the transformer architecture,  success of the attention  mechanism, this paper proposes a  feature extraction module (ie, the spectral attention module)."
        },
        "filled": false,
        "gsrank": 245,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10587243/",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:1TT5LIsFoRcJ:scholar.google.com/&output=cite&scirp=244&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D240%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1TT5LIsFoRcJ&ei=6XMBZ-ShNoHOy9YPxaCm4AE&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=1702648229435880661&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:1TT5LIsFoRcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel8/6287639/10380310/10587243.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "TSCA-Net: Transformer based spatial-channel attention segmentation network for medical images",
            "author": [
                "Y Fu",
                "J Liu",
                "J Shi"
            ],
            "pub_year": "2024",
            "venue": "Computers in Biology and Medicine",
            "abstract": "and the application of attention mechanism in medical images.  However, the  Transformer-based channel attention module  We are expected to optimize the attention"
        },
        "filled": false,
        "gsrank": 246,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0010482524000222",
        "author_id": [
            "6R43UjIAAAAJ",
            "",
            "GLF3Pl8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:sCwZqbvEGY8J:scholar.google.com/&output=cite&scirp=245&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D240%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sCwZqbvEGY8J&ei=6XMBZ-ShNoHOy9YPxaCm4AE&json=",
        "num_citations": 16,
        "citedby_url": "/scholar?cites=10311489132116389040&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:sCwZqbvEGY8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0010482524000222"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "TRS: Transformers for remote sensing scene classification",
            "author": [
                "J Zhang",
                "H Zhao",
                "J Li"
            ],
            "pub_year": "2021",
            "venue": "Remote Sensing",
            "abstract": "through the attention mechanism. The self-attention-based structure proposed in Transformer  [ In further works, we will attempt to apply the complete Transformer architecture (encoder +"
        },
        "filled": false,
        "gsrank": 247,
        "pub_url": "https://www.mdpi.com/2072-4292/13/20/4143",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:w05VTDcI0dEJ:scholar.google.com/&output=cite&scirp=246&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D240%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=w05VTDcI0dEJ&ei=6XMBZ-ShNoHOy9YPxaCm4AE&json=",
        "num_citations": 97,
        "citedby_url": "/scholar?cites=15118874457656348355&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:w05VTDcI0dEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2072-4292/13/20/4143/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Swiftformer: Efficient additive attention for transformer-based real-time mobile vision applications",
            "author": [
                "A Shaker",
                "M Maaz",
                "H Rasheed"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the …",
            "abstract": "a novel efficient additive attention mechanism that effectively  Inspired by the transformer  architecture, we employ a linear  baseline model, which only includes the self-attention based"
        },
        "filled": false,
        "gsrank": 248,
        "pub_url": "http://openaccess.thecvf.com/content/ICCV2023/html/Shaker_SwiftFormer_Efficient_Additive_Attention_for_Transformer-based_Real-time_Mobile_Vision_Applications_ICCV_2023_paper.html",
        "author_id": [
            "eEz4Wu4AAAAJ",
            "vTy9Te8AAAAJ",
            "yhDdEuEAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:2PujIILGOgkJ:scholar.google.com/&output=cite&scirp=247&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D240%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2PujIILGOgkJ&ei=6XMBZ-ShNoHOy9YPxaCm4AE&json=",
        "num_citations": 47,
        "citedby_url": "/scholar?cites=665062157186235352&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:2PujIILGOgkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://openaccess.thecvf.com/content/ICCV2023/papers/Shaker_SwiftFormer_Efficient_Additive_Attention_for_Transformer-based_Real-time_Mobile_Vision_Applications_ICCV_2023_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Learning Attention from Attention: Efficient Self-Refinement Transformer for Face Super-Resolution.",
            "author": [
                "G Li",
                "J Shi",
                "Y Zong",
                "F Wang",
                "T Wang",
                "Y Gong"
            ],
            "pub_year": "2023",
            "venue": "IJCAI",
            "abstract": "module which integrates shallowlayer structure and deep- mechanism for Transformer, called  Region Selection  • We propose an efficient self-refinement Transformerbased architecture"
        },
        "filled": false,
        "gsrank": 249,
        "pub_url": "https://www.ijcai.org/proceedings/2023/0115.pdf",
        "author_id": [
            "",
            "N2ftCz4AAAAJ",
            "rv14ap4AAAAJ",
            "LKPpmXQAAAAJ",
            "IP_f-voAAAAJ",
            "x2xdU7gAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:NniORFjc8zMJ:scholar.google.com/&output=cite&scirp=248&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D240%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NniORFjc8zMJ&ei=6XMBZ-ShNoHOy9YPxaCm4AE&json=",
        "num_citations": 7,
        "citedby_url": "/scholar?cites=3743577986940434486&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:NniORFjc8zMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.ijcai.org/proceedings/2023/0115.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Recent progress in transformer-based medical image analysis",
            "author": [
                "Z Liu",
                "Q Lv",
                "Z Yang",
                "Y Li",
                "CH Lee",
                "L Shen"
            ],
            "pub_year": "2023",
            "venue": "Computers in Biology and …",
            "abstract": "the core component of the transformer, the attention mechanism, and the detailed structures  of the transformer. After that, we depict the recent progress of the transformer in the field of"
        },
        "filled": false,
        "gsrank": 250,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0010482523007333",
        "author_id": [
            "GX1z4QkAAAAJ",
            "Gu-qh44AAAAJ",
            "w21CvkUAAAAJ",
            "NuZDso4AAAAJ",
            "",
            "EIk6Z1MAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:vbrZeOrTdgkJ:scholar.google.com/&output=cite&scirp=249&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D240%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vbrZeOrTdgkJ&ei=6XMBZ-ShNoHOy9YPxaCm4AE&json=",
        "num_citations": 39,
        "citedby_url": "/scholar?cites=681965397596551869&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:vbrZeOrTdgkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0010482523007333"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Learning affinity from attention: End-to-end weakly-supervised semantic segmentation with transformers",
            "author": [
                "L Ru",
                "Y Zhan",
                "B Yu",
                "B Du"
            ],
            "pub_year": "2022",
            "venue": "… of the IEEE/CVF conference on …",
            "abstract": "(AFA) module to learn semantic affinity from the multihead  We argue that the Transformer  architecture naturally benefits  Embedded discriminative attention mechanism for weakly"
        },
        "filled": false,
        "gsrank": 251,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2022/html/Ru_Learning_Affinity_From_Attention_End-to-End_Weakly-Supervised_Semantic_Segmentation_With_Transformers_CVPR_2022_paper.html",
        "author_id": [
            "y7wegKQAAAAJ",
            "rjd977cAAAAJ",
            "fjzIdMQAAAAJ",
            "Shy1gnMAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:D21lrn73a1oJ:scholar.google.com/&output=cite&scirp=250&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D250%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=D21lrn73a1oJ&ei=s3QBZ4idOPbYy9YPrPDH6Aw&json=",
        "num_citations": 199,
        "citedby_url": "/scholar?cites=6515573409385377039&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:D21lrn73a1oJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Ru_Learning_Affinity_From_Attention_End-to-End_Weakly-Supervised_Semantic_Segmentation_With_Transformers_CVPR_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Facial action unit detection based on transformer and attention mechanism",
            "author": [
                "W Song",
                "S Shi",
                "G An"
            ],
            "pub_year": "2021",
            "venue": "International Conference on Image and Graphics",
            "abstract": "network based on Transformer and Attention Mechanism named  predefined attention map,  we refer to the skipping layer structure  module on the fixed attention mechanism is effective."
        },
        "filled": false,
        "gsrank": 252,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-87358-5_37",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:SMDe5a2jeYMJ:scholar.google.com/&output=cite&scirp=251&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D250%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SMDe5a2jeYMJ&ei=s3QBZ4idOPbYy9YPrPDH6Aw&json=",
        "num_citations": 2,
        "citedby_url": "/scholar?cites=9473783258431799368&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:SMDe5a2jeYMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Two-stream transformer network for sensor-based human activity recognition",
            "author": [
                "S Xiao",
                "S Wang",
                "Z Huang",
                "Y Wang",
                "H Jiang"
            ],
            "pub_year": "2022",
            "venue": "Neurocomputing",
            "abstract": "propose a self-attention based Two-stream Transformer Network ( In aggregation layer, we  use the attention mechanism to  the contribution of two-stream architecture. From Table 5, we"
        },
        "filled": false,
        "gsrank": 253,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0925231222011924",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Wd6e2fIxpZ8J:scholar.google.com/&output=cite&scirp=252&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D250%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Wd6e2fIxpZ8J&ei=s3QBZ4idOPbYy9YPrPDH6Aw&json=",
        "num_citations": 30,
        "citedby_url": "/scholar?cites=11503655742290714201&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Wd6e2fIxpZ8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0925231222011924"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Psvit: Better vision transformer via token pooling and attention sharing",
            "author": [
                "B Chen",
                "P Li",
                "B Li",
                "C Li",
                "L Bai",
                "C Lin",
                "M Sun"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv …",
            "abstract": "The flexible attention mechanism of the transformer in the whole pipeline intrigued the   independent multi-head attention module in designing the overall transformer architecture."
        },
        "filled": false,
        "gsrank": 254,
        "pub_url": "https://arxiv.org/abs/2108.03428",
        "author_id": [
            "o5wjqPEAAAAJ",
            "O9wW1OQAAAAJ",
            "",
            "",
            "sakOO04AAAAJ",
            "rObgGWIAAAAJ",
            "1b1jbCQAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:irijo4y1AEIJ:scholar.google.com/&output=cite&scirp=253&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D250%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=irijo4y1AEIJ&ei=s3QBZ4idOPbYy9YPrPDH6Aw&json=",
        "num_citations": 42,
        "citedby_url": "/scholar?cites=4756000822148708490&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:irijo4y1AEIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2108.03428"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "LUCMT: Learnable under-sampling and reconstructed network with cross multi-head attention transformer for accelerating MR image reconstruction",
            "author": [
                "Z Yang",
                "M Jiang",
                "D Ruan",
                "Y Li",
                "T Tan",
                "S Huang"
            ],
            "pub_year": "2024",
            "venue": "Computer Methods and …",
            "abstract": "network employs a CS-MRI depth-expanded architecture,  In addition, we introduce a  multi-head attention mechanism  of the multi-head attention mechanism in the CMA module and"
        },
        "filled": false,
        "gsrank": 255,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0169260724003523",
        "author_id": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:e7w6BImQuq8J:scholar.google.com/&output=cite&scirp=254&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D250%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=e7w6BImQuq8J&ei=s3QBZ4idOPbYy9YPrPDH6Aw&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:e7w6BImQuq8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0169260724003523"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A Convolutional Neural Network Based on Attention Mechanism for Designing Vibration Similarity Models of Converter Transformers",
            "author": [
                "H Wang",
                "L Zhang",
                "Y Sun",
                "L Zou"
            ],
            "pub_year": "2023",
            "venue": "Machines",
            "abstract": "layer, convolutional layer, pooling layer, fully connected layer The attention module can  be added between different  convolutional neural network vibration prediction model for ±"
        },
        "filled": false,
        "gsrank": 256,
        "pub_url": "https://www.mdpi.com/2075-1702/12/1/11",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:TmXs6dacxp4J:scholar.google.com/&output=cite&scirp=255&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D250%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TmXs6dacxp4J&ei=s3QBZ4idOPbYy9YPrPDH6Aw&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=11441004350242841934&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:TmXs6dacxp4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2075-1702/12/1/11/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Underwater image enhancement via adaptive group attention-based multiscale cascade transformer",
            "author": [
                "Z Huang",
                "J Li",
                "Z Hua",
                "L Fan"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on …",
            "abstract": "Swin Transformer block and the channel attention mechanism network, we compose the  encoder with Patch Merging layer  , ie, with the Swin Transformer network architecture alone, the"
        },
        "filled": false,
        "gsrank": 257,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9825662/",
        "author_id": [
            "4WsSaSsAAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:jQA2IQ5LDpIJ:scholar.google.com/&output=cite&scirp=256&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D250%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jQA2IQ5LDpIJ&ei=s3QBZ4idOPbYy9YPrPDH6Aw&json=",
        "num_citations": 58,
        "citedby_url": "/scholar?cites=10524431903270240397&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:jQA2IQ5LDpIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/19/4407674/09825662.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "ST-MDAMNet: Swin transformer combines multi-dimensional attention mechanism for semantic segmentation of high-resolution earth surface images",
            "author": [
                "B Liu",
                "B Li",
                "H Liu",
                "S Li"
            ],
            "pub_year": "2024",
            "venue": "Advances in Space Research",
            "abstract": "field, traditional convolutional neural networks (CNN) cannot  Swin Transformer encoder to  effectively enhance the model’s  We demonstrate the effectiveness of each module through"
        },
        "filled": false,
        "gsrank": 258,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0273117724006471",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:YcigcV2RkY0J:scholar.google.com/&output=cite&scirp=257&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D250%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YcigcV2RkY0J&ei=s3QBZ4idOPbYy9YPrPDH6Aw&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:YcigcV2RkY0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0273117724006471"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Glalt: Global-local attention-augmented light transformer for scene text recognition",
            "author": [
                "H Zhang",
                "G Luo",
                "J Kang",
                "S Huang"
            ],
            "pub_year": "2023",
            "venue": "… on Neural Networks …",
            "abstract": "attention mechanism. The encoder integrates the self-attention module with the convolution  module  encoding and decoding mechanism in the attention architecture but also leverage"
        },
        "filled": false,
        "gsrank": 259,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10059008/",
        "author_id": [
            "X91w9HIAAAAJ",
            "_UznnkwAAAAJ",
            "C1mHxJEAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:H-wjnwyRvFEJ:scholar.google.com/&output=cite&scirp=258&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D250%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=H-wjnwyRvFEJ&ei=s3QBZ4idOPbYy9YPrPDH6Aw&json=",
        "num_citations": 10,
        "citedby_url": "/scholar?cites=5889741896089332767&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:H-wjnwyRvFEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/5962385/6104215/10059008.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Crackformer: Transformer network for fine-grained crack detection",
            "author": [
                "H Liu",
                "X Miao",
                "C Mertz",
                "C Xu"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the IEEE …",
            "abstract": "Therefore, we seek the help from scalingattention mechanism propose novel self-attention  blocks as the basic module. To  We derive our model from the SegNet basic architecture and"
        },
        "filled": false,
        "gsrank": 260,
        "pub_url": "http://openaccess.thecvf.com/content/ICCV2021/html/Liu_CrackFormer_Transformer_Network_for_Fine-Grained_Crack_Detection_ICCV_2021_paper.html",
        "author_id": [
            "e_R7-24AAAAJ",
            "",
            "KrgivkIAAAAJ",
            "XsBBTUgAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:8aNFLbbz59UJ:scholar.google.com/&output=cite&scirp=259&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D250%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8aNFLbbz59UJ&ei=s3QBZ4idOPbYy9YPrPDH6Aw&json=",
        "num_citations": 117,
        "citedby_url": "/scholar?cites=15413556213467948017&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:8aNFLbbz59UJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Liu_CrackFormer_Transformer_Network_for_Fine-Grained_Crack_Detection_ICCV_2021_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Architecting an enterprise financial management model: leveraging multi-head attention mechanism-transformer for user information transformation",
            "author": [
                "W Yu",
                "H Hamam"
            ],
            "pub_year": "2024",
            "venue": "PeerJ Computer Science",
            "abstract": "refined Transformer network, the multi-head attention mechanism ( This mechanism endows  the network with heightened semantic  Each module includes a 1-D convolution layer with"
        },
        "filled": false,
        "gsrank": 261,
        "pub_url": "https://peerj.com/articles/cs-1928/",
        "author_id": [
            "",
            "y3c3DwUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:JkNS7ANZoB4J:scholar.google.com/&output=cite&scirp=260&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D260%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JkNS7ANZoB4J&ei=fnUBZ_yKNKyCy9YPseaHkQg&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=2206861690796131110&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:JkNS7ANZoB4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://peerj.com/articles/cs-1928.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A superior image inpainting scheme using Transformer-based self-supervised attention GAN model",
            "author": [
                "M Zhou",
                "X Liu",
                "T Yi",
                "Z Bai",
                "P Zhang"
            ],
            "pub_year": "2023",
            "venue": "Expert Systems with Applications",
            "abstract": "-supervised attention module in the Transformer to overcome  This paper constructs a new  Swin GAN overall architecture,  supervised attention mechanism and Swin Transformer-based"
        },
        "filled": false,
        "gsrank": 262,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0957417423014082",
        "author_id": [
            "",
            "",
            "",
            "",
            "uQSN2bEAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:5EINE2zB0nwJ:scholar.google.com/&output=cite&scirp=261&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D260%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=5EINE2zB0nwJ&ei=fnUBZ_yKNKyCy9YPseaHkQg&json=",
        "num_citations": 11,
        "citedby_url": "/scholar?cites=8994464075732566756&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:5EINE2zB0nwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0957417423014082"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Cross-attention transformer for video interpolation",
            "author": [
                "HH Kim",
                "S Yu",
                "S Yuan"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the Asian …",
            "abstract": "vision transformer module and spatial attention module, still  various modules in the transformer  architecture, and derives  cross-similarity transformer and spatial attention mechanism."
        },
        "filled": false,
        "gsrank": 263,
        "pub_url": "https://openaccess.thecvf.com/content/ACCV2022W/TCV/html/Kim_Cross-Attention_Transformer_for_Video_Interpolation_ACCVW_2022_paper.html",
        "author_id": [
            "",
            "XRyxNgcAAAAJ",
            "uViWK0EAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:XKOzjcZsJOAJ:scholar.google.com/&output=cite&scirp=262&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D260%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XKOzjcZsJOAJ&ei=fnUBZ_yKNKyCy9YPseaHkQg&json=",
        "num_citations": 16,
        "citedby_url": "/scholar?cites=16151153763694125916&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:XKOzjcZsJOAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/ACCV2022W/TCV/papers/Kim_Cross-Attention_Transformer_for_Video_Interpolation_ACCVW_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A robust attention-enhanced network with transformer for visual tracking",
            "author": [
                "F Gu",
                "J Lu",
                "C Cai"
            ],
            "pub_year": "2023",
            "venue": "Multimedia Tools and Applications",
            "abstract": "module and the global feature information fusion module is  The network architecture of our  tracker is very concise, which  Transformer and introduce the attention mechanism into our"
        },
        "filled": false,
        "gsrank": 264,
        "pub_url": "https://link.springer.com/article/10.1007/s11042-023-15168-5",
        "author_id": [
            "",
            "WRdf568AAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:nWCsOkSX36wJ:scholar.google.com/&output=cite&scirp=263&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D260%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=nWCsOkSX36wJ&ei=fnUBZ_yKNKyCy9YPseaHkQg&json=",
        "num_citations": 8,
        "citedby_url": "/scholar?cites=12456841413628027037&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:nWCsOkSX36wJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s11042-023-15168-5.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "An arrhythmia classification model based on vision transformer with deformable attention",
            "author": [
                "Y Dong",
                "M Zhang",
                "L Qiu",
                "L Wang",
                "Y Yu"
            ],
            "pub_year": "2023",
            "venue": "Micromachines",
            "abstract": ", the transformer architecture can capture temporal features through an attention mechanism  and  First, we assess the effectiveness of the deformable attention module. In our CNN-DVIT"
        },
        "filled": false,
        "gsrank": 265,
        "pub_url": "https://www.mdpi.com/2072-666X/14/6/1155",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:DLQlpkVJZP0J:scholar.google.com/&output=cite&scirp=264&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D260%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DLQlpkVJZP0J&ei=fnUBZ_yKNKyCy9YPseaHkQg&json=",
        "num_citations": 15,
        "citedby_url": "/scholar?cites=18258799352755893260&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:DLQlpkVJZP0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2072-666X/14/6/1155/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Dual aggregation transformer for image super-resolution",
            "author": [
                "Z Chen",
                "Y Zhang",
                "J Gu",
                "L Kong"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the …",
            "abstract": "self-attention module, we propose the adaptive interaction  in deepening the layer of the  network for better performance.  introducing the architecture of dual aggregation Transformer ("
        },
        "filled": false,
        "gsrank": 266,
        "pub_url": "http://openaccess.thecvf.com/content/ICCV2023/html/Chen_Dual_Aggregation_Transformer_for_Image_Super-Resolution_ICCV_2023_paper.html",
        "author_id": [
            "nLZtXdgAAAAJ",
            "ORmLjWoAAAAJ",
            "uMQ-G-QAAAAJ",
            "-wm2X-8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Xo6m4xKnwFEJ:scholar.google.com/&output=cite&scirp=265&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D260%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Xo6m4xKnwFEJ&ei=fnUBZ_yKNKyCy9YPseaHkQg&json=",
        "num_citations": 132,
        "citedby_url": "/scholar?cites=5890892012171202142&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Xo6m4xKnwFEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Dual_Aggregation_Transformer_for_Image_Super-Resolution_ICCV_2023_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Streaming automatic speech recognition with the transformer model",
            "author": [
                "N Moritz",
                "T Hori",
                "J Le"
            ],
            "pub_year": "2020",
            "venue": "ICASSP 2020-2020 IEEE International …",
            "abstract": ") compared to recurrent neural network (RNN) based system  -decoder attention mechanism  of the transformer model to  transformer architecture consists of a two-layer CNN module"
        },
        "filled": false,
        "gsrank": 267,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9054476/",
        "author_id": [
            "9YaJtg8AAAAJ",
            "ZZGpQG4AAAAJ",
            "aUpxty8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:m1IFoRB_Sg0J:scholar.google.com/&output=cite&scirp=266&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D260%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=m1IFoRB_Sg0J&ei=fnUBZ_yKNKyCy9YPseaHkQg&json=",
        "num_citations": 218,
        "citedby_url": "/scholar?cites=957717580167336603&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:m1IFoRB_Sg0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9040208/9052899/09054476.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Bayesian Transformer Using Disentangled Mask Attention.",
            "author": [
                "JT Chien",
                "YH Huang"
            ],
            "pub_year": "2022",
            "venue": "INTERSPEECH",
            "abstract": "First, due to the natural property that attention mechanism  mask attention based on the  disentangled attention heads for  The first level involves latent variables of transformer layer"
        },
        "filled": false,
        "gsrank": 268,
        "pub_url": "https://www.isca-archive.org/interspeech_2022/chien22_interspeech.pdf",
        "author_id": [
            "KkyjekUAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:DyAGfft5OsIJ:scholar.google.com/&output=cite&scirp=267&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D260%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DyAGfft5OsIJ&ei=fnUBZ_yKNKyCy9YPseaHkQg&json=",
        "num_citations": 13,
        "citedby_url": "/scholar?cites=13995632913048543247&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:DyAGfft5OsIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.isca-archive.org/interspeech_2022/chien22_interspeech.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer based memory network for sentiment analysis of web comments",
            "author": [
                "M Jiang",
                "J Wu",
                "X Shi",
                "M Zhang"
            ],
            "pub_year": "2019",
            "venue": "IEEE Access",
            "abstract": "module. We use a global self-attention mechanism and a local attention mechanism (memory   of the sequence transduction model, we need to use a novel architecture, the self-attention"
        },
        "filled": false,
        "gsrank": 269,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/8918438/",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:sRBJ0dUSKOcJ:scholar.google.com/&output=cite&scirp=268&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D260%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sRBJ0dUSKOcJ&ei=fnUBZ_yKNKyCy9YPseaHkQg&json=",
        "num_citations": 43,
        "citedby_url": "/scholar?cites=16656583931378340017&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:sRBJ0dUSKOcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6287639/8600701/08918438.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Cross aggregation transformer for image restoration",
            "author": [
                "Z Chen",
                "Y Zhang",
                "J Gu",
                "L Kong"
            ],
            "pub_year": "2022",
            "venue": "Advances in Neural …",
            "abstract": "Recently, Transformer architecture has been introduced into  Module to complement the  self-attention mechanism, which  44], we propose a new window attention mechanism as"
        },
        "filled": false,
        "gsrank": 270,
        "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/a37fea8e67f907311826bc1ba2654d97-Abstract-Conference.html",
        "author_id": [
            "nLZtXdgAAAAJ",
            "ORmLjWoAAAAJ",
            "uMQ-G-QAAAAJ",
            "-wm2X-8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:H78rskiR3I4J:scholar.google.com/&output=cite&scirp=269&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D260%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=H78rskiR3I4J&ei=fnUBZ_yKNKyCy9YPseaHkQg&json=",
        "num_citations": 111,
        "citedby_url": "/scholar?cites=10294262589674995487&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:H78rskiR3I4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/a37fea8e67f907311826bc1ba2654d97-Paper-Conference.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "High-performance transformer tracking",
            "author": [
                "X Chen",
                "B Yan",
                "J Zhu",
                "H Lu",
                "X Ruan"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on …",
            "abstract": "an attention-based feature fusion network and pro-  -decoder architecture in the original  transformer because it does  of the transformer and exploit the attention mechanism to design the"
        },
        "filled": false,
        "gsrank": 271,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9999490/",
        "author_id": [
            "A04HWTIAAAAJ",
            "3f8qn4cAAAAJ",
            "j_gYsS8AAAAJ",
            "D3nE0agAAAAJ",
            "2Xfg_gMAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:CBE3VKz-YTYJ:scholar.google.com/&output=cite&scirp=270&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D270%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CBE3VKz-YTYJ&ei=SnYBZ_7lCfbYy9YPrPDH6Aw&json=",
        "num_citations": 26,
        "citedby_url": "/scholar?cites=3918693166889767176&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:CBE3VKz-YTYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/34/4359286/09999490.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Sentiment analysis with adaptive multi-head attention in Transformer",
            "author": [
                "F Meng",
                "D Demeter"
            ],
            "pub_year": "2023",
            "venue": "arXiv preprint arXiv:2310.14505",
            "abstract": "We propose a novel framework based on the attention mechanism  an adaptive multi-head  attention architecture (AdaptAttn)  The multi-head self-attention module is a key component in"
        },
        "filled": false,
        "gsrank": 272,
        "pub_url": "https://arxiv.org/abs/2310.14505",
        "author_id": [
            "",
            "TUnj2lIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:LQ6l-gIhJNgJ:scholar.google.com/&output=cite&scirp=271&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D270%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LQ6l-gIhJNgJ&ei=SnYBZ_7lCfbYy9YPrPDH6Aw&json=",
        "num_citations": 5,
        "citedby_url": "/scholar?cites=15574609708032790061&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:LQ6l-gIhJNgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2310.14505"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Gene transformer: Transformers for the gene expression-based classification of lung cancer subtypes",
            "author": [
                "A Khan",
                "B Lee"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv:2108.11833",
            "abstract": "with a multi-head self-attention module by identifying relevant  attention mechanism, in the  Gene Transformer architecture,  Our findings indicate that employing the attention mechanism"
        },
        "filled": false,
        "gsrank": 273,
        "pub_url": "https://arxiv.org/abs/2108.11833",
        "author_id": [
            "ju2zgioAAAAJ",
            "JqgY430AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:4P7Bv-NcfKQJ:scholar.google.com/&output=cite&scirp=272&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D270%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=4P7Bv-NcfKQJ&ei=SnYBZ_7lCfbYy9YPrPDH6Aw&json=",
        "num_citations": 29,
        "citedby_url": "/scholar?cites=11852450452576796384&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:4P7Bv-NcfKQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2108.11833"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Paraformer: Parallel attention transformer for efficient feature matching",
            "author": [
                "X Lu",
                "Y Yan",
                "B Kang",
                "S Du"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the AAAI Conference on …",
            "abstract": "of quadratic complexity of attention mechanism. So we focus on  In this paper, we propose  a novel attention-based network  attention architecture that not only integrates self-attention"
        },
        "filled": false,
        "gsrank": 274,
        "pub_url": "https://ojs.aaai.org/index.php/AAAI/article/view/25275",
        "author_id": [
            "U6eyWMkAAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:0113o8TaT78J:scholar.google.com/&output=cite&scirp=273&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D270%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0113o8TaT78J&ei=SnYBZ_7lCfbYy9YPrPDH6Aw&json=",
        "num_citations": 15,
        "citedby_url": "/scholar?cites=13785477522495331795&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:0113o8TaT78J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ojs.aaai.org/index.php/AAAI/article/view/25275/25047"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Conformer: Convolution-augmented transformer for speech recognition",
            "author": [
                "A Gulati",
                "J Qin",
                "CC Chiu",
                "N Parmar",
                "Y Zhang"
            ],
            "pub_year": "2020",
            "venue": "arXiv preprint arXiv …",
            "abstract": "as proposed in [6] deploys a feed forward module after the MHSA layer and is composed of   module in our Conformer architecture. The combination of convolution and self-attention has"
        },
        "filled": false,
        "gsrank": 275,
        "pub_url": "https://arxiv.org/abs/2005.08100",
        "author_id": [
            "S2Pk9ooAAAAJ",
            "zkDuH4MAAAAJ",
            "ixhmT3AAAAAJ",
            "q2YXPSgAAAAJ",
            "EilVnKwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:kE7932uf7IAJ:scholar.google.com/&output=cite&scirp=274&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D270%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kE7932uf7IAJ&ei=SnYBZ_7lCfbYy9YPrPDH6Aw&json=",
        "num_citations": 3116,
        "citedby_url": "/scholar?cites=9289975417026727568&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:kE7932uf7IAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2005.08100"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Generating the captions for remote sensing images: A spatial-channel attention based memory-guided transformer approach",
            "author": [
                "GO Gajbhiye",
                "AV Nandedkar"
            ],
            "pub_year": "2022",
            "venue": "Engineering Applications of Artificial …",
            "abstract": "Further, an attention mechanism was integrated with an encoder–decoder architecture to  co- In nutshell, Transformer’s decoder layer relies on MHA module to attend sequence patterns"
        },
        "filled": false,
        "gsrank": 276,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0952197622002317",
        "author_id": [
            "-_FtA_gAAAAJ",
            "nNveFNUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:oyi_zrXjOMsJ:scholar.google.com/&output=cite&scirp=275&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D270%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=oyi_zrXjOMsJ&ei=SnYBZ_7lCfbYy9YPrPDH6Aw&json=",
        "num_citations": 19,
        "citedby_url": "/scholar?cites=14643704558392387747&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:oyi_zrXjOMsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0952197622002317"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention mechanism and context modeling system for text mining machine translation",
            "author": [
                "S Bo",
                "Y Zhang",
                "J Huang",
                "S Liu",
                "Z Chen",
                "Z Li"
            ],
            "pub_year": "2024",
            "venue": "arXiv preprint arXiv …",
            "abstract": "Transformer is its encoder module[30], which is composed of a series of stacked encoder  layers with a consistent architecture , in the multi-head attention mechanism of the Transformer,"
        },
        "filled": false,
        "gsrank": 277,
        "pub_url": "https://arxiv.org/abs/2408.04216",
        "author_id": [
            "",
            "-bjmT4kAAAAJ",
            "N69tJ9gAAAAJ",
            "JfQtXwQAAAAJ",
            "41QvgKwAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:66JHgMaWProJ:scholar.google.com/&output=cite&scirp=276&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D270%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=66JHgMaWProJ&ei=SnYBZ_7lCfbYy9YPrPDH6Aw&json=",
        "num_citations": 14,
        "citedby_url": "/scholar?cites=13420329718910526187&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:66JHgMaWProJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2408.04216"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multimodal transformer fusion for continuous emotion recognition",
            "author": [
                "J Huang",
                "J Tao",
                "B Liu",
                "Z Lian"
            ],
            "pub_year": "2020",
            "venue": "ICASSP 2020-2020 IEEE …",
            "abstract": "It extends conventional attention mechanism to have h multiple  Besides, every multi-head  attention module is followed with  combine the Transformer network and LSTM layer to explore"
        },
        "filled": false,
        "gsrank": 278,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9053762/",
        "author_id": [
            "",
            "",
            "UEB_5QEAAAAJ",
            "S34nWz0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:AovmYwCbRsoJ:scholar.google.com/&output=cite&scirp=277&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D270%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AovmYwCbRsoJ&ei=SnYBZ_7lCfbYy9YPrPDH6Aw&json=",
        "num_citations": 156,
        "citedby_url": "/scholar?cites=14575507670009547522&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:AovmYwCbRsoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9040208/9052899/09053762.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "LiteTransNet: An interpretable approach for landslide displacement prediction using transformer model with attention mechanism",
            "author": [
                "Q Ge",
                "J Li",
                "X Wang",
                "Y Deng",
                "K Zhang",
                "H Sun"
            ],
            "pub_year": "2024",
            "venue": "Engineering Geology",
            "abstract": "Additionally, the Transformer's attention-based feature  our lightweighted Transformer model  simplifies the network architecture  the attention map, we extract the last layer of the attention"
        },
        "filled": false,
        "gsrank": 279,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0013795224000449",
        "author_id": [
            "NSbQJB0AAAAJ",
            "DiFoVRIAAAAJ",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:86vyy3NB0h4J:scholar.google.com/&output=cite&scirp=278&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D270%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=86vyy3NB0h4J&ei=SnYBZ_7lCfbYy9YPrPDH6Aw&json=",
        "num_citations": 5,
        "citedby_url": "/scholar?cites=2220909531845798899&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:86vyy3NB0h4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0013795224000449"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Castling-vit: Compressing self-attention via switching towards linear-angular attention at vision transformer inference",
            "author": [
                "H You",
                "Y Xiong",
                "X Dai",
                "B Wu",
                "P Zhang"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the …",
            "abstract": "In this way, our attention module can be formulated as:  backbones with transformer blocks,  whose architecture could  a new linear-angular attention mechanism: we decompose angular"
        },
        "filled": false,
        "gsrank": 280,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2023/html/You_Castling-ViT_Compressing_Self-Attention_via_Switching_Towards_Linear-Angular_Attention_at_Vision_CVPR_2023_paper.html",
        "author_id": [
            "z5Eku1sAAAAJ",
            "k5FaRwcAAAAJ",
            "u4olrOcAAAAJ",
            "K3QJPdMAAAAJ",
            "eqQQkM4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:k8x1Sr4r0OIJ:scholar.google.com/&output=cite&scirp=279&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D270%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=k8x1Sr4r0OIJ&ei=SnYBZ_7lCfbYy9YPrPDH6Aw&json=",
        "num_citations": 25,
        "citedby_url": "/scholar?cites=16343611144020544659&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:k8x1Sr4r0OIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/You_Castling-ViT_Compressing_Self-Attention_via_Switching_Towards_Linear-Angular_Attention_at_Vision_CVPR_2023_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "SIT: A spatial interaction-aware transformer-based model for freeway trajectory prediction",
            "author": [
                "X Li",
                "J Xia",
                "X Chen",
                "Y Tan",
                "J Chen"
            ],
            "pub_year": "2022",
            "venue": "ISPRS International Journal of Geo …",
            "abstract": "Although Transformer, a multi-head attention-based network, has  Compared to the pooling  methods, the attention mechanism  Although the Transformer architecture can capture longer"
        },
        "filled": false,
        "gsrank": 281,
        "pub_url": "https://www.mdpi.com/2220-9964/11/2/79",
        "author_id": [
            "",
            "z_Sa1wgAAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:L7CdveJoxdcJ:scholar.google.com/&output=cite&scirp=280&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D280%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=L7CdveJoxdcJ&ei=FHcBZ-uFNKyCy9YPseaHkQg&json=",
        "num_citations": 19,
        "citedby_url": "/scholar?cites=15547948611619631151&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:L7CdveJoxdcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2220-9964/11/2/79/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers",
            "author": [
                "MV Ntrougkas",
                "N Gkalelis",
                "V Mezaris"
            ],
            "pub_year": "2024",
            "venue": "IEEE Access",
            "abstract": "images, thanks to its multi-head attention layer, and we confirm  : a trainable attention  mechanism architecture, along with a  the Attention Mechanism • The fusion module of the"
        },
        "filled": false,
        "gsrank": 282,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10539635/",
        "author_id": [
            "amFlIfUAAAAJ",
            "ssjx44oAAAAJ",
            "252ni8gAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:yQYDgtTGpb0J:scholar.google.com/&output=cite&scirp=281&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D280%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yQYDgtTGpb0J&ei=FHcBZ-uFNKyCy9YPseaHkQg&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:yQYDgtTGpb0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6287639/6514899/10539635.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Image captioning using transformer-based double attention network",
            "author": [
                "H Parvin",
                "AR Naghsh-Nilchi",
                "HM Mohammadi"
            ],
            "pub_year": "2023",
            "venue": "Engineering Applications of …",
            "abstract": "problem in transformers, a Masked Self-Attention module is  Therefore, the dropout layer  causes the proposed architecture better  task with a global attention mechanism. The semantic"
        },
        "filled": false,
        "gsrank": 283,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0952197623007297",
        "author_id": [
            "fSOBGboAAAAJ",
            "mnRKQS4AAAAJ",
            "qcZIAA4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:561muDbvEAQJ:scholar.google.com/&output=cite&scirp=282&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D280%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=561muDbvEAQJ&ei=FHcBZ-uFNKyCy9YPseaHkQg&json=",
        "num_citations": 7,
        "citedby_url": "/scholar?cites=292996994080091623&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:561muDbvEAQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0952197623007297"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Choose a transformer: Fourier or galerkin",
            "author": [
                "S Cao"
            ],
            "pub_year": "2021",
            "venue": "Advances in neural information processing systems",
            "abstract": "simple attention-based operator learner, Galerkin Transformer,  66], we have modified the  attention mechanism minimally yet in a  If we apply the regular layer normalization rule that"
        },
        "filled": false,
        "gsrank": 284,
        "pub_url": "https://proceedings.neurips.cc/paper/2021/hash/d0921d442ee91b896ad95059d13df618-Abstract.html",
        "author_id": [
            "XMNDlgwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:1uCKcR-k83YJ:scholar.google.com/&output=cite&scirp=283&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D280%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1uCKcR-k83YJ&ei=FHcBZ-uFNKyCy9YPseaHkQg&json=",
        "num_citations": 166,
        "citedby_url": "/scholar?cites=8571374970772054230&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:1uCKcR-k83YJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper/2021/file/d0921d442ee91b896ad95059d13df618-Paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "MSTCRB: Predicting circRNA-RBP interaction by extracting multi-scale features based on transformer and attention mechanism",
            "author": [
                "Y Zhou",
                "H Cui",
                "D Liu",
                "W Wang"
            ],
            "pub_year": "2024",
            "venue": "International Journal of Biological …",
            "abstract": ", the coding layer can be removed from transformer framework.  , transformer module and  attention mechanism module are  extraction module, an optimized transformer architecture is"
        },
        "filled": false,
        "gsrank": 285,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0141813024056101",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:fL8fWZzMFd8J:scholar.google.com/&output=cite&scirp=284&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D280%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fL8fWZzMFd8J&ei=FHcBZ-uFNKyCy9YPseaHkQg&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:fL8fWZzMFd8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0141813024056101"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Action unit detection by exploiting spatial-temporal and label-wise attention with transformer",
            "author": [
                "L Wang",
                "J Qi",
                "J Cheng",
                "K Suzuki"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the IEEE …",
            "abstract": ", we proposed a transformer based correlation module to learn  We therefore employ attention  mechanism to focus on  18 model, proposed CNN-transformer hybrid architecture obtain"
        },
        "filled": false,
        "gsrank": 286,
        "pub_url": "https://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Wang_Action_Unit_Detection_by_Exploiting_Spatial-Temporal_and_Label-Wise_Attention_With_CVPRW_2022_paper.html",
        "author_id": [
            "",
            "",
            "",
            "MdNb8Z0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:LyYeQoUed_YJ:scholar.google.com/&output=cite&scirp=285&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D280%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LyYeQoUed_YJ&ei=FHcBZ-uFNKyCy9YPseaHkQg&json=",
        "num_citations": 11,
        "citedby_url": "/scholar?cites=17759697213247596079&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:LyYeQoUed_YJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Wang_Action_Unit_Detection_by_Exploiting_Spatial-Temporal_and_Label-Wise_Attention_With_CVPRW_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Sparse MLP for image recognition: Is self-attention really necessary?",
            "author": [
                "C Tang",
                "Y Zhao",
                "G Wang",
                "C Luo",
                "W Xie"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the AAAI …",
            "abstract": "whether the core self-attention module in Transformer is the key  (a) illustrates the overall  architecture of our designed network.  this fusion module with concatenation and a FC layer:"
        },
        "filled": false,
        "gsrank": 287,
        "pub_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20133",
        "author_id": [
            "3ZC8B7MAAAAJ",
            "QWemjjQAAAAJ",
            "cKY8e8sAAAAJ",
            "01iBf38AAAAJ",
            "7vjHnasAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:LlAqmYez-AAJ:scholar.google.com/&output=cite&scirp=286&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D280%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LlAqmYez-AAJ&ei=FHcBZ-uFNKyCy9YPseaHkQg&json=",
        "num_citations": 97,
        "citedby_url": "/scholar?cites=70003189195886638&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:LlAqmYez-AAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20133/19892"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Spectral–spatial morphological attention transformer for hyperspectral image classification",
            "author": [
                "SK Roy",
                "A Deria",
                "C Shah",
                "JM Haut"
            ],
            "pub_year": "2023",
            "venue": "IEEE Transactions on …",
            "abstract": "(in conjunction with the attention mechanism) to improve the  a transformer encoder module  and utilizing adjacent bands.  transformer architecture search subject to find out the optimal"
        },
        "filled": false,
        "gsrank": 288,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10036472/",
        "author_id": [
            "1WVrFGwAAAAJ",
            "tPBmwzQAAAAJ",
            "FOyY21MAAAAJ",
            "zLffeY8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:XAS34LC09DoJ:scholar.google.com/&output=cite&scirp=287&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D280%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XAS34LC09DoJ&ei=FHcBZ-uFNKyCy9YPseaHkQg&json=",
        "num_citations": 105,
        "citedby_url": "/scholar?cites=4248219020294554716&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:XAS34LC09DoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/36/4358825/10036472.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Decoding selective auditory attention with EEG using a transformer model",
            "author": [
                "Z Xu",
                "Y Bai",
                "R Zhao",
                "H Hu",
                "G Ni",
                "D Ming"
            ],
            "pub_year": "2022",
            "venue": "Methods",
            "abstract": "-decoder architecture model for auditory attention detection  that introducing an attention  mechanism will help improve  attention- mechanism, including a temporal self-attention module"
        },
        "filled": false,
        "gsrank": 289,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1046202322000986",
        "author_id": [
            "",
            "Zx_VJ3YAAAAJ",
            "",
            "",
            "6G_C-ocAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:6QSpTSQDH-EJ:scholar.google.com/&output=cite&scirp=288&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D280%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6QSpTSQDH-EJ&ei=FHcBZ-uFNKyCy9YPseaHkQg&json=",
        "num_citations": 19,
        "citedby_url": "/scholar?cites=16221687837268444393&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:6QSpTSQDH-EJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1046202322000986"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Image captioning through image transformer",
            "author": [
                "S He",
                "W Liao",
                "HR Tavakoli",
                "M Yang"
            ],
            "pub_year": "2020",
            "venue": "Proceedings of the …",
            "abstract": "use a recurrent neural network with attention mechanism in the decoding  architecture for  the transformer layer adapted to the image captioning task, with a modified attention module"
        },
        "filled": false,
        "gsrank": 290,
        "pub_url": "http://openaccess.thecvf.com/content/ACCV2020/html/He_Image_Captioning_through_Image_Transformer_ACCV_2020_paper.html",
        "author_id": [
            "",
            "aEX6tgEAAAAJ",
            "auVrkIkAAAAJ",
            "lgGmYBoAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:YYke6LKEzsoJ:scholar.google.com/&output=cite&scirp=289&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D280%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YYke6LKEzsoJ&ei=FHcBZ-uFNKyCy9YPseaHkQg&json=",
        "num_citations": 127,
        "citedby_url": "/scholar?cites=14613763744797198689&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:YYke6LKEzsoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/ACCV2020/papers/He_Image_Captioning_through_Image_Transformer_ACCV_2020_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Explicit sparse transformer: Concentrated attention through explicit selection",
            "author": [
                "G Zhao",
                "J Lin",
                "Z Zhang",
                "X Ren",
                "Q Su",
                "X Sun"
            ],
            "pub_year": "2019",
            "venue": "arXiv preprint arXiv …",
            "abstract": "attention mechanism and the attention-based framework of  we propose a novel model,  Explicit Sparse Transformer, which  of the attention at the top layer of the vanilla Transformer, and"
        },
        "filled": false,
        "gsrank": 291,
        "pub_url": "https://arxiv.org/abs/1912.11637",
        "author_id": [
            "0IXhrDMAAAAJ",
            "qp6IwtgAAAAJ",
            "gSEzCUkAAAAJ",
            "",
            "9f4JUrUAAAAJ",
            "tpXiQkYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:0Op0iDaatPUJ:scholar.google.com/&output=cite&scirp=290&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D290%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0Op0iDaatPUJ&ei=4HcBZ8blCsDBy9YP683bgAE&json=",
        "num_citations": 115,
        "citedby_url": "/scholar?cites=17704945594108537552&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:0Op0iDaatPUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/1912.11637"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A convolutional transformer architecture for remaining useful life estimation",
            "author": [
                "Y Ding",
                "M Jia"
            ],
            "pub_year": "2021",
            "venue": "2021 Global Reliability and Prognostics and …",
            "abstract": "global context capturing of attention mechanism with the local  convolutional module to the  vanilla Transformer architecture with a  One limitation of using local receptive field layer is that"
        },
        "filled": false,
        "gsrank": 292,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9612814/",
        "author_id": [
            "6UNuAmgAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:jhoxEJFQy-IJ:scholar.google.com/&output=cite&scirp=291&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D290%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jhoxEJFQy-IJ&ei=4HcBZ8blCsDBy9YP683bgAE&json=",
        "num_citations": 16,
        "citedby_url": "/scholar?cites=16342244256816110222&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:jhoxEJFQy-IJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.researchgate.net/profile/Yifei-Ding-3/publication/356514113_A_Convolutional_Transformer_Architecture_for_Remaining_Useful_Life_Estimation/links/619fa3e8f8565a76fdf46fbf/A-Convolutional-Transformer-Architecture-for-Remaining-Useful-Life-Estimation.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Sanvis: Visual analytics for understanding self-attention networks",
            "author": [
                "C Park",
                "I Na",
                "Y Jo",
                "S Shin",
                "J Yoo"
            ],
            "pub_year": "2019",
            "venue": "2019 IEEE …",
            "abstract": "inspired by humans’ attention mechanism, have seen  -of-the-art self-attention model  called Transformer, we demon self-attention module originally proposed in Transformer [26]."
        },
        "filled": false,
        "gsrank": 293,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/8933677/",
        "author_id": [
            "nwU418UAAAAJ",
            "",
            "",
            "uqStCsUAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:FixiDu2p1o8J:scholar.google.com/&output=cite&scirp=292&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D290%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FixiDu2p1o8J&ei=4HcBZ8blCsDBy9YP683bgAE&json=",
        "num_citations": 28,
        "citedby_url": "/scholar?cites=10364658428053433366&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:FixiDu2p1o8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/1909.09595"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "MTAtrack: Multilevel transformer attention for visual tracking",
            "author": [
                "D An",
                "F Zhang",
                "Y Zhao",
                "B Luo",
                "C Yang",
                "B Chen"
            ],
            "pub_year": "2023",
            "venue": "Optics & Laser …",
            "abstract": "networks based on the Transformer attention mechanism  The overall architecture of the  MTAtrack network mainly  model, we connect every layer output of the self-attention module,"
        },
        "filled": false,
        "gsrank": 294,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0030399223005522",
        "author_id": [
            "",
            "",
            "",
            "YKgO7ZQAAAAJ",
            "39DpNi0AAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Yj6PwqUYkSsJ:scholar.google.com/&output=cite&scirp=293&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D290%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Yj6PwqUYkSsJ&ei=4HcBZ8blCsDBy9YP683bgAE&json=",
        "num_citations": 2,
        "citedby_url": "/scholar?cites=3139317515466784354&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Yj6PwqUYkSsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0030399223005522"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer and group parallel axial attention co-encoder for medical image segmentation",
            "author": [
                "C Li",
                "L Wang",
                "Y Li"
            ],
            "pub_year": "2022",
            "venue": "Scientific Reports",
            "abstract": "Specifically, we propose a new attention mechanism to  The network is based on U-shape  architecture. The encoder  To further preserve global information, we introduce a module at the"
        },
        "filled": false,
        "gsrank": 295,
        "pub_url": "https://www.nature.com/articles/s41598-022-20440-z",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:lOQHnkDX2_AJ:scholar.google.com/&output=cite&scirp=294&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D290%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lOQHnkDX2_AJ&ei=4HcBZ8blCsDBy9YP683bgAE&json=",
        "num_citations": 12,
        "citedby_url": "/scholar?cites=17355702261531534484&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:lOQHnkDX2_AJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.nature.com/articles/s41598-022-20440-z.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Asymmetric cross-attention hierarchical network based on CNN and transformer for bitemporal remote sensing images change detection",
            "author": [
                "X Zhang",
                "S Cheng",
                "L Wang",
                "H Li"
            ],
            "pub_year": "2023",
            "venue": "IEEE Transactions on …",
            "abstract": "a transformer module in the UNet architecture to form a CNN- CNN with the Transformer in  parallel within the module. In  the asymmetric multi-head cross-attention mechanism is critical"
        },
        "filled": false,
        "gsrank": 296,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10045704/",
        "author_id": [
            "",
            "YQMoxi0AAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:URHpmaYSis8J:scholar.google.com/&output=cite&scirp=295&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D290%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=URHpmaYSis8J&ei=4HcBZ8blCsDBy9YP683bgAE&json=",
        "num_citations": 77,
        "citedby_url": "/scholar?cites=14954786019393212753&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:URHpmaYSis8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/36/4358825/10045704.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Sotr: Segmenting objects with transformers",
            "author": [
                "R Guo",
                "D Niu",
                "L Qu",
                "Z Li"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the IEEE/CVF …",
            "abstract": "propose the twin attention mechanism to simplify the attention matrix  by transformer on  the multi-level upsampling module. As  We denote the backbone architecture with network-depth-"
        },
        "filled": false,
        "gsrank": 297,
        "pub_url": "http://openaccess.thecvf.com/content/ICCV2021/html/Guo_SOTR_Segmenting_Objects_With_Transformers_ICCV_2021_paper.html",
        "author_id": [
            "hMWIp6MAAAAJ",
            "AzlUrvUAAAAJ",
            "IDbqDdEAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:ggKmKvM6DIsJ:scholar.google.com/&output=cite&scirp=296&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D290%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ggKmKvM6DIsJ&ei=4HcBZ8blCsDBy9YP683bgAE&json=",
        "num_citations": 117,
        "citedby_url": "/scholar?cites=10019448087059497602&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:ggKmKvM6DIsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_SOTR_Segmenting_Objects_With_Transformers_ICCV_2021_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A hybrid multi-scale attention convolution and aging transformer network for Alzheimer's disease diagnosis",
            "author": [
                "X Gao",
                "H Cai",
                "M Liu"
            ],
            "pub_year": "2023",
            "venue": "IEEE Journal of Biomedical and Health …",
            "abstract": "Attention mechanism has attracted a lot of interest to enhance  Architecture of our proposed  aging transformer subnetwork  of the attention module and aging transformer for disease"
        },
        "filled": false,
        "gsrank": 298,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10109788/",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:ukH2frEIZ1wJ:scholar.google.com/&output=cite&scirp=297&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D290%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ukH2frEIZ1wJ&ei=4HcBZ8blCsDBy9YP683bgAE&json=",
        "num_citations": 17,
        "citedby_url": "/scholar?cites=6658300132522869178&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:ukH2frEIZ1wJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6221020/6363502/10109788.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer-based Monocular Depth Estimation with Attention Supervision.",
            "author": [
                "W Chang",
                "Y Zhang",
                "Z Xiong"
            ],
            "pub_year": "2021",
            "venue": "BMVC",
            "abstract": "To this end, we propose Attention-based Up-sample Block (AUB) to compensate  attention  mechanism to depth estimation, the attention maps extracted from the last  in our architecture."
        },
        "filled": false,
        "gsrank": 299,
        "pub_url": "https://www.bmvc2021-virtualconference.com/assets/papers/0244.pdf",
        "author_id": [
            "VbY0304AAAAJ",
            "LatWlFAAAAAJ",
            "Snl0HPEAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:O_ibPnYBXi8J:scholar.google.com/&output=cite&scirp=298&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D290%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=O_ibPnYBXi8J&ei=4HcBZ8blCsDBy9YP683bgAE&json=",
        "num_citations": 19,
        "citedby_url": "/scholar?cites=3413167174961592379&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:O_ibPnYBXi8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.bmvc2021-virtualconference.com/assets/papers/0244.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Re-attention transformer for weakly supervised object localization",
            "author": [
                "H Su",
                "Y Ye",
                "Z Chen",
                "M Song",
                "L Cheng"
            ],
            "pub_year": "2022",
            "venue": "arXiv preprint arXiv:2208.01838",
            "abstract": "a reattention mechanism termed token refinement transformer ( tokens from each transformer  layer to compensate for the  ablations about the re-attention module. Both qualitative and"
        },
        "filled": false,
        "gsrank": 300,
        "pub_url": "https://arxiv.org/abs/2208.01838",
        "author_id": [
            "",
            "",
            "",
            "7oLbhAwAAAAJ",
            "PKFAv-cAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:EH8kJ5bTnoQJ:scholar.google.com/&output=cite&scirp=299&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D290%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EH8kJ5bTnoQJ&ei=4HcBZ8blCsDBy9YP683bgAE&json=",
        "num_citations": 15,
        "citedby_url": "/scholar?cites=9556308101182029584&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:EH8kJ5bTnoQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2208.01838"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Learning texture transformer network for image super-resolution",
            "author": [
                "F Yang",
                "H Yang",
                "J Fu",
                "H Lu"
            ],
            "pub_year": "2020",
            "venue": "Proceedings of the IEEE …",
            "abstract": "of the first to introduce the transformer architecture into image generation tasks attention  module to transfer the HR texture features V from the Ref image. Traditional attention mechanism"
        },
        "filled": false,
        "gsrank": 301,
        "pub_url": "http://openaccess.thecvf.com/content_CVPR_2020/html/Yang_Learning_Texture_Transformer_Network_for_Image_Super-Resolution_CVPR_2020_paper.html",
        "author_id": [
            "OL-HiBAAAAAJ",
            "O8NTlooAAAAJ",
            "-WqSwu8AAAAJ",
            "GtNuBJcAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:BJtbuNiq-ZYJ:scholar.google.com/&output=cite&scirp=300&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D300%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BJtbuNiq-ZYJ&ei=qngBZ7-OLKSMy9YPoKv0mAY&json=",
        "num_citations": 947,
        "citedby_url": "/scholar?cites=10878914222672812804&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:BJtbuNiq-ZYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_Texture_Transformer_Network_for_Image_Super-Resolution_CVPR_2020_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention weights in transformer NMT fail aligning words between sequences but largely explain model predictions",
            "author": [
                "J Ferrando",
                "MR Costa-jussà"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv:2109.05853",
            "abstract": "Transformer architecture in the Neural Machine Translation (NMT) setting. Focusing on the  encoder-decoder attention mechanism, we prove that attention  -decoder attention module is"
        },
        "filled": false,
        "gsrank": 302,
        "pub_url": "https://arxiv.org/abs/2109.05853",
        "author_id": [
            "ZNsw8ZUAAAAJ",
            "ESqQ7FoAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:CGvxU9JJblcJ:scholar.google.com/&output=cite&scirp=301&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D300%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CGvxU9JJblcJ&ei=qngBZ7-OLKSMy9YPoKv0mAY&json=",
        "num_citations": 17,
        "citedby_url": "/scholar?cites=6300054096438192904&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:CGvxU9JJblcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2109.05853"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Evolving attention with residual convolutions",
            "author": [
                "Y Wang",
                "Y Yang",
                "J Bai",
                "M Zhang",
                "J Bai"
            ],
            "pub_year": "2021",
            "venue": "International …",
            "abstract": "The attention maps are indispensable for a transformer model to  • We propose a novel  evolving attention mechanism aug module that generalizes attention maps in the current layer"
        },
        "filled": false,
        "gsrank": 303,
        "pub_url": "https://proceedings.mlr.press/v139/wang21ab.html",
        "author_id": [
            "YgL4rywAAAAJ",
            "",
            "Hv19hrgAAAAJ",
            "",
            "Z9EG7qgAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:_Us8Dq8XcpYJ:scholar.google.com/&output=cite&scirp=302&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D300%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_Us8Dq8XcpYJ&ei=qngBZ7-OLKSMy9YPoKv0mAY&json=",
        "num_citations": 30,
        "citedby_url": "/scholar?cites=10840753293659753469&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:_Us8Dq8XcpYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://proceedings.mlr.press/v139/wang21ab/wang21ab.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Improving transformer-based networks with locality for automatic speaker verification",
            "author": [
                "M Sang",
                "Y Zhao",
                "G Liu",
                "JHL Hansen"
            ],
            "pub_year": "2023",
            "venue": "ICASSP 2023-2023 …",
            "abstract": "layer allows CNNs to model the local dependencies well, but it  architecture of Conformer  block is shown in the left side of Fig. 1(b). It consists of multi-head self-attention (MSA) module"
        },
        "filled": false,
        "gsrank": 304,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10096333/",
        "author_id": [
            "72W8uPwAAAAJ",
            "",
            "U4CeTeAAAAAJ",
            "hfADwdIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:tBH0GThx4rgJ:scholar.google.com/&output=cite&scirp=303&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D300%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tBH0GThx4rgJ&ei=qngBZ7-OLKSMy9YPoKv0mAY&json=",
        "num_citations": 15,
        "citedby_url": "/scholar?cites=13322335133482881460&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:tBH0GThx4rgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/10094559/10094560/10096333.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer-based local-global guidance for image captioning",
            "author": [
                "H Parvin",
                "AR Naghsh-Nilchi",
                "HM Mohammadi"
            ],
            "pub_year": "2023",
            "venue": "Expert Systems with …",
            "abstract": "using a single attention mechanism. In this paper, a new  present a new transformer-based  architecture without recurrence  a generator network and a selector module to collaboratively"
        },
        "filled": false,
        "gsrank": 305,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0957417423002750",
        "author_id": [
            "fSOBGboAAAAJ",
            "mnRKQS4AAAAJ",
            "qcZIAA4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:R2CCLyI5LC0J:scholar.google.com/&output=cite&scirp=304&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D300%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=R2CCLyI5LC0J&ei=qngBZ7-OLKSMy9YPoKv0mAY&json=",
        "num_citations": 15,
        "citedby_url": "/scholar?cites=3255039449670770759&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:R2CCLyI5LC0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0957417423002750"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Reducing carbon emissions in the architectural design process via transformer with cross-attention mechanism",
            "author": [
                "HD Li",
                "X Yang",
                "HL Zhu"
            ],
            "pub_year": "2023",
            "venue": "Frontiers in Ecology and Evolution",
            "abstract": "using an efficient neural network with transformer grouping. The  a CA-Transformer model  with a cross-attention mechanism to aid  on the Transformer module. We selected Time Series"
        },
        "filled": false,
        "gsrank": 306,
        "pub_url": "https://www.frontiersin.org/articles/10.3389/fevo.2023.1249308/full",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:rZyBD_5AFUAJ:scholar.google.com/&output=cite&scirp=305&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D300%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rZyBD_5AFUAJ&ei=qngBZ7-OLKSMy9YPoKv0mAY&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:rZyBD_5AFUAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.frontiersin.org/articles/10.3389/fevo.2023.1249308/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Guideformer: Transformers for image guided depth completion",
            "author": [
                "K Rho",
                "J Ha",
                "Y Kim"
            ],
            "pub_year": "2022",
            "venue": "… of the IEEE/CVF Conference on …",
            "abstract": ", a fully transformer-based architecture for dense depth com based on guided-attention  mechanism. It explicitly models  We introduce a guided-attention module (GAM) by extending"
        },
        "filled": false,
        "gsrank": 307,
        "pub_url": "https://openaccess.thecvf.com/content/CVPR2022/html/Rho_GuideFormer_Transformers_for_Image_Guided_Depth_Completion_CVPR_2022_paper.html?ref=https://githubhelp.com",
        "author_id": [
            "_izQenUAAAAJ",
            "",
            "vWMasLQAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:0Ue6UrVp4VsJ:scholar.google.com/&output=cite&scirp=306&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D300%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=0Ue6UrVp4VsJ&ei=qngBZ7-OLKSMy9YPoKv0mAY&json=",
        "num_citations": 46,
        "citedby_url": "/scholar?cites=6620689154709276625&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:0Ue6UrVp4VsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Rho_GuideFormer_Transformers_for_Image_Guided_Depth_Completion_CVPR_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Can vision transformers perform convolution?",
            "author": [
                "S Li",
                "X Chen",
                "D He",
                "CJ Hsieh"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv:2111.01353",
            "abstract": "that attention-based networks, such as Vision Transformer ( , where the multi-head attention  mechanism and the relative  have some constraints on the ViT architecture. In particular, we"
        },
        "filled": false,
        "gsrank": 308,
        "pub_url": "https://arxiv.org/abs/2111.01353",
        "author_id": [
            "-9NCUG8AAAAJ",
            "vNcBx1sAAAAJ",
            "orVoz4IAAAAJ",
            "Wy89g4IAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:HzGJoho1swsJ:scholar.google.com/&output=cite&scirp=307&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D300%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HzGJoho1swsJ&ei=qngBZ7-OLKSMy9YPoKv0mAY&json=",
        "num_citations": 19,
        "citedby_url": "/scholar?cites=843075943760736543&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:HzGJoho1swsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2111.01353"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multi-modal motion prediction with transformer-based neural network for autonomous driving",
            "author": [
                "Z Huang",
                "X Mo",
                "C Lv"
            ],
            "pub_year": "2022",
            "venue": "2022 International Conference on …",
            "abstract": "We also demonstrate that the multi-modal attention module can automatically  propose to  modify the multi-head attention mechanism in the Transformer to multi-modal  architecture. The"
        },
        "filled": false,
        "gsrank": 309,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9812060/",
        "author_id": [
            "aLZEVCsAAAAJ",
            "JUYVmAQAAAAJ",
            "UKVs2CEAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:b1KO4g3z5RcJ:scholar.google.com/&output=cite&scirp=308&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D300%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=b1KO4g3z5RcJ&ei=qngBZ7-OLKSMy9YPoKv0mAY&json=",
        "num_citations": 109,
        "citedby_url": "/scholar?cites=1722049673500185199&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:b1KO4g3z5RcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9811522/9811357/09812060.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "End-to-end dense video captioning with masked transformer",
            "author": [
                "L Zhou",
                "Y Zhou",
                "JJ Corso"
            ],
            "pub_year": "2018",
            "venue": "Proceedings of the …",
            "abstract": "networks and selfattention  an attention mechanism within a module and is a potential way  to learn this long-range dependence. In selfattention the higher layer in the same module is"
        },
        "filled": false,
        "gsrank": 310,
        "pub_url": "http://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_End-to-End_Dense_Video_CVPR_2018_paper.html",
        "author_id": [
            "M-3cIR0AAAAJ",
            "H_6RQ7oAAAAJ",
            "g9bV-_sAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:-m-sLU6TOpYJ:scholar.google.com/&output=cite&scirp=309&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D300%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-m-sLU6TOpYJ&ei=qngBZ7-OLKSMy9YPoKv0mAY&json=",
        "num_citations": 653,
        "citedby_url": "/scholar?cites=10825126618321416186&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:-m-sLU6TOpYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_End-to-End_Dense_Video_CVPR_2018_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Efficient attention: Attention with linear complexities",
            "author": [
                "Z Shen",
                "M Zhang",
                "H Zhao",
                "S Yi"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the IEEE …",
            "abstract": "efficient attention mechanism equivalent to dot-product attention but  The Transformer  architecture is highly successful on  ) module [10] uses global average pooling and a linear layer"
        },
        "filled": false,
        "gsrank": 311,
        "pub_url": "http://openaccess.thecvf.com/content/WACV2021/html/Shen_Efficient_Attention_Attention_With_Linear_Complexities_WACV_2021_paper.html",
        "author_id": [
            "vrj_PtkAAAAJ",
            "2QLD4fAAAAAJ",
            "sMQV1ecAAAAJ",
            "afbbNmwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:9jG99EJDatsJ:scholar.google.com/&output=cite&scirp=310&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D310%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9jG99EJDatsJ&ei=dHkBZ_2sIsDBy9YP683bgAE&json=",
        "num_citations": 482,
        "citedby_url": "/scholar?cites=15810523396690489846&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:9jG99EJDatsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/WACV2021/papers/Shen_Efficient_Attention_Attention_With_Linear_Complexities_WACV_2021_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multi-Scale Transformer and Attention Mechanism for Magnetic Spatiotemporal Sequence Localization",
            "author": [
                "Q Wang",
                "L Wang",
                "M Fu",
                "J Wang",
                "L Sun"
            ],
            "pub_year": "2024",
            "venue": "IEEE Internet of …",
            "abstract": "4) Localization module includes an FC layer activated by the  noises, we design a data  denoising module based on DAEs.  method with a client-server architecture. The client side was"
        },
        "filled": false,
        "gsrank": 312,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10436406/",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:M8Vn0d4KDEAJ:scholar.google.com/&output=cite&scirp=311&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D310%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=M8Vn0d4KDEAJ&ei=dHkBZ_2sIsDBy9YP683bgAE&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=4615075670260172083&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:M8Vn0d4KDEAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6488907/6702522/10436406.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Spatial-channel transformer network for trajectory prediction on the traffic scenes",
            "author": [
                "J Zhao",
                "X Li",
                "Q Xue",
                "W Zhang"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv:2101.11472",
            "abstract": "A channel-wise module is inserted to measure the social  Recently the transformer architecture  has also been  use complete attention mechanism to process time-series data and model"
        },
        "filled": false,
        "gsrank": 313,
        "pub_url": "https://arxiv.org/abs/2101.11472",
        "author_id": [
            "",
            "J1AZJnYAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Z5kiEWlCGUAJ:scholar.google.com/&output=cite&scirp=312&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D310%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Z5kiEWlCGUAJ&ei=dHkBZ_2sIsDBy9YP683bgAE&json=",
        "num_citations": 18,
        "citedby_url": "/scholar?cites=4618795911871633767&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Z5kiEWlCGUAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2101.11472"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "ACTNet: Attention based CNN and Transformer network for respiratory rate estimation",
            "author": [
                "H Chen",
                "X Zhang",
                "Z Guo",
                "N Ying",
                "M Yang"
            ],
            "pub_year": "2024",
            "venue": "… Signal Processing and …",
            "abstract": "spatial attention, so we use a local attention mechanism in  (EDC) module and transformer  encoder architecture. Finally, a  in the Transformer branch, and the linear projection layer is a"
        },
        "filled": false,
        "gsrank": 314,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S174680942400555X",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:WaZvwvcE1nMJ:scholar.google.com/&output=cite&scirp=313&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D310%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WaZvwvcE1nMJ&ei=dHkBZ_2sIsDBy9YP683bgAE&json=",
        "num_citations": 2,
        "citedby_url": "/scholar?cites=8346864421543323225&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:WaZvwvcE1nMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S174680942400555X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Machine Translation Using Improved Attention-based Transformer with Hybrid Input",
            "author": [
                "M Abrishami",
                "MJ Rashti"
            ],
            "pub_year": "2020",
            "venue": "2020 6th International …",
            "abstract": "In this study, an attention-based deep learning architecture is proposed for MT, with all   connections from the encoder output to the decoder's first layer). Attention Mechanism allows the"
        },
        "filled": false,
        "gsrank": 315,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9122317/",
        "author_id": [
            "",
            "_rvMG5EAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:eG1ZZsatYb4J:scholar.google.com/&output=cite&scirp=314&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D310%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eG1ZZsatYb4J&ei=dHkBZ_2sIsDBy9YP683bgAE&json=",
        "num_citations": 6,
        "citedby_url": "/scholar?cites=13718437007579508088&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:eG1ZZsatYb4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9118608/9122268/09122317.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer networks for trajectory forecasting",
            "author": [
                "F Giuliari",
                "I Hasan",
                "M Cristani"
            ],
            "pub_year": "2021",
            "venue": "2020 25th international …",
            "abstract": "only-attention-based memory mechanisms of Transformers.  , weighting them according  to an attention mechanism.  Compared to these techniques, our transformer architecture"
        },
        "filled": false,
        "gsrank": 316,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9412190/",
        "author_id": [
            "EOU10lkAAAAJ",
            "G-4aeV4AAAAJ",
            "LbgTPRwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:bpLQdraWvQwJ:scholar.google.com/&output=cite&scirp=315&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D310%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bpLQdraWvQwJ&ei=dHkBZ_2sIsDBy9YP683bgAE&json=",
        "num_citations": 446,
        "citedby_url": "/scholar?cites=918055609475043950&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:bpLQdraWvQwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9411940/9411911/09412190.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Energy transformer",
            "author": [
                "B Hoover",
                "Y Liang",
                "B Pham",
                "R Panda"
            ],
            "pub_year": "2024",
            "venue": "Advances in …",
            "abstract": "a novel architecture, called the Energy Transformer (or ET for  of the energy attention  mechanism and the corresponding  matrix in the Hopfield Network module (HN) of our model."
        },
        "filled": false,
        "gsrank": 317,
        "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/57a9b97477b67936298489e3c1417b0a-Abstract-Conference.html",
        "author_id": [
            "n10P0tYAAAAJ",
            "yVhSIBcAAAAJ",
            "m96Cuh8AAAAJ",
            "_ySuu6gAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:ta2oxYlimWQJ:scholar.google.com/&output=cite&scirp=316&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D310%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ta2oxYlimWQJ&ei=dHkBZ_2sIsDBy9YP683bgAE&json=",
        "num_citations": 28,
        "citedby_url": "/scholar?cites=7248933419095731637&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:ta2oxYlimWQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/57a9b97477b67936298489e3c1417b0a-Paper-Conference.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "MALS-Net: A multi-head attention-based LSTM sequence-to-sequence network for socio-temporal interaction modelling and trajectory prediction",
            "author": [
                "F Hasan",
                "H Huang"
            ],
            "pub_year": "2023",
            "venue": "Sensors",
            "abstract": ", the multi-head attention mechanism of the transformer does  data, we propose another  multi-head attention layer (TMHA),  architecture as a replacement for the traditional transformer"
        },
        "filled": false,
        "gsrank": 318,
        "pub_url": "https://www.mdpi.com/1424-8220/23/1/530",
        "author_id": [
            "-OJ9HeMAAAAJ",
            "ulsViyoAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:AO23kabHf44J:scholar.google.com/&output=cite&scirp=317&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D310%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AO23kabHf44J&ei=dHkBZ_2sIsDBy9YP683bgAE&json=",
        "num_citations": 19,
        "citedby_url": "/scholar?cites=10268145193651268864&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:AO23kabHf44J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/1424-8220/23/1/530/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "VSNet: classification of pulmonary nodules in 3D using vision transformer and sequence spatial attention mechanism",
            "author": [
                "D Tang",
                "T Xiao",
                "F Yang",
                "C Zhang",
                "Z Wang"
            ],
            "pub_year": "2024",
            "venue": "Multimedia Tools and …",
            "abstract": "Differently, we introduce the SSAM module to improve the model’ We have presented an  overview of VSNet architecture in Fig.  last Transformer layer, we utilize a de-convolution layer to"
        },
        "filled": false,
        "gsrank": 319,
        "pub_url": "https://link.springer.com/article/10.1007/s11042-024-19475-3",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:xNgpeMgBc-YJ:scholar.google.com/&output=cite&scirp=318&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D310%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xNgpeMgBc-YJ&ei=dHkBZ_2sIsDBy9YP683bgAE&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:xNgpeMgBc-YJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s11042-024-19475-3.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "… : A Multi-Branch Fault Diagnosis Method for Scroll Compressors Using Swin Transformer Sliding Window, Shallow ResNet, and Global Attention Mechanism  …",
            "author": [
                "Z Xu",
                "T Liu",
                "Z Xia",
                "Y Fan",
                "M Yan",
                "X Dang"
            ],
            "pub_year": "2024",
            "venue": "Sensors",
            "abstract": "of the module via identity mapping, ensuring that each layer  a fully attention-based design,  the Swin Transformer model  the architecture of the proposed multi-channel SSG-Net model"
        },
        "filled": false,
        "gsrank": 320,
        "pub_url": "https://www.mdpi.com/1424-8220/24/19/6237",
        "author_id": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:F2U05J7g3vEJ:scholar.google.com/&output=cite&scirp=319&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D310%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=F2U05J7g3vEJ&ei=dHkBZ_2sIsDBy9YP683bgAE&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:F2U05J7g3vEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/1424-8220/24/19/6237"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "COVID-19 diagnosis based on swin transformer model with demographic information fusion and enhanced multi-head attention mechanism",
            "author": [
                "Y Sun",
                "J Lian",
                "Z Teng",
                "Z Wei",
                "Y Tang",
                "L Yang"
            ],
            "pub_year": "2024",
            "venue": "Expert Systems with …",
            "abstract": "of the Swin Transformer layer are retained to reconstruct a regression model that predict the   In order to test the effectiveness of each module and compare our model with other networks"
        },
        "filled": false,
        "gsrank": 321,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0957417423033079",
        "author_id": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:7ffMV1G1QlsJ:scholar.google.com/&output=cite&scirp=320&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D320%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7ffMV1G1QlsJ&ei=PnoBZ4u1OqSMy9YPoKv0mAY&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=6576017766884374509&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:7ffMV1G1QlsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0957417423033079"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "RDTNet: A residual deformable attention based transformer network for breast cancer classification",
            "author": [
                "DR Nayak"
            ],
            "pub_year": "2024",
            "venue": "Expert Systems with Applications",
            "abstract": "multi-head deformable self-attention  CNN architecture and a residual deformable transformer  layer. The proposed RDTNet enjoys the benefits of both CNN and ViT, allowing the model"
        },
        "filled": false,
        "gsrank": 322,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0957417424004342",
        "author_id": [
            "ciWMFiAAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:mWZG3ECmqxgJ:scholar.google.com/&output=cite&scirp=321&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D320%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mWZG3ECmqxgJ&ei=PnoBZ4u1OqSMy9YPoKv0mAY&json=",
        "num_citations": 4,
        "citedby_url": "/scholar?cites=1777697275431511705&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:mWZG3ECmqxgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0957417424004342"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Entangled transformer for image captioning",
            "author": [
                "G Li",
                "L Zhu",
                "P Liu",
                "Y Yang"
            ],
            "pub_year": "2019",
            "venue": "Proceedings of the IEEE/CVF …",
            "abstract": "attention module can be readily applied to the Transformer  As shown in Figure 2, the  overall architecture follows the  To mimic the attention mechanism of the human vision system"
        },
        "filled": false,
        "gsrank": 323,
        "pub_url": "http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.html",
        "author_id": [
            "McEfO8UAAAAJ",
            "9ZukE28AAAAJ",
            "KRz4JecAAAAJ",
            "RMSuNFwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:yUw1A2R-LMUJ:scholar.google.com/&output=cite&scirp=322&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D320%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yUw1A2R-LMUJ&ei=PnoBZ4u1OqSMy9YPoKv0mAY&json=",
        "num_citations": 402,
        "citedby_url": "/scholar?cites=14207869892462726345&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:yUw1A2R-LMUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer for single image super-resolution",
            "author": [
                "Z Lu",
                "J Li",
                "H Liu",
                "C Huang",
                "L Zhang"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the …",
            "abstract": "Inspired by the high-pass filter, we design a High-frequency Filtering Module (HFM) to capture   residual network with residualin-residual architecture and channel attention mechanism."
        },
        "filled": false,
        "gsrank": 324,
        "pub_url": "https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Lu_Transformer_for_Single_Image_Super-Resolution_CVPRW_2022_paper.html",
        "author_id": [
            "",
            "a5jkbmkAAAAJ",
            "4CQKG8oAAAAJ",
            "Sun7dRgAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:O5LJgXU4FaYJ:scholar.google.com/&output=cite&scirp=323&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D320%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=O5LJgXU4FaYJ&ei=PnoBZ4u1OqSMy9YPoKv0mAY&json=",
        "num_citations": 376,
        "citedby_url": "/scholar?cites=11967533662146761275&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:O5LJgXU4FaYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Lu_Transformer_for_Single_Image_Super-Resolution_CVPRW_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "… framework for disease detection in video capsule endoscopy images using a vision transformer and convolutional neural network with a specific attention mechanism",
            "author": [
                "Y Oukdach",
                "Z Kerkaou",
                "M El Ansari",
                "L Koutti"
            ],
            "pub_year": "2024",
            "venue": "Multimedia Tools and …",
            "abstract": "The best features selected are fed to the Multi-Layer  2 and 3, we can divide the CNN  architecture into three main  , attention mechanism, CNN module, and ViT. We observed that the"
        },
        "filled": false,
        "gsrank": 325,
        "pub_url": "https://link.springer.com/article/10.1007/s11042-023-18039-1",
        "author_id": [
            "gxxFOUwAAAAJ",
            "MJDs8qAAAAAJ",
            "L_t1nC0AAAAJ",
            "mAaANykAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:BR-wQfjlc0MJ:scholar.google.com/&output=cite&scirp=324&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D320%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=BR-wQfjlc0MJ&ei=PnoBZ4u1OqSMy9YPoKv0mAY&json=",
        "num_citations": 7,
        "citedby_url": "/scholar?cites=4860481277279608581&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:BR-wQfjlc0MJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s11042-023-18039-1.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Etma: Efficient transformer-based multilevel attention framework for multimodal fake news detection",
            "author": [
                "A Yadav",
                "S Gaba",
                "H Khan",
                "I Budhiraja"
            ],
            "pub_year": "2023",
            "venue": "IEEE Transactions …",
            "abstract": ": a visual attention-based encoder, a textual attention-based  the attention mechanism and  LSTM network. 4) MVAE [25 We first remove the self-attention block from the architecture and"
        },
        "filled": false,
        "gsrank": 326,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10077443/",
        "author_id": [
            "pnYPJaEAAAAJ",
            "s_TKts8AAAAJ",
            "jc_aaUkAAAAJ",
            "QPoA_EcAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:xcT8zbeTdVwJ:scholar.google.com/&output=cite&scirp=325&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D320%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xcT8zbeTdVwJ&ei=PnoBZ4u1OqSMy9YPoKv0mAY&json=",
        "num_citations": 13,
        "citedby_url": "/scholar?cites=6662393641408709829&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:xcT8zbeTdVwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6570650/6780646/10077443.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformers in time series: A survey",
            "author": [
                "Q Wen",
                "T Zhou",
                "C Zhang",
                "W Chen",
                "Z Ma",
                "J Yan"
            ],
            "pub_year": "2022",
            "venue": "arXiv preprint arXiv …",
            "abstract": "module, a residual connection module followed by a layer  on both module level and  architecture level of Transformer in  multi-head attention with a multi-branch attention mechanism,"
        },
        "filled": false,
        "gsrank": 327,
        "pub_url": "https://arxiv.org/abs/2202.07125",
        "author_id": [
            "vjPJvwYAAAAJ",
            "9o5r8bUAAAAJ",
            "2bL2FJ0AAAAJ",
            "dMg_soMAAAAJ",
            "JLwF4YIAAAAJ",
            "ga230VoAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:YZKEOzGOBWQJ:scholar.google.com/&output=cite&scirp=326&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D320%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YZKEOzGOBWQJ&ei=PnoBZ4u1OqSMy9YPoKv0mAY&json=",
        "num_citations": 698,
        "citedby_url": "/scholar?cites=7207323120779432545&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:YZKEOzGOBWQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2202.07125"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Combining convolutional attention mechanism and residual deformable Transformer for infarct segmentation from CT scans of acute ischemic stroke patients",
            "author": [
                "Z Xu",
                "C Ding"
            ],
            "pub_year": "2023",
            "venue": "Frontiers in Neurology",
            "abstract": "SETR (19) employs a Transformer as the encoder and a CNN architecture as the decoder to   block attention module, and the green rectangle is the Deformable Transformer layer. The"
        },
        "filled": false,
        "gsrank": 328,
        "pub_url": "https://www.frontiersin.org/articles/10.3389/fneur.2023.1178637/full",
        "author_id": [
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:GFs_lwyK9-0J:scholar.google.com/&output=cite&scirp=327&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D320%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GFs_lwyK9-0J&ei=PnoBZ4u1OqSMy9YPoKv0mAY&json=",
        "num_citations": 7,
        "citedby_url": "/scholar?cites=17147325892918205208&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:GFs_lwyK9-0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.frontiersin.org/articles/10.3389/fneur.2023.1178637/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "STGAFormer: Spatial–temporal Gated Attention Transformer based Graph Neural Network for traffic flow forecasting",
            "author": [
                "Z Geng",
                "J Xu",
                "R Wu",
                "C Zhao",
                "J Wang",
                "Y Li",
                "C Zhang"
            ],
            "pub_year": "2024",
            "venue": "Information Fusion",
            "abstract": "the encoder architecture based on the transformer model to  By employing the multi-head  attention mechanism to capture  This paper proposes a distance spatial self-attention module to"
        },
        "filled": false,
        "gsrank": 329,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S156625352400006X",
        "author_id": [
            "",
            "hIQW1MgAAAAJ",
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:swLQa10Pw-cJ:scholar.google.com/&output=cite&scirp=328&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D320%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=swLQa10Pw-cJ&ei=PnoBZ4u1OqSMy9YPoKv0mAY&json=",
        "num_citations": 11,
        "citedby_url": "/scholar?cites=16700208737135100595&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:swLQa10Pw-cJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S156625352400006X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "[Retracted] Research on Intelligent English Translation Method Based on the Improved Attention Mechanism Model",
            "author": [
                "R Wang"
            ],
            "pub_year": "2021",
            "venue": "Scientific Programming",
            "abstract": "The attention mechanism module is mainly divided into the  module is the head attention  single-layer structure, the  the multiheaded attention network in the Transformer model with an"
        },
        "filled": false,
        "gsrank": 330,
        "pub_url": "https://onlinelibrary.wiley.com/doi/abs/10.1155/2021/9667255",
        "author_id": [
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:9FdV5HRSGTkJ:scholar.google.com/&output=cite&scirp=329&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D320%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9FdV5HRSGTkJ&ei=PnoBZ4u1OqSMy9YPoKv0mAY&json=",
        "num_citations": 21,
        "citedby_url": "/scholar?cites=4114410396580141044&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:9FdV5HRSGTkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://onlinelibrary.wiley.com/doi/pdf/10.1155/2021/9667255"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Scale-aware modulation meet transformer",
            "author": [
                "W Lin",
                "Z Wu",
                "J Chen",
                "J Huang"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the IEEE …",
            "abstract": "introduce the Multi-Head Mixed Convolution (MHMC) module,  -Transformer architecture  called the Evolutionary Hybrid  -head and multi-head mixed convolution in the last layer during"
        },
        "filled": false,
        "gsrank": 331,
        "pub_url": "http://openaccess.thecvf.com/content/ICCV2023/html/Lin_Scale-Aware_Modulation_Meet_Transformer_ICCV_2023_paper.html",
        "author_id": [
            "8iCZxIAAAAAJ",
            "",
            "",
            "-bxeZs8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:wlyyY14yLvEJ:scholar.google.com/&output=cite&scirp=330&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D330%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wlyyY14yLvEJ&ei=CXsBZ5mlMPbYy9YPrPDH6Aw&json=",
        "num_citations": 54,
        "citedby_url": "/scholar?cites=17378883393050270914&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:wlyyY14yLvEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_Scale-Aware_Modulation_Meet_Transformer_ICCV_2023_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multi-compound transformer for accurate biomedical image segmentation",
            "author": [
                "Y Ji",
                "R Zhang",
                "H Wang",
                "Z Li",
                "L Wu",
                "S Zhang"
            ],
            "pub_year": "2021",
            "venue": "… Image Computing and …",
            "abstract": "tokens to the Transformer-Self-Attention module to construct  approach, thoroughly leveraging  the attention mechanism’s  K_c}\\)of the last layer of the TCA module is further passed to a"
        },
        "filled": false,
        "gsrank": 332,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-87193-2_31",
        "author_id": [
            "7HGv1bkAAAAJ",
            "ZJwZdtgAAAAJ",
            "Xg4cp-EAAAAJ",
            "79uQHj4AAAAJ",
            "WmAYPtkAAAAJ",
            "oiBMWK4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:FvrATiqBN2sJ:scholar.google.com/&output=cite&scirp=331&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D330%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FvrATiqBN2sJ&ei=CXsBZ5mlMPbYy9YPrPDH6Aw&json=",
        "num_citations": 155,
        "citedby_url": "/scholar?cites=7725785704487254550&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:FvrATiqBN2sJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2106.14385"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Incorporating transformers and attention networks for stock movement prediction",
            "author": [
                "Y Li",
                "S Lv",
                "X Liu",
                "Q Zhang"
            ],
            "pub_year": "2022",
            "venue": "Complexity",
            "abstract": "Among them, the transformer model and attention mechanism are  transformer encoder  attention (TEA) network architecture,  the role of the corresponding module in the framework in a"
        },
        "filled": false,
        "gsrank": 333,
        "pub_url": "https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/7739087",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:bXwthwBvnBQJ:scholar.google.com/&output=cite&scirp=332&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D330%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bXwthwBvnBQJ&ei=CXsBZ5mlMPbYy9YPrPDH6Aw&json=",
        "num_citations": 16,
        "citedby_url": "/scholar?cites=1485184025184009325&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:bXwthwBvnBQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://onlinelibrary.wiley.com/doi/pdf/10.1155/2022/7739087"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Spatio-temporal graph transformer networks for pedestrian trajectory prediction",
            "author": [
                "C Yu",
                "X Ma",
                "J Ren",
                "H Zhao",
                "S Yi"
            ],
            "pub_year": "2020",
            "venue": "… , Glasgow, UK, August 23–28, 2020 …",
            "abstract": "memory module, consistently being updated by the temporal  We show that with only attention  mechanism, STAR achieves  our experiment that Transformer based architecture provides"
        },
        "filled": false,
        "gsrank": 334,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-58610-2_30",
        "author_id": [
            "4xwyGM8AAAAJ",
            "hR4G6hoAAAAJ",
            "YUKPVCoAAAAJ",
            "sMQV1ecAAAAJ",
            "afbbNmwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:rsO75fHTFToJ:scholar.google.com/&output=cite&scirp=333&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D330%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rsO75fHTFToJ&ei=CXsBZ5mlMPbYy9YPrPDH6Aw&json=",
        "num_citations": 495,
        "citedby_url": "/scholar?cites=4185484464605610926&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:rsO75fHTFToJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2005.08514"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Decomformer: Decompose Self-Attention of Transformer for Efficient Image Restoration",
            "author": [
                "E Lee",
                "Y Hwang"
            ],
            "pub_year": "2024",
            "venue": "IEEE Access",
            "abstract": "A transformer architecture achieves outstanding  attention mechanism, D-MSA,  with previous self-attention  the attention module of the first encoder layer in the network"
        },
        "filled": false,
        "gsrank": 335,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10464292/",
        "author_id": [
            "",
            "DKeoflwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:brt22i8P5jUJ:scholar.google.com/&output=cite&scirp=334&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D330%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=brt22i8P5jUJ&ei=CXsBZ5mlMPbYy9YPrPDH6Aw&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:brt22i8P5jUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6287639/10380310/10464292.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A transformer-based framework for scene text recognition",
            "author": [
                "P Selvam",
                "JAS Koilraj",
                "CAT Romero",
                "M Alharbi"
            ],
            "pub_year": "2022",
            "venue": "IEEE …",
            "abstract": "use the 83 attention mechanism in its prediction module for STR  We propose a modified  Transformer-based architecture to  the encoder’s multi-head self- 517 attention mechanism, we"
        },
        "filled": false,
        "gsrank": 336,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9894408/",
        "author_id": [
            "4RDfiKwAAAAJ",
            "",
            "KI6Yr9cAAAAJ",
            "fDNMqeUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:SQ6Y9T6XUlgJ:scholar.google.com/&output=cite&scirp=335&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D330%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SQ6Y9T6XUlgJ&ei=CXsBZ5mlMPbYy9YPrPDH6Aw&json=",
        "num_citations": 21,
        "citedby_url": "/scholar?cites=6364315520092081737&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:SQ6Y9T6XUlgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6287639/9668973/09894408.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Occlusion-aware spatial attention transformer for occluded object recognition",
            "author": [
                "J Heo",
                "Y Wang",
                "J Park"
            ],
            "pub_year": "2022",
            "venue": "Pattern Recognition Letters",
            "abstract": "Visual Transformer (ViT) [5] uses the attention mechanism on  We adopt ViT as our backbone  architecture to analyze  the speed effects of adding OMP module in Table 5. The inference"
        },
        "filled": false,
        "gsrank": 337,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0167865522001581",
        "author_id": [
            "",
            "x23yu2AAAAAJ",
            "BKc1EDMAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:pFdIM7_AjcgJ:scholar.google.com/&output=cite&scirp=336&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D330%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=pFdIM7_AjcgJ&ei=CXsBZ5mlMPbYy9YPrPDH6Aw&json=",
        "num_citations": 12,
        "citedby_url": "/scholar?cites=14451418706733455268&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:pFdIM7_AjcgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0167865522001581"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Pyramid medical transformer for medical image segmentation",
            "author": [
                "Z Zhang",
                "W Zhang"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv:2104.14702",
            "abstract": "network architecture, namely Pyramid Medical Transformer ( axial attention module does  not concern global attention  The first integrates the attention mechanism into their CNN"
        },
        "filled": false,
        "gsrank": 338,
        "pub_url": "https://arxiv.org/abs/2104.14702",
        "author_id": [
            "fmDLQ7oAAAAJ",
            "Minb5QMAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:WHVCgK-34twJ:scholar.google.com/&output=cite&scirp=337&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D330%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=WHVCgK-34twJ&ei=CXsBZ5mlMPbYy9YPrPDH6Aw&json=",
        "num_citations": 59,
        "citedby_url": "/scholar?cites=15916485997479753048&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:WHVCgK-34twJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2104.14702"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "TGDAUNet: Transformer and GCNN based dual-branch attention UNet for medical image segmentation",
            "author": [
                "P Song",
                "J Li",
                "H Fan",
                "L Fan"
            ],
            "pub_year": "2023",
            "venue": "Computers in Biology and Medicine",
            "abstract": "network and parallel attention mechanism to solve the problem of clinical focus segmentation.  TGAUNet uses the MF module and PSA module to  architecture of the TGDAUNet network"
        },
        "filled": false,
        "gsrank": 339,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S001048252301048X",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:mXI7Su-gfN8J:scholar.google.com/&output=cite&scirp=338&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D330%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mXI7Su-gfN8J&ei=CXsBZ5mlMPbYy9YPrPDH6Aw&json=",
        "num_citations": 12,
        "citedby_url": "/scholar?cites=16103923317173088921&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:mXI7Su-gfN8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S001048252301048X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Weighted feature fusion of dual attention convolutional neural network and transformer encoder module for ocean HABs classification",
            "author": [
                "GK Wu",
                "J Xu",
                "YD Zhang",
                "BY Wen",
                "BP Zhang"
            ],
            "pub_year": "2024",
            "venue": "Expert Systems with …",
            "abstract": "the attention mechanism with the convolutional neural network as  architecture leverages a  combination of the Transformer  We apply layer normalization (LN) before each block and use"
        },
        "filled": false,
        "gsrank": 340,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S095741742303381X",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:H9bBCjFd-9gJ:scholar.google.com/&output=cite&scirp=339&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D330%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=H9bBCjFd-9gJ&ei=CXsBZ5mlMPbYy9YPrPDH6Aw&json=",
        "num_citations": 3,
        "citedby_url": "/scholar?cites=15635192996562064927&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:H9bBCjFd-9gJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S095741742303381X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Perceiver: General perception with iterative attention",
            "author": [
                "A Jaegle",
                "F Gimeno",
                "A Brock"
            ],
            "pub_year": "2021",
            "venue": "International …",
            "abstract": "ceiver – a model that builds upon Transformers and hence  The model leverages an asymmetric  attention mechanism to  architecture from two components: (i) a cross-attention module"
        },
        "filled": false,
        "gsrank": 341,
        "pub_url": "http://proceedings.mlr.press/v139/jaegle21a.html",
        "author_id": [
            "2iBYdwEAAAAJ",
            "fgWuGoAAAAAJ",
            "NIxD36wAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Q4SbQOepy3gJ:scholar.google.com/&output=cite&scirp=340&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D340%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Q4SbQOepy3gJ&ei=1HsBZ5_tLqSMy9YPoKv0mAY&json=",
        "num_citations": 916,
        "citedby_url": "/scholar?cites=8704237515510088771&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Q4SbQOepy3gJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://proceedings.mlr.press/v139/jaegle21a/jaegle21a.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A Simplified Query-Only Attention for Encoder-Based Transformer Models",
            "author": [
                "H Yeom",
                "K An"
            ],
            "pub_year": "2024",
            "venue": "Applied Sciences",
            "abstract": "’s architecture. Our findings suggest that query-only  -only attention mechanism for  encoder-based transformer models.  in light green, and the fully connected layer module in"
        },
        "filled": false,
        "gsrank": 342,
        "pub_url": "https://www.mdpi.com/2076-3417/14/19/8646",
        "author_id": [
            "8m64YzIAAAAJ",
            "7v46g14AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:VP-QhAGwN34J:scholar.google.com/&output=cite&scirp=341&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D340%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VP-QhAGwN34J&ei=1HsBZ5_tLqSMy9YPoKv0mAY&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:VP-QhAGwN34J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2076-3417/14/19/8646"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "RPformer: A robust parallel transformer for visual tracking in complex scenes",
            "author": [
                "F Gu",
                "J Lu",
                "C Cai"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on Instrumentation and …",
            "abstract": "Transformer architecture with the attention mechanism to replace the correlation-based network  module propose two fresh Transformer variant structures based on attention mechanism,"
        },
        "filled": false,
        "gsrank": 343,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9764834/",
        "author_id": [
            "",
            "WRdf568AAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:s03vN-4PbEUJ:scholar.google.com/&output=cite&scirp=342&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D340%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=s03vN-4PbEUJ&ei=1HsBZ5_tLqSMy9YPoKv0mAY&json=",
        "num_citations": 46,
        "citedby_url": "/scholar?cites=5002390801916841395&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:s03vN-4PbEUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/19/4407674/09764834.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Efficient conformer with prob-sparse attention mechanism for end-to-endspeech recognition",
            "author": [
                "X Wang",
                "S Sun",
                "L Xie",
                "L Ma"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv:2106.09236",
            "abstract": "The Transformer architecture with self-attention has recently  The prediction network has  1 LSTM layer with 256 hidden  consumption of the self-attention module for sentences with"
        },
        "filled": false,
        "gsrank": 344,
        "pub_url": "https://arxiv.org/abs/2106.09236",
        "author_id": [
            "rtKnr2wAAAAJ",
            "",
            "Qddov9wAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:aYBXWKJnFk4J:scholar.google.com/&output=cite&scirp=343&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D340%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aYBXWKJnFk4J&ei=1HsBZ5_tLqSMy9YPoKv0mAY&json=",
        "num_citations": 18,
        "citedby_url": "/scholar?cites=5626798731410505833&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:aYBXWKJnFk4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2106.09236"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multimodal tweet classification in disaster response systems using transformer-based bidirectional attention model",
            "author": [
                "R Koshy",
                "S Elango"
            ],
            "pub_year": "2023",
            "venue": "Neural Computing and Applications",
            "abstract": "for image, biLSTM and attention mechanism. We put forward  is the key component in the  transformer module. In addition,  We present a neural network architecture that considers both"
        },
        "filled": false,
        "gsrank": 345,
        "pub_url": "https://link.springer.com/article/10.1007/s00521-022-07790-5",
        "author_id": [
            "",
            "o6ucKFAAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:giv6hcPzSJgJ:scholar.google.com/&output=cite&scirp=344&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D340%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=giv6hcPzSJgJ&ei=1HsBZ5_tLqSMy9YPoKv0mAY&json=",
        "num_citations": 29,
        "citedby_url": "/scholar?cites=10973288513180150658&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:giv6hcPzSJgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s00521-022-07790-5.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Prediction of BLEVE-induced response of road tunnel using Transformer network with modified self-attention (SAMT)",
            "author": [
                "R Cheng",
                "W Chen",
                "H Hao",
                "Q Li"
            ],
            "pub_year": "2024",
            "venue": "Engineering Structures",
            "abstract": "model that employs a Transformer-based architecture with a  Transformer layer, which has  the same architecture as the first  tokens output from the module of Transformer layers. The"
        },
        "filled": false,
        "gsrank": 346,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0141029624009775",
        "author_id": [
            "nBouSSQAAAAJ",
            "",
            "xRoknNkAAAAJ",
            "KpqM4F4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:btJu9gaZEo4J:scholar.google.com/&output=cite&scirp=345&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D340%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=btJu9gaZEo4J&ei=1HsBZ5_tLqSMy9YPoKv0mAY&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:btJu9gaZEo4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0141029624009775"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multi-modal learning for AU detection based on multi-head fused transformers",
            "author": [
                "X Zhang",
                "L Yin"
            ],
            "pub_year": "2021",
            "venue": "2021 16th IEEE International Conference on …",
            "abstract": "Meanwhile, the attention mechanism has been adopted in  that are utilized transformer  architecture for both feature  multi-head fusion attention module in the fusion transformer to"
        },
        "filled": false,
        "gsrank": 347,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9667030/",
        "author_id": [
            "YcwH0ioAAAAJ",
            "PYV9NvYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:uYoi_ZosNLUJ:scholar.google.com/&output=cite&scirp=346&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D340%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uYoi_ZosNLUJ&ei=1HsBZ5_tLqSMy9YPoKv0mAY&json=",
        "num_citations": 11,
        "citedby_url": "/scholar?cites=13057110263837395641&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:uYoi_ZosNLUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9666787/9666788/09667030.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer meets convolution: A bilateral awareness network for semantic segmentation of very fine resolution urban scene images",
            "author": [
                "L Wang",
                "R Li",
                "D Wang",
                "C Duan",
                "T Wang",
                "X Meng"
            ],
            "pub_year": "2021",
            "venue": "Remote Sensing",
            "abstract": "linear attention mechanism, a feature aggregation module is  , an entirely novel architecture  named Transformer [45] has  layer with BN and ReLU is deployed to obtain the attention"
        },
        "filled": false,
        "gsrank": 348,
        "pub_url": "https://www.mdpi.com/2072-4292/13/16/3065",
        "author_id": [
            "ywBbW7AAAAAJ",
            "9CagXHgAAAAJ",
            "",
            "yY_l7f4AAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:3rkYaWbVuPkJ:scholar.google.com/&output=cite&scirp=347&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D340%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3rkYaWbVuPkJ&ei=1HsBZ5_tLqSMy9YPoKv0mAY&json=",
        "num_citations": 140,
        "citedby_url": "/scholar?cites=17994366946985425374&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:3rkYaWbVuPkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2072-4292/13/16/3065/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A study on transformer-based object detection",
            "author": [
                "H Vaidwan",
                "N Seth",
                "AS Parihar"
            ],
            "pub_year": "2021",
            "venue": "… conference on intelligent …",
            "abstract": "of them are based on the attention mechanism. In this work,  layer has a multi-head attention  module, masked multi-head  Since the transformer has a permutation invariant architecture"
        },
        "filled": false,
        "gsrank": 349,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9498550/",
        "author_id": [
            "",
            "",
            "JRr4wjoAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:8INIAnkvvFEJ:scholar.google.com/&output=cite&scirp=348&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D340%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8INIAnkvvFEJ&ei=1HsBZ5_tLqSMy9YPoKv0mAY&json=",
        "num_citations": 20,
        "citedby_url": "/scholar?cites=5889634609469621232&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:8INIAnkvvFEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9497779/9498265/09498550.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Additional Self-Attention Transformer With Adapter for Thick Haze Removal",
            "author": [
                "Z Cai",
                "J Ning",
                "Z Ding",
                "B Duo"
            ],
            "pub_year": "2024",
            "venue": "IEEE Geoscience and Remote …",
            "abstract": "neural network (CNN)-based, and Transformer-based  module comprises a new Additional  Attention mechanism  adapter module in this letter does not use the linear layer combination"
        },
        "filled": false,
        "gsrank": 350,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10443626/",
        "author_id": [
            "",
            "",
            "",
            "Bm94cNkAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:03LaZiLnX6IJ:scholar.google.com/&output=cite&scirp=349&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D340%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=03LaZiLnX6IJ&ei=1HsBZ5_tLqSMy9YPoKv0mAY&json=",
        "num_citations": 3,
        "citedby_url": "/scholar?cites=11700324491872334547&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:03LaZiLnX6IJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/8859/10365397/10443626.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Cross-view geo-localization with layer-to-layer transformer",
            "author": [
                "H Yang",
                "X Lu",
                "Y Zhu"
            ],
            "pub_year": "2021",
            "venue": "Advances in Neural Information …",
            "abstract": "a novel layer-to-layer Transformer (L2LTR) architecture with  a novel self-cross attention  mechanism to interact features  performance of selfattention-based model fluctuates a lot, while"
        },
        "filled": false,
        "gsrank": 351,
        "pub_url": "https://proceedings.neurips.cc/paper/2021/hash/f31b20466ae89669f9741e047487eb37-Abstract.html",
        "author_id": [
            "",
            "",
            "Qz6p1rIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:c26J17sNrwMJ:scholar.google.com/&output=cite&scirp=350&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D350%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=c26J17sNrwMJ&ei=n3wBZ96hJLCDy9YPgrj5gAQ&json=",
        "num_citations": 95,
        "citedby_url": "/scholar?cites=265446003464302195&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:c26J17sNrwMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper/2021/file/f31b20466ae89669f9741e047487eb37-Paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Vision-language transformer and query generation for referring segmentation",
            "author": [
                "H Ding",
                "C Liu",
                "S Wang",
                "X Jiang"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the IEEE …",
            "abstract": "However, most of them only utilize the attention mechanism  attention based architecture,  which helps us to easily model  an attention map from the second layer of the transformer"
        },
        "filled": false,
        "gsrank": 352,
        "pub_url": "http://openaccess.thecvf.com/content/ICCV2021/html/Ding_Vision-Language_Transformer_and_Query_Generation_for_Referring_Segmentation_ICCV_2021_paper.html",
        "author_id": [
            "WI_flSwAAAAJ",
            "XlQP0GIAAAAJ",
            "eP__svIAAAAJ",
            "IL3mSioAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:fn0AmDAwec8J:scholar.google.com/&output=cite&scirp=351&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D350%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=fn0AmDAwec8J&ei=n3wBZ96hJLCDy9YPgrj5gAQ&json=",
        "num_citations": 235,
        "citedby_url": "/scholar?cites=14950033423299804542&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:fn0AmDAwec8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Ding_Vision-Language_Transformer_and_Query_Generation_for_Referring_Segmentation_ICCV_2021_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "DMFormer: Closing the gap Between CNN and Vision Transformers",
            "author": [
                "Z Wei",
                "H Pan",
                "L Li",
                "M Lu",
                "X Niu"
            ],
            "pub_year": "2023",
            "venue": "ICASSP 2023-2023 …",
            "abstract": "a Dynamic Multi-level Attention mechanism (DMA), which  DMA, we extend the architecture  of Swin Transformer [3] and  stem module, which consists of a 7×7 convolution layer with"
        },
        "filled": false,
        "gsrank": 353,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10097256/",
        "author_id": [
            "032A7KYAAAAJ",
            "HxAw_YMAAAAJ",
            "_alCMdsAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:JNIA0mpoZ6gJ:scholar.google.com/&output=cite&scirp=352&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D350%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JNIA0mpoZ6gJ&ei=n3wBZ96hJLCDy9YPgrj5gAQ&json=",
        "num_citations": 21,
        "citedby_url": "/scholar?cites=12134782528972182052&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:JNIA0mpoZ6gJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/10094559/10094560/10097256.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multiple paddy disease recognition methods based on deformable transformer attention mechanism in complex scenarios",
            "author": [
                "X Zhang",
                "H Dong",
                "L Gong",
                "X Cheng"
            ],
            "pub_year": "2023",
            "venue": "International Journal of …",
            "abstract": "network architecture fuses multi-scale features by adding a feature fusion module after the  backbone network of the last decoder layer, we employ two Feed Forward Networks (FFNs) to"
        },
        "filled": false,
        "gsrank": 354,
        "pub_url": "https://www.tandfonline.com/doi/abs/10.1080/1206212X.2023.2263254",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:uL9EMdzpgBMJ:scholar.google.com/&output=cite&scirp=353&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D350%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uL9EMdzpgBMJ&ei=n3wBZ96hJLCDy9YPgrj5gAQ&json=",
        "num_citations": 2,
        "citedby_url": "/scholar?cites=1405380215668260792&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:uL9EMdzpgBMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A novel transformer-based DL model enhanced by position-sensitive attention and gated hierarchical LSTM for aero-engine RUL prediction",
            "author": [
                "X Chen"
            ],
            "pub_year": "2024",
            "venue": "Scientific Reports",
            "abstract": "the attention mechanism employed by the Transformer encoder and architecture of the  Transformer encoder module. The  module consists of a simple fully connected (FC) layer and"
        },
        "filled": false,
        "gsrank": 355,
        "pub_url": "https://www.nature.com/articles/s41598-024-59095-3",
        "author_id": [
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Iidz975PSMwJ:scholar.google.com/&output=cite&scirp=354&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D350%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Iidz975PSMwJ&ei=n3wBZ96hJLCDy9YPgrj5gAQ&json=",
        "num_citations": 5,
        "citedby_url": "/scholar?cites=14720103063674365730&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Iidz975PSMwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.nature.com/articles/s41598-024-59095-3.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "RSAM: Robust self-attention based multi-horizon model for solar irradiance forecasting",
            "author": [
                "S Sharda",
                "M Singh",
                "K Sharma"
            ],
            "pub_year": "2020",
            "venue": "IEEE Transactions on …",
            "abstract": "A self-attention based Transformer model belonging to the family  attention mechanism for  multivariate solar time-series  uses self-attention based deep learning architecture because of"
        },
        "filled": false,
        "gsrank": 356,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9301241/",
        "author_id": [
            "yYp42bkAAAAJ",
            "mSfrxCkAAAAJ",
            "LkBDGkUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:D-yliqDAZKMJ:scholar.google.com/&output=cite&scirp=355&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D350%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=D-yliqDAZKMJ&ei=n3wBZ96hJLCDy9YPgrj5gAQ&json=",
        "num_citations": 67,
        "citedby_url": "/scholar?cites=11773747121606749199&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:D-yliqDAZKMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/5165391/5433168/09301241.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Adversarial joint training with self-attention mechanism for robust end-to-end speech recognition",
            "author": [
                "L Li",
                "Y Kang",
                "Y Shi",
                "L Kürzinger",
                "T Watzel"
            ],
            "pub_year": "2021",
            "venue": "EURASIP Journal on …",
            "abstract": "We propose a self-attention-based jointly trained adversarial framework  The location-based  attention mechanism comprises 10  The same as the Transformer, we train the network with"
        },
        "filled": false,
        "gsrank": 357,
        "pub_url": "https://link.springer.com/article/10.1186/s13636-021-00215-6",
        "author_id": [
            "",
            "",
            "",
            "",
            "tOusErsAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:VjuTYvlK_T4J:scholar.google.com/&output=cite&scirp=356&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D350%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VjuTYvlK_T4J&ei=n3wBZ96hJLCDy9YPgrj5gAQ&json=",
        "num_citations": 22,
        "citedby_url": "/scholar?cites=4538866434420456278&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:VjuTYvlK_T4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1186/s13636-021-00215-6.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Beyond self-attention: Deformable large kernel attention for medical image segmentation",
            "author": [
                "R Azad",
                "L Niggemeier",
                "M Hüttemann"
            ],
            "pub_year": "2024",
            "venue": "Proceedings of the …",
            "abstract": "Additionally, our proposed attention mechanism benefits  Transformer architecture, the  D-LKA Net. Evaluations of our  transformer-based pipeline featuring a double attention module"
        },
        "filled": false,
        "gsrank": 358,
        "pub_url": "https://openaccess.thecvf.com/content/WACV2024/html/Azad_Beyond_Self-Attention_Deformable_Large_Kernel_Attention_for_Medical_Image_Segmentation_WACV_2024_paper.html",
        "author_id": [
            "Qb5ildMAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:1Mw0sdlPJ6IJ:scholar.google.com/&output=cite&scirp=357&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D350%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1Mw0sdlPJ6IJ&ei=n3wBZ96hJLCDy9YPgrj5gAQ&json=",
        "num_citations": 38,
        "citedby_url": "/scholar?cites=11684395554635566292&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:1Mw0sdlPJ6IJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/WACV2024/papers/Azad_Beyond_Self-Attention_Deformable_Large_Kernel_Attention_for_Medical_Image_Segmentation_WACV_2024_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "UATNet: U-shape attention-based transformer net for meteorological satellite cloud recognition",
            "author": [
                "Z Wang",
                "J Zhao",
                "R Zhang",
                "Z Li",
                "Q Lin",
                "X Wang"
            ],
            "pub_year": "2021",
            "venue": "Remote Sensing",
            "abstract": "network model based on the U-shaped architecture, in which  connection, and the attention  mechanism is designed to  the number of attention heads to 4 and build a 4-layer CCT."
        },
        "filled": false,
        "gsrank": 359,
        "pub_url": "https://www.mdpi.com/2072-4292/14/1/104",
        "author_id": [
            "",
            "",
            "qZNpUgIAAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:rMV2xqXd4aMJ:scholar.google.com/&output=cite&scirp=358&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D350%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rMV2xqXd4aMJ&ei=n3wBZ96hJLCDy9YPgrj5gAQ&json=",
        "num_citations": 18,
        "citedby_url": "/scholar?cites=11808963402011166124&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:rMV2xqXd4aMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2072-4292/14/1/104/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Image super-resolution method based on the interactive fusion of transformer and CNN features",
            "author": [
                "J Wang",
                "Y Zou",
                "O Alfarraj",
                "PK Sharma",
                "W Said"
            ],
            "pub_year": "2024",
            "venue": "The Visual …",
            "abstract": "into the attention module to improve the Transformer, utilizing  to the attention mechanism  and feed-forward network (FFN),  The new transformer architecture combines self-attention"
        },
        "filled": false,
        "gsrank": 360,
        "pub_url": "https://link.springer.com/article/10.1007/s00371-023-03138-9",
        "author_id": [
            "",
            "",
            "PZTxwmcAAAAJ",
            "",
            "UbNIZcAAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:EmfAiG2WUO8J:scholar.google.com/&output=cite&scirp=359&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D350%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EmfAiG2WUO8J&ei=n3wBZ96hJLCDy9YPgrj5gAQ&json=",
        "num_citations": 3,
        "citedby_url": "/scholar?cites=17244448370391541522&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:EmfAiG2WUO8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s00371-023-03138-9.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Dlgsanet: lightweight dynamic local and global self-attention networks for image super-resolution",
            "author": [
                "X Li",
                "J Dong",
                "J Tang",
                "J Pan"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the IEEE/CVF …",
            "abstract": "network designs of Transformers, we develop a simple yet effective multi-head dynamic local  self-attention (MHDLSA) module to  a channel attention mechanism to enable the network’s"
        },
        "filled": false,
        "gsrank": 361,
        "pub_url": "http://openaccess.thecvf.com/content/ICCV2023/html/Li_DLGSANet_Lightweight_Dynamic_Local_and_Global_Self-Attention_Networks_for_Image_ICCV_2023_paper.html",
        "author_id": [
            "UlJRQJ0AAAAJ",
            "ruebFVEAAAAJ",
            "ByBLlEwAAAAJ",
            "CMsNjGIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:258PdQCQbkMJ:scholar.google.com/&output=cite&scirp=360&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D360%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=258PdQCQbkMJ&ei=an0BZ5LQL6SMy9YPoKv0mAY&json=",
        "num_citations": 36,
        "citedby_url": "/scholar?cites=4858979379617701851&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:258PdQCQbkMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://openaccess.thecvf.com/content/ICCV2023/papers/Li_DLGSANet_Lightweight_Dynamic_Local_and_Global_Self-Attention_Networks_for_Image_ICCV_2023_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Cross attention is all you need: relational remote sensing change detection with transformer",
            "author": [
                "K Lu",
                "X Huang",
                "R Xia",
                "P Zhang"
            ],
            "pub_year": "2024",
            "venue": "GIScience & Remote …",
            "abstract": "The proposed architecture is able to excavate the long-range  cross attention model is  passed to a simple three-layer FFN  RCDT module that involves the cross attention mechanism."
        },
        "filled": false,
        "gsrank": 362,
        "pub_url": "https://www.tandfonline.com/doi/abs/10.1080/15481603.2024.2380126",
        "author_id": [
            "",
            "Bj6cqukAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:TVjWgGIpG7sJ:scholar.google.com/&output=cite&scirp=361&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D360%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TVjWgGIpG7sJ&ei=an0BZ5LQL6SMy9YPoKv0mAY&json=",
        "num_citations": 2,
        "citedby_url": "/scholar?cites=13482415412508776525&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:TVjWgGIpG7sJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.tandfonline.com/doi/pdf/10.1080/15481603.2024.2380126"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Dual self-attention Swin transformer for hyperspectral image super-resolution",
            "author": [
                "Y Long",
                "X Wang",
                "M Xu",
                "S Zhang"
            ],
            "pub_year": "2023",
            "venue": "IEEE Transactions on …",
            "abstract": "by integrating it with the Swin Transformer architecture.  DSSTSR network uses a multi-layer  perceptron (MLP) layer  Denoising module and Dual Self-Attention Swin Transformer module"
        },
        "filled": false,
        "gsrank": 363,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10123084/",
        "author_id": [
            "triu-OgAAAAJ",
            "",
            "o8YoNzIAAAAJ",
            "O48TQQ4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:cLPoaLYpPzAJ:scholar.google.com/&output=cite&scirp=362&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D360%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cLPoaLYpPzAJ&ei=an0BZ5LQL6SMy9YPoKv0mAY&json=",
        "num_citations": 23,
        "citedby_url": "/scholar?cites=3476543300774179696&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:cLPoaLYpPzAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/36/4358825/10123084.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Lightweight Vision Transformer with Spatial and Channel Enhanced Self-Attention",
            "author": [
                "J Zheng",
                "L Yang",
                "Y Li",
                "K Yang"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the …",
            "abstract": "principles of lightweight transformer architecture design [8, 16] We use the SE module  in [4] as a channel attention operation our network is Spatial and Channel enhanced Self-Attention,"
        },
        "filled": false,
        "gsrank": 364,
        "pub_url": "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Zheng_Lightweight_Vision_Transformer_with_Spatial_and_Channel_Enhanced_Self-Attention_ICCVW_2023_paper.html",
        "author_id": [
            "",
            "",
            "",
            "5NU3Hh0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:qQv9vZBv38cJ:scholar.google.com/&output=cite&scirp=363&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D360%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qQv9vZBv38cJ&ei=an0BZ5LQL6SMy9YPoKv0mAY&json=",
        "num_citations": 3,
        "citedby_url": "/scholar?cites=14402352800807586729&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:qQv9vZBv38cJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/ICCV2023W/RCV/papers/Zheng_Lightweight_Vision_Transformer_with_Spatial_and_Channel_Enhanced_Self-Attention_ICCVW_2023_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Spatial temporal transformer network for skeleton-based action recognition",
            "author": [
                "C Plizzari",
                "M Cannici",
                "M Matteucci"
            ],
            "pub_year": "2021",
            "venue": "… : virtual event, January 10–15, 2021 …",
            "abstract": "We design a Spatial Self-Attention (SSA) module to  Transformer (ST-TR), an architecture  using the Transformer  T-TR layer, a standard graph convolution sub-module [16] is"
        },
        "filled": false,
        "gsrank": 365,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-68796-0_50",
        "author_id": [
            "OlK2hyIAAAAJ",
            "Xd9geyMAAAAJ",
            "PdbEg5YAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:aNsB2KygzIEJ:scholar.google.com/&output=cite&scirp=364&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D360%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aNsB2KygzIEJ&ei=an0BZ5LQL6SMy9YPoKv0mAY&json=",
        "num_citations": 240,
        "citedby_url": "/scholar?cites=9353027190360497000&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:aNsB2KygzIEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2012.06399"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "U-shaped transformer with frequency-band aware attention for speech enhancement",
            "author": [
                "Y Li",
                "Y Sun",
                "W Wang",
                "SM Naqvi"
            ],
            "pub_year": "2023",
            "venue": "IEEE/ACM Transactions on …",
            "abstract": "to the conventional attention mechanism, the self-attention  attention based UTransformer  with the frequency-band aware attentions. Each block in the overall architecture of the network"
        },
        "filled": false,
        "gsrank": 366,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10100864/",
        "author_id": [
            "IgL7X3AAAAAJ",
            "-NuodQ8AAAAJ",
            "JQFnV5IAAAAJ",
            "3MdEZ-AAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:vO0UYF6rpmAJ:scholar.google.com/&output=cite&scirp=365&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D360%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vO0UYF6rpmAJ&ei=an0BZ5LQL6SMy9YPoKv0mAY&json=",
        "num_citations": 18,
        "citedby_url": "/scholar?cites=6964442295602310588&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:vO0UYF6rpmAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6570655/6633080/10100864.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "DI-Unet: Dimensional interaction self-attention for medical image segmentation",
            "author": [
                "Y Wu",
                "G Wang",
                "Z Wang",
                "H Wang",
                "Y Li"
            ],
            "pub_year": "2022",
            "venue": "Biomedical Signal Processing and …",
            "abstract": "basic network architecture based on Transformer structure  processed by the attention  mechanism in different windows. It  network, and the Cross-Window(CsWin) Self-Attention module"
        },
        "filled": false,
        "gsrank": 367,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1746809422004062",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:cZmmCbCNSFQJ:scholar.google.com/&output=cite&scirp=366&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D360%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cZmmCbCNSFQJ&ei=an0BZ5LQL6SMy9YPoKv0mAY&json=",
        "num_citations": 18,
        "citedby_url": "/scholar?cites=6073259884724787569&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:cZmmCbCNSFQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1746809422004062"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Batch Transformer: Look for Attention in Batch",
            "author": [
                "MB Her",
                "J Jeong",
                "H Song",
                "JH Han"
            ],
            "pub_year": "2024",
            "venue": "arXiv preprint arXiv:2407.04218",
            "abstract": "the proposed class batch attention (CBA) module, to prevent  Recently, attention mechanism  models have been applied to  batch transformer network (BTN) architecture with multi-level"
        },
        "filled": false,
        "gsrank": 368,
        "pub_url": "https://arxiv.org/abs/2407.04218",
        "author_id": [
            "",
            "",
            "",
            "mr045G4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:uaS63xNMEXgJ:scholar.google.com/&output=cite&scirp=367&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D360%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uaS63xNMEXgJ&ei=an0BZ5LQL6SMy9YPoKv0mAY&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=8651780007397074105&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:uaS63xNMEXgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2407.04218"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer-Based Fused Attention Combined with CNNs for Image Classification",
            "author": [
                "J Jiang",
                "H Xu",
                "X Xu",
                "Y Cui",
                "J Wu"
            ],
            "pub_year": "2023",
            "venue": "Neural Processing Letters",
            "abstract": "the hierarchical architecture of the Swin Transformer by  , this paper introduces an attention  mechanism to the sequence  convolution module following the attention extraction layer. As"
        },
        "filled": false,
        "gsrank": 369,
        "pub_url": "https://link.springer.com/article/10.1007/s11063-023-11402-1",
        "author_id": [
            "NNaMeE0AAAAJ",
            "Tyzef4sAAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:7vzMr4MxxiwJ:scholar.google.com/&output=cite&scirp=368&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D360%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7vzMr4MxxiwJ&ei=an0BZ5LQL6SMy9YPoKv0mAY&json=",
        "num_citations": 5,
        "citedby_url": "/scholar?cites=3226320624717462766&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:7vzMr4MxxiwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s11063-023-11402-1.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention fusion of transformer-based and scale-based method for hyperspectral and LiDAR joint classification",
            "author": [
                "M Zhang",
                "F Gao",
                "T Zhang",
                "Y Gan",
                "J Dong",
                "H Yu"
            ],
            "pub_year": "2023",
            "venue": "Remote Sensing",
            "abstract": "We propose a module named fusion transformer (FUTR) for  obtained by the second  convolution layer in the stage that uses  fusion technique and cross attention mechanism for the"
        },
        "filled": false,
        "gsrank": 370,
        "pub_url": "https://www.mdpi.com/2072-4292/15/3/650",
        "author_id": [
            "",
            "k91CLXQAAAAJ",
            "",
            "zVhafb4AAAAJ",
            "iPYdUpAAAAAJ",
            "K9bBXkYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:9blm_cqaC8EJ:scholar.google.com/&output=cite&scirp=369&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D360%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9blm_cqaC8EJ&ei=an0BZ5LQL6SMy9YPoKv0mAY&json=",
        "num_citations": 10,
        "citedby_url": "/scholar?cites=13910382070689348085&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:9blm_cqaC8EJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2072-4292/15/3/650/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "ASCAM-Former: Blind image quality assessment based on adaptive spatial & channel attention merging transformer and image to patch weights sharing",
            "author": [
                "X Ma",
                "S Zhang",
                "Y Wang",
                "R Li",
                "X Chen",
                "D Yu"
            ],
            "pub_year": "2023",
            "venue": "Expert Systems with …",
            "abstract": "feasibility of incorporating attention mechanism in a channel- channel attention mechanism  in a Transform architecture.  spatial and channel attention merging module to dynamically"
        },
        "filled": false,
        "gsrank": 371,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0957417422022862",
        "author_id": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:DAHRWVUe5kYJ:scholar.google.com/&output=cite&scirp=370&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D370%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DAHRWVUe5kYJ&ei=NX4BZ5SmLNKAy9YPm-_p6Ao&json=",
        "num_citations": 13,
        "citedby_url": "/scholar?cites=5108804179226329356&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:DAHRWVUe5kYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0957417422022862"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Laplacian mesh transformer: Dual attention and topology aware network for 3D mesh classification and segmentation",
            "author": [
                "XJ Li",
                "J Yang",
                "FL Zhang"
            ],
            "pub_year": "2022",
            "venue": "European Conference on Computer Vision",
            "abstract": "This paper builds a dual attention architecture in a topology- Then we apply a final  attention-based fusion module to learn  we propose the dual attention mechanism that achieves"
        },
        "filled": false,
        "gsrank": 372,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-031-19818-2_31",
        "author_id": [
            "",
            "RTr75A0AAAAJ",
            "_zLn05oAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:mVR8HcB0EYcJ:scholar.google.com/&output=cite&scirp=371&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D370%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mVR8HcB0EYcJ&ei=NX4BZ5SmLNKAy9YPm-_p6Ao&json=",
        "num_citations": 13,
        "citedby_url": "/scholar?cites=9732688638201582745&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:mVR8HcB0EYcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136890532.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Time series forecasting based on convolution transformer",
            "author": [
                "N Wang",
                "X Zhao"
            ],
            "pub_year": "2023",
            "venue": "IEICE TRANSACTIONS on Information and …",
            "abstract": "Each attention mechanism module is followed by a convolution pooling layer to adjust the   and effect of the proposed network model architecture in time series forecasting. MSE is"
        },
        "filled": false,
        "gsrank": 373,
        "pub_url": "https://search.ieice.org/bin/summary.php?id=e106-d_5_976",
        "author_id": [
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:XqcXsipznnMJ:scholar.google.com/&output=cite&scirp=372&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D370%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XqcXsipznnMJ&ei=NX4BZ5SmLNKAy9YPm-_p6Ao&json=",
        "num_citations": 8,
        "citedby_url": "/scholar?cites=8331222987895711582&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:XqcXsipznnMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.jstage.jst.go.jp/article/transinf/E106.D/5/E106.D_2022EDP7136/_pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A free lunch from vit: Adaptive attention multi-scale fusion transformer for fine-grained visual recognition",
            "author": [
                "Y Zhang",
                "J Cao",
                "L Zhang",
                "X Liu",
                "Z Wang"
            ],
            "pub_year": "2022",
            "venue": "ICASSP 2022-2022 …",
            "abstract": "vision due to its attention mechanism. Nonetheless, with the  transform layer, the Selective  Attention Collection Module ( warded to a network to produce our layer attention 𝑀𝐿 ∈ ℝ𝑁"
        },
        "filled": false,
        "gsrank": 374,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9747591/",
        "author_id": [
            "dXj1WskAAAAJ",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:MNB_sbQfztoJ:scholar.google.com/&output=cite&scirp=373&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D370%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MNB_sbQfztoJ&ei=NX4BZ5SmLNKAy9YPm-_p6Ao&json=",
        "num_citations": 65,
        "citedby_url": "/scholar?cites=15766574206403203120&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:MNB_sbQfztoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9745891/9746004/09747591.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "SRRNet: A Transformer Structure with Adaptive 2D Spatial Attention Mechanism for Cell Phone-Captured Shopping Receipt Recognition",
            "author": [
                "H Zhou",
                "L Shao",
                "H Zhang"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on Consumer …",
            "abstract": "Our model is largely inspired by transformer architecture, which  key module, adaptive 2D  spatial attention mechanism, can be  an embedding layer and a linear layer after position range"
        },
        "filled": false,
        "gsrank": 375,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9987538/",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:MNxqI7x7CuoJ:scholar.google.com/&output=cite&scirp=374&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D370%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MNxqI7x7CuoJ&ei=NX4BZ5SmLNKAy9YPm-_p6Ao&json=",
        "num_citations": 3,
        "citedby_url": "/scholar?cites=16864427802620517424&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:MNxqI7x7CuoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/30/8306365/09987538.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Learning dynamic and hierarchical traffic spatiotemporal features with transformer",
            "author": [
                "H Yan",
                "X Ma",
                "Z Pu"
            ],
            "pub_year": "2021",
            "venue": "IEEE Transactions on Intelligent …",
            "abstract": "Transformer only uses attention mechanism and fully  The overall architecture of the traffic  transformer is shown in Fig. 2 the second module is a fully connected feedforward layer. The"
        },
        "filled": false,
        "gsrank": 376,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9520129/",
        "author_id": [
            "",
            "GAQKxKAAAAAJ",
            "EzCLa-4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:a0V0DgHh0mIJ:scholar.google.com/&output=cite&scirp=375&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D370%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=a0V0DgHh0mIJ&ei=NX4BZ5SmLNKAy9YPm-_p6Ao&json=",
        "num_citations": 132,
        "citedby_url": "/scholar?cites=7121001355479893355&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:a0V0DgHh0mIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6979/9942712/09520129.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "The lipschitz constant of self-attention",
            "author": [
                "H Kim",
                "G Papamakarios",
                "A Mnih"
            ],
            "pub_year": "2021",
            "venue": "… Conference on Machine …",
            "abstract": "self-attention and use it in a Transformer-based architecture for  -layer Transformer (L2), WQ  = WK model and (DP) model used  Having a provably Lipschitz self-attention module at our"
        },
        "filled": false,
        "gsrank": 377,
        "pub_url": "https://proceedings.mlr.press/v139/kim21i.html",
        "author_id": [
            "vxU3Zk4AAAAJ",
            "wHcpf58AAAAJ",
            "mxiO4IkAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:jX9lTQlpeasJ:scholar.google.com/&output=cite&scirp=376&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D370%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jX9lTQlpeasJ&ei=NX4BZ5SmLNKAy9YPm-_p6Ao&json=",
        "num_citations": 145,
        "citedby_url": "/scholar?cites=12356022541341785997&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:jX9lTQlpeasJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://proceedings.mlr.press/v139/kim21i/kim21i.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Interpretable deep learning model for building energy consumption prediction based on attention mechanism",
            "author": [
                "Y Gao",
                "Y Ruan"
            ],
            "pub_year": "2021",
            "venue": "Energy and Buildings",
            "abstract": "Attention based on hidden layer states and feature-based  prediction model based on the  transformer model. The specific  help of the encoder and decoder architecture, the prediction of"
        },
        "filled": false,
        "gsrank": 378,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0378778821006630",
        "author_id": [
            "tYjB0V8AAAAJ",
            "7tbmZAIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Sj-tnqp0h2cJ:scholar.google.com/&output=cite&scirp=377&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D370%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Sj-tnqp0h2cJ&ei=NX4BZ5SmLNKAy9YPm-_p6Ao&json=",
        "num_citations": 75,
        "citedby_url": "/scholar?cites=7460059583917932362&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Sj-tnqp0h2cJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0378778821006630"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "SwinPA-Net: Swin transformer-based multiscale feature pyramid aggregation network for medical image segmentation",
            "author": [
                "H Du",
                "J Wang",
                "M Liu",
                "Y Wang"
            ],
            "pub_year": "2022",
            "venue": "… on Neural Networks and …",
            "abstract": "Although the above transformer-based architecture can  Under the action of the LPA module,  the attention mechanism is  -layer LPA module is better than a one-layer global attention,"
        },
        "filled": false,
        "gsrank": 379,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9895210/",
        "author_id": [
            "",
            "Nc0myBIAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:14ZItSGYgOEJ:scholar.google.com/&output=cite&scirp=378&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D370%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=14ZItSGYgOEJ&ei=NX4BZ5SmLNKAy9YPm-_p6Ao&json=",
        "num_citations": 52,
        "citedby_url": "/scholar?cites=16249154726095521495&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:14ZItSGYgOEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/5962385/6104215/09895210.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Pale transformer: A general vision transformer backbone with pale-shaped attention",
            "author": [
                "S Wu",
                "T Wu",
                "H Tan",
                "G Guo"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the AAAI Conference on …",
            "abstract": "Consequently, their receptive fields in a single attention layer  2021d) proposed a cascade  Transformer architecture to  , the proposed PS-Attention module for capturing contextual"
        },
        "filled": false,
        "gsrank": 380,
        "pub_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20176",
        "author_id": [
            "0ao4z_MAAAAJ",
            "FHdkcWsAAAAJ",
            "",
            "f2Y5nygAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:DLaeaotBdzsJ:scholar.google.com/&output=cite&scirp=379&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D370%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DLaeaotBdzsJ&ei=NX4BZ5SmLNKAy9YPm-_p6Ao&json=",
        "num_citations": 67,
        "citedby_url": "/scholar?cites=4284965637511362060&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:DLaeaotBdzsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20176/19935"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A weakly-supervised transformer-based hybrid network with multi-attention for pavement crack detection",
            "author": [
                "Z Wang",
                "Z Leng",
                "Z Zhang"
            ],
            "pub_year": "2024",
            "venue": "Construction and Building Materials",
            "abstract": ", a novel architecture integrating Gated Axial Transformer into  attention mechanism  decomposed the self-attention module  MLP layer to process the feature map from Transformer"
        },
        "filled": false,
        "gsrank": 381,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0950061823038527",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:YFrA27jYKKkJ:scholar.google.com/&output=cite&scirp=380&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D380%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YFrA27jYKKkJ&ei=AH8BZ6mHBaSMy9YPoKv0mAY&json=",
        "num_citations": 7,
        "citedby_url": "/scholar?cites=12189230679950645856&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:YFrA27jYKKkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0950061823038527"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Grouped self-attention mechanism for a memory-efficient Transformer",
            "author": [
                "B Jung",
                "Y Mukuta",
                "T Harada"
            ],
            "pub_year": "2022",
            "venue": "arXiv preprint arXiv:2210.00440",
            "abstract": ", which is the bottleneck of the Transformer architecture. This  decoder layer and the complexity  of the cross-attention module  GSA module is a type of local attention mechanism that can"
        },
        "filled": false,
        "gsrank": 382,
        "pub_url": "https://www.researchgate.net/profile/Bumjun-Jung/publication/364126129_Grouped_self-attention_mechanism_for_a_memory-efficient_Transformer/links/638a363f7d9b40514e0b0c9e/Grouped-self-attention-mechanism-for-a-memory-efficient-Transformer.pdf",
        "author_id": [
            "dZy-clsAAAAJ",
            "emo91rIAAAAJ",
            "k8rlJ8AAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:MQRINSLoUZAJ:scholar.google.com/&output=cite&scirp=381&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D380%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MQRINSLoUZAJ&ei=AH8BZ6mHBaSMy9YPoKv0mAY&json=",
        "num_citations": 2,
        "citedby_url": "/scholar?cites=10399348248195630129&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:MQRINSLoUZAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.researchgate.net/profile/Bumjun-Jung/publication/364126129_Grouped_self-attention_mechanism_for_a_memory-efficient_Transformer/links/638a363f7d9b40514e0b0c9e/Grouped-self-attention-mechanism-for-a-memory-efficient-Transformer.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Recurrent video restoration transformer with guided deformable attention",
            "author": [
                "J Liang",
                "Y Fan",
                "X Xiang",
                "R Ranjan"
            ],
            "pub_year": "2022",
            "venue": "Advances in …",
            "abstract": "Transformer [77] is the de-facto standard architecture in natural  ture refinement module that  consists of a convolution layer for  Besides, in the attention mechanism, we can further divide"
        },
        "filled": false,
        "gsrank": 383,
        "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/02687e7b22abc64e651be8da74ec610e-Abstract-Conference.html",
        "author_id": [
            "3-Hz9BgAAAAJ",
            "BlfdYL0AAAAJ",
            "KTn1AoUAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:mDUVwj4Vc6YJ:scholar.google.com/&output=cite&scirp=382&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D380%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=mDUVwj4Vc6YJ&ei=AH8BZ6mHBaSMy9YPoKv0mAY&json=",
        "num_citations": 121,
        "citedby_url": "/scholar?cites=11993953591906088344&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:mDUVwj4Vc6YJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/02687e7b22abc64e651be8da74ec610e-Paper-Conference.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attransunet: An enhanced hybrid transformer architecture for ultrasound and histopathology image segmentation",
            "author": [
                "X Li",
                "S Pang",
                "R Zhang",
                "J Zhu",
                "X Fu",
                "Y Tian"
            ],
            "pub_year": "2023",
            "venue": "Computers in Biology and …",
            "abstract": "single-layer SFRM does not enable the network to enhance  the SFRM with the spatial  attention mechanism in the same  and Transformer, a Selective Feature Reinforcement Module ("
        },
        "filled": false,
        "gsrank": 384,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0010482522010733",
        "author_id": [
            "",
            "",
            "aKSBE7MAAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:g2C9uYXY2RYJ:scholar.google.com/&output=cite&scirp=383&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D380%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=g2C9uYXY2RYJ&ei=AH8BZ6mHBaSMy9YPoKv0mAY&json=",
        "num_citations": 20,
        "citedby_url": "/scholar?cites=1646585207639072899&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:g2C9uYXY2RYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0010482522010733"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer-based visual object tracking via fine–coarse concatenated attention and cross concatenated MLP",
            "author": [
                "L Gao",
                "L Chen",
                "P Liu",
                "Y Jiang",
                "Y Li",
                "J Ning"
            ],
            "pub_year": "2024",
            "venue": "Pattern Recognition",
            "abstract": "within one attention layer of the original self-attention learning  in the transformer to achieve  nonlocal attention-based  of multi-head self-attention (MSA) and feed-forward network ("
        },
        "filled": false,
        "gsrank": 385,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0031320323006623",
        "author_id": [
            "",
            "",
            "",
            "",
            "aY_2RzkAAAAJ",
            "bePJGzMAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:yUOggdp5br8J:scholar.google.com/&output=cite&scirp=384&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D380%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yUOggdp5br8J&ei=AH8BZ6mHBaSMy9YPoKv0mAY&json=",
        "num_citations": 14,
        "citedby_url": "/scholar?cites=13794096688067003337&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:yUOggdp5br8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0031320323006623"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "ReTransformer: ReRAM-based processing-in-memory architecture for transformer acceleration",
            "author": [
                "X Yang",
                "B Yan",
                "H Li",
                "Y Chen"
            ],
            "pub_year": "2020",
            "venue": "… of the 39th International Conference on …",
            "abstract": "-based NLP tasks, attention mechanism is a popular and  , an outstanding self-attention  based model – Transformer [28 between the linear layer and MatMul in Transformer. Fig. 6(a"
        },
        "filled": false,
        "gsrank": 386,
        "pub_url": "https://dl.acm.org/doi/abs/10.1145/3400302.3415640",
        "author_id": [
            "_JIESQ8AAAAJ",
            "oN9gwMIAAAAJ",
            "E6Tpfq8AAAAJ",
            "3G-nnjMAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:EZ3jKA-DYYsJ:scholar.google.com/&output=cite&scirp=385&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D380%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EZ3jKA-DYYsJ&ei=AH8BZ6mHBaSMy9YPoKv0mAY&json=",
        "num_citations": 92,
        "citedby_url": "/scholar?cites=10043452745146670353&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:EZ3jKA-DYYsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://dl.acm.org/doi/pdf/10.1145/3400302.3415640"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Double-stream position learning transformer network for image captioning",
            "author": [
                "W Jiang",
                "W Zhou",
                "H Hu"
            ],
            "pub_year": "2022",
            "venue": "… on Circuits and Systems for Video …",
            "abstract": "• We propose a double-stream Transformer architecture to  -stream Transformer encoder, we  apply the RPL module to  an Adaptive Fusion Attention mechanism for the language decoder"
        },
        "filled": false,
        "gsrank": 387,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9791335/",
        "author_id": [
            "",
            "eyQteL0AAAAJ",
            "gieROVcAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:vVexM8DErS4J:scholar.google.com/&output=cite&scirp=386&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D380%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vVexM8DErS4J&ei=AH8BZ6mHBaSMy9YPoKv0mAY&json=",
        "num_citations": 36,
        "citedby_url": "/scholar?cites=3363560826495653821&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:vVexM8DErS4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/76/4358651/09791335.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "ABT-GAMNet: A novel adaptive Boundary-aware transformer with Gated attention mechanism for automated skin lesion segmentation",
            "author": [
                "J Deepa",
                "P Madhavan"
            ],
            "pub_year": "2023",
            "venue": "Biomedical Signal Processing and Control",
            "abstract": "between encoder and decoder networks. It has extracted the  given to the encoder layer in  every transformer to render the  typical design of a multi-head self-attention module (msa) and"
        },
        "filled": false,
        "gsrank": 388,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1746809423004044",
        "author_id": [
            "",
            "gDgxBPIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:AgeHcuBVs70J:scholar.google.com/&output=cite&scirp=387&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D380%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=AgeHcuBVs70J&ei=AH8BZ6mHBaSMy9YPoKv0mAY&json=",
        "num_citations": 6,
        "citedby_url": "/scholar?cites=13669363716482074370&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:AgeHcuBVs70J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1746809423004044"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Pay attention to the activations: A modular attention mechanism for fine-grained image recognition",
            "author": [
                "P Rodriguez",
                "D Velazquez",
                "G Cucurull"
            ],
            "pub_year": "2019",
            "venue": "IEEE Transactions …",
            "abstract": "number of attention heads per module. This is implicitly done in the transformer architecture  [ By adding attention layers with AW = 1 after each layer, we obtain increasingly better results"
        },
        "filled": false,
        "gsrank": 389,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/8762109/",
        "author_id": [
            "IwBx73wAAAAJ",
            "A005Xm8AAAAJ",
            "dEtv5r4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:eHZi6lf9TecJ:scholar.google.com/&output=cite&scirp=388&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D380%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=eHZi6lf9TecJ&ei=AH8BZ6mHBaSMy9YPoKv0mAY&json=",
        "num_citations": 90,
        "citedby_url": "/scholar?cites=16667256350004377208&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:eHZi6lf9TecJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6046/4456689/08762109.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A novel hybrid framework based on temporal convolution network and transformer for network traffic prediction",
            "author": [
                "Z Zhang",
                "S Gong",
                "Z Liu",
                "D Chen"
            ],
            "pub_year": "2023",
            "venue": "Plos one",
            "abstract": "The Transformer architecture outperformed previous models  Block Attention Module (CBAM)  attention mechanism [20] to  , a layer of Transformer with a full self-attention mechanism"
        },
        "filled": false,
        "gsrank": 390,
        "pub_url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0288935",
        "author_id": [
            "",
            "4RjofccAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:pWGc8WKnN5MJ:scholar.google.com/&output=cite&scirp=389&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D380%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=pWGc8WKnN5MJ&ei=AH8BZ6mHBaSMy9YPoKv0mAY&json=",
        "num_citations": 4,
        "citedby_url": "/scholar?cites=10608131490696683941&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:pWGc8WKnN5MJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0288935&type=printable"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Conversational question answering over knowledge graphs with transformer and graph attention networks",
            "author": [
                "E Kacupaj",
                "J Plepi",
                "K Singh",
                "H Thakkar"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv …",
            "abstract": "a transformer architecture extended with Graph Attention  multi-head attention mechanism.  The decoder output is  Furthermore, we also introduce a graph attentionbased module, which"
        },
        "filled": false,
        "gsrank": 391,
        "pub_url": "https://arxiv.org/abs/2104.01569",
        "author_id": [
            "XQb-xDYAAAAJ",
            "",
            "23EKFE4AAAAJ",
            "Rq0eahkAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:1kG6_4ECFtsJ:scholar.google.com/&output=cite&scirp=390&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D390%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1kG6_4ECFtsJ&ei=yX8BZ4vwO_bYy9YPrPDH6Aw&json=",
        "num_citations": 73,
        "citedby_url": "/scholar?cites=15786808301158285782&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:1kG6_4ECFtsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2104.01569"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Depression detection in speech using transformer and parallel convolutional neural networks",
            "author": [
                "F Yin",
                "J Du",
                "X Xu",
                "L Zhao"
            ],
            "pub_year": "2023",
            "venue": "Electronics",
            "abstract": "improved transformer with a linear attention mechanism as a  the transformer module  included four transformer layers with  ) [72,73], the CNN architecture used in this paper set three"
        },
        "filled": false,
        "gsrank": 392,
        "pub_url": "https://www.mdpi.com/2079-9292/12/2/328",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:EoFSD8d0z9oJ:scholar.google.com/&output=cite&scirp=391&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D390%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EoFSD8d0z9oJ&ei=yX8BZ4vwO_bYy9YPrPDH6Aw&json=",
        "num_citations": 22,
        "citedby_url": "/scholar?cites=15766949218751774994&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:EoFSD8d0z9oJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2079-9292/12/2/328/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multi-behavior hypergraph-enhanced transformer for sequential recommendation",
            "author": [
                "Y Yang",
                "C Huang",
                "L Xia",
                "Y Liang",
                "Y Yu"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the 28th …",
            "abstract": "We propose a multi-scale Transformer module to  the overall architecture of our proposed  MBHT model which  structure in [22], we design a low-rank-based self-attention layer without"
        },
        "filled": false,
        "gsrank": 393,
        "pub_url": "https://dl.acm.org/doi/abs/10.1145/3534678.3539342",
        "author_id": [
            "kCGC_DMAAAAJ",
            "Zkv9FqwAAAAJ",
            "fDDjoUEAAAAJ",
            "n9cODgcAAAAJ",
            "8RXF4Q4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:EIB-WRcavfYJ:scholar.google.com/&output=cite&scirp=392&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D390%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EIB-WRcavfYJ&ei=yX8BZ4vwO_bYy9YPrPDH6Aw&json=",
        "num_citations": 87,
        "citedby_url": "/scholar?cites=17779395591516618768&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:EIB-WRcavfYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://dl.acm.org/doi/pdf/10.1145/3534678.3539342"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A hybrid network of cnn and transformer for lightweight image super-resolution",
            "author": [
                "J Fang",
                "H Lin",
                "X Chen",
                "K Zeng"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the IEEE …",
            "abstract": "Attention-based networks. Inspired by human visual system which can focus on significant  regions automatically, attention mechanism  Transformer layer (STL) adopts the architecture of"
        },
        "filled": false,
        "gsrank": 394,
        "pub_url": "https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Fang_A_Hybrid_Network_of_CNN_and_Transformer_for_Lightweight_Image_CVPRW_2022_paper.html",
        "author_id": [
            "",
            "",
            "",
            "HcLfTB4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:9SMpBNpdjusJ:scholar.google.com/&output=cite&scirp=393&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D390%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9SMpBNpdjusJ&ei=yX8BZ4vwO_bYy9YPrPDH6Aw&json=",
        "num_citations": 115,
        "citedby_url": "/scholar?cites=16973607236560036853&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:9SMpBNpdjusJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Fang_A_Hybrid_Network_of_CNN_and_Transformer_for_Lightweight_Image_CVPRW_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A transformer-based approach combining deep learning network and spatial-temporal information for raw EEG classification",
            "author": [
                "J Xie",
                "J Zhang",
                "J Sun",
                "Z Ma",
                "L Qin",
                "G Li"
            ],
            "pub_year": "2022",
            "venue": "… on Neural Systems …",
            "abstract": "The attention mechanism of the Transformer has  the CNN module and the  Transformer module. CNN was  : We adopted the network architecture of Transformer [25], which"
        },
        "filled": false,
        "gsrank": 395,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9845479/",
        "author_id": [
            "",
            "bRIVwJMAAAAJ",
            "",
            "",
            "",
            "mOnDB8kAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:M-GbTkdCH1wJ:scholar.google.com/&output=cite&scirp=394&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D390%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=M-GbTkdCH1wJ&ei=yX8BZ4vwO_bYy9YPrPDH6Aw&json=",
        "num_citations": 124,
        "citedby_url": "/scholar?cites=6638097249796350259&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:M-GbTkdCH1wJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/7333/4359219/09845479.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "FusionNet: a convolution–transformer fusion network for hyperspectral image classification",
            "author": [
                "L Yang",
                "Y Yang",
                "J Yang",
                "N Zhao",
                "L Wu",
                "L Wang"
            ],
            "pub_year": "2022",
            "venue": "Remote Sensing",
            "abstract": "that the network that uses the attention mechanism can learn  layer with Transformer,  and investigated Transformer  –Transformer module with an additional convolution module"
        },
        "filled": false,
        "gsrank": 396,
        "pub_url": "https://www.mdpi.com/2072-4292/14/16/4066",
        "author_id": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:_rTU2uzZkScJ:scholar.google.com/&output=cite&scirp=395&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D390%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_rTU2uzZkScJ&ei=yX8BZ4vwO_bYy9YPrPDH6Aw&json=",
        "num_citations": 44,
        "citedby_url": "/scholar?cites=2851299650409116926&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:_rTU2uzZkScJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2072-4292/14/16/4066/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "TransBoNet: Learning camera localization with Transformer Bottleneck and Attention",
            "author": [
                "X Song",
                "H Li",
                "L Liang",
                "W Shi",
                "G Xie",
                "X Lu",
                "X Hei"
            ],
            "pub_year": "2024",
            "venue": "Pattern Recognition",
            "abstract": "a framework based on hybrid attention mechanism which can be  We propose a new  attention-based neural network, which  , we use ResNet50 architecture as our basic network,"
        },
        "filled": false,
        "gsrank": 397,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0031320323006738",
        "author_id": [
            "",
            "",
            "",
            "hGvdUjEAAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:LwoQDR0zPgEJ:scholar.google.com/&output=cite&scirp=396&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D390%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LwoQDR0zPgEJ&ei=yX8BZ4vwO_bYy9YPrPDH6Aw&json=",
        "num_citations": 7,
        "citedby_url": "/scholar?cites=89565242460211759&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:LwoQDR0zPgEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0031320323006738"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "CSwin-PNet: A CNN-Swin Transformer combined pyramid network for breast lesion segmentation in ultrasound images",
            "author": [
                "H Yang",
                "D Yang"
            ],
            "pub_year": "2023",
            "venue": "Expert Systems with Applications",
            "abstract": "channel attention (ICA) module using channel-wise attention to  architecture that is used  for the construction of global  the attention mechanism can encourage the network to pay"
        },
        "filled": false,
        "gsrank": 398,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0957417422020425",
        "author_id": [
            "3fjqRuUAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:jMrT_v7tvFEJ:scholar.google.com/&output=cite&scirp=397&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D390%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jMrT_v7tvFEJ&ei=yX8BZ4vwO_bYy9YPrPDH6Aw&json=",
        "num_citations": 76,
        "citedby_url": "/scholar?cites=5889844092146535052&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:jMrT_v7tvFEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0957417422020425"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Low-rank and locality constrained self-attention for sequence modeling",
            "author": [
                "Q Guo",
                "X Qiu",
                "X Xue",
                "Z Zhang"
            ],
            "pub_year": "2019",
            "venue": "IEEE/ACM Transactions on …",
            "abstract": ", we parameterize the selfattention module with two constraints the learned attention matrix  with the Transformer architecture.  layer structure, we further design a Transformerlike model"
        },
        "filled": false,
        "gsrank": 399,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/8894858/",
        "author_id": [
            "k3mPGKgAAAAJ",
            "Pq4Yp_kAAAAJ",
            "DTbhX6oAAAAJ",
            "k0KiE4wAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:KTkNqeQdRCsJ:scholar.google.com/&output=cite&scirp=398&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D390%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KTkNqeQdRCsJ&ei=yX8BZ4vwO_bYy9YPrPDH6Aw&json=",
        "num_citations": 31,
        "citedby_url": "/scholar?cites=3117649709973190953&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:KTkNqeQdRCsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6570655/6633080/08894858.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Optimization-inspired cross-attention transformer for compressive sensing",
            "author": [
                "J Song",
                "C Mou",
                "S Wang",
                "S Ma"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the …",
            "abstract": "effect by a cross attention mechanism between adjacent iterations.  Architecture of our OCTUF,  which consists of K iterations. x  a Dual Cross Attention (DualCA) sub-module to efficiently"
        },
        "filled": false,
        "gsrank": 400,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2023/html/Song_Optimization-Inspired_Cross-Attention_Transformer_for_Compressive_Sensing_CVPR_2023_paper.html",
        "author_id": [
            "EBOtupAAAAAJ",
            "SYQoDk0AAAAJ",
            "Pr7s2VUAAAAJ",
            "y3YqlaUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Z0shOAPAp34J:scholar.google.com/&output=cite&scirp=399&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D390%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Z0shOAPAp34J&ei=yX8BZ4vwO_bYy9YPrPDH6Aw&json=",
        "num_citations": 23,
        "citedby_url": "/scholar?cites=9126474289948740455&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Z0shOAPAp34J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Song_Optimization-Inspired_Cross-Attention_Transformer_for_Compressive_Sensing_CVPR_2023_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformers in medical image analysis",
            "author": [
                "K He",
                "C Gan",
                "Z Li",
                "I Rekik",
                "Z Yin",
                "W Ji",
                "Y Gao",
                "Q Wang"
            ],
            "pub_year": "2023",
            "venue": "Intelligent …",
            "abstract": "the core principle of the attention mechanism, followed by a  A typical transformer  architecture is shown in Figure 3 and  As an attention-based network, transformer is suitable for"
        },
        "filled": false,
        "gsrank": 401,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S2667102622000717",
        "author_id": [
            "0Do_BMIAAAAJ",
            "",
            "",
            "tb6CVoAAAAAJ",
            "",
            "TVDayAYAAAAJ",
            "k0jjQo8AAAAJ",
            "m6ZNDewAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Gf885gn7OoQJ:scholar.google.com/&output=cite&scirp=400&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D400%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Gf885gn7OoQJ&ei=k4ABZ4ShO4HOy9YPxaCm4AE&json=",
        "num_citations": 273,
        "citedby_url": "/scholar?cites=9528203981591740185&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Gf885gn7OoQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S2667102622000717"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "An effective CNN and Transformer complementary network for medical image segmentation",
            "author": [
                "F Yuan",
                "Z Zhang",
                "Z Fang"
            ],
            "pub_year": "2023",
            "venue": "Pattern Recognition",
            "abstract": "We propose an effective Feature Complementary Module (FCM words, our CAB is factually  a mixed attention mechanism.  , which is a pure transformer architecture. The variant has an"
        },
        "filled": false,
        "gsrank": 402,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0031320322007075",
        "author_id": [
            "",
            "",
            "bd7u8-0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Ab-znesuF8gJ:scholar.google.com/&output=cite&scirp=401&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D400%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Ab-znesuF8gJ&ei=k4ABZ4ShO4HOy9YPxaCm4AE&json=",
        "num_citations": 222,
        "citedby_url": "/scholar?cites=14418044321547927297&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Ab-znesuF8gJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0031320322007075"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Shuffle transformer: Rethinking spatial shuffle for vision transformer",
            "author": [
                "Z Huang",
                "Y Ben",
                "G Luo",
                "P Cheng",
                "G Yu"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv …",
            "abstract": "layer with a residual connection into the Shuffle Transformer  module into the shuffle  transformer block for building rich crosswindow connections. Finally, the overall network architecture"
        },
        "filled": false,
        "gsrank": 403,
        "pub_url": "https://arxiv.org/abs/2106.03650",
        "author_id": [
            "GW9vw8UAAAAJ",
            "SVYUvrYAAAAJ",
            "HHgJmnMAAAAJ",
            "",
            "BJdigYsAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:M2jKrQBMpT8J:scholar.google.com/&output=cite&scirp=402&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D400%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=M2jKrQBMpT8J&ei=k4ABZ4ShO4HOy9YPxaCm4AE&json=",
        "num_citations": 202,
        "citedby_url": "/scholar?cites=4586155361346152499&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:M2jKrQBMpT8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2106.03650"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Remaining useful life prediction via a deep adaptive transformer framework enhanced by graph attention network",
            "author": [
                "P Liang",
                "Y Li",
                "B Wang",
                "X Yuan",
                "L Zhang"
            ],
            "pub_year": "2023",
            "venue": "International Journal of Fatigue",
            "abstract": "the attention mechanism to calculate the attention coefficients  Therefore, we propose a DAT  module for RUL estimation to  choice of neural network architecture and model parameters"
        },
        "filled": false,
        "gsrank": 404,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0142112323002232",
        "author_id": [
            "Su1ubtUAAAAJ",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:8luJwMTgLqIJ:scholar.google.com/&output=cite&scirp=403&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D400%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8luJwMTgLqIJ&ei=k4ABZ4ShO4HOy9YPxaCm4AE&json=",
        "num_citations": 21,
        "citedby_url": "/scholar?cites=11686525218721455090&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:8luJwMTgLqIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0142112323002232"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Attention retractable frequency fusion transformer for image super resolution",
            "author": [
                "Q Zhu",
                "P Li",
                "Q Li"
            ],
            "pub_year": "2023",
            "venue": "… of the IEEE/CVF Conference on …",
            "abstract": "], and attention mechanism, eg, residual channel attention  The architecture of our ARFFT  is illustrated in Fig. 1. It is  Transformer [16] was developed using the multiaxis attention based"
        },
        "filled": false,
        "gsrank": 405,
        "pub_url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Zhu_Attention_Retractable_Frequency_Fusion_Transformer_for_Image_Super_Resolution_CVPRW_2023_paper.html",
        "author_id": [
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:RbFaBpO8q8UJ:scholar.google.com/&output=cite&scirp=404&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D400%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RbFaBpO8q8UJ&ei=k4ABZ4ShO4HOy9YPxaCm4AE&json=",
        "num_citations": 16,
        "citedby_url": "/scholar?cites=14243685586142146885&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:RbFaBpO8q8UJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Zhu_Attention_Retractable_Frequency_Fusion_Transformer_for_Image_Super_Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Vicinity vision transformer",
            "author": [
                "W Sun",
                "Z Qin",
                "H Deng",
                "J Wang",
                "Y Zhang"
            ],
            "pub_year": "2023",
            "venue": "… on Pattern Analysis …",
            "abstract": "into each layer throughout the whole model via convolutional  self-attention (MSA) module  to reduce the feature dimension  the overall architecture of Vicinity Vision Transformer (VVT)"
        },
        "filled": false,
        "gsrank": 406,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10149455/",
        "author_id": [
            "",
            "KTGwGNEAAAAJ",
            "",
            "2wk2RdgAAAAJ",
            "hzR7V5AAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:9Gg1hvYLUUIJ:scholar.google.com/&output=cite&scirp=405&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D400%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=9Gg1hvYLUUIJ&ei=k4ABZ4ShO4HOy9YPxaCm4AE&json=",
        "num_citations": 23,
        "citedby_url": "/scholar?cites=4778613833058314484&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:9Gg1hvYLUUIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/34/4359286/10149455.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Centerformer: Center-based transformer for 3d object detection",
            "author": [
                "Z Zhou",
                "X Zhao",
                "Y Wang",
                "P Wang"
            ],
            "pub_year": "2022",
            "venue": "European Conference on …",
            "abstract": "transformer module, we use a deformable cross attention  The architecture of our model is  illustrated in Fig. 2. We use a  the fusion due to the attention mechanism. To further explore the"
        },
        "filled": false,
        "gsrank": 407,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-031-19839-7_29",
        "author_id": [
            "0SnTURUAAAAJ",
            "fvWS59UAAAAJ",
            "uNj2w9gAAAAJ",
            "z1CFfzwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:uv6mifwyiacJ:scholar.google.com/&output=cite&scirp=406&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D400%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uv6mifwyiacJ&ei=k4ABZ4ShO4HOy9YPxaCm4AE&json=",
        "num_citations": 140,
        "citedby_url": "/scholar?cites=12072236336365895354&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:uv6mifwyiacJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2209.05588"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Dilated transformer: residual axial attention for breast ultrasound image segmentation",
            "author": [
                "X Shen",
                "L Wang",
                "Y Zhao",
                "R Liu",
                "W Qian"
            ],
            "pub_year": "2022",
            "venue": "Quantitative Imaging in …",
            "abstract": "axial attention scores to the axial attention mechanism,  of the multi-head on both height-layer  and width-axial layer  dilated convolution module which is often used in the architecture of"
        },
        "filled": false,
        "gsrank": 408,
        "pub_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9403584/",
        "author_id": [
            "",
            "",
            "",
            "35NnfTsAAAAJ",
            "PWabr-YAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:3QxHdz5VaRUJ:scholar.google.com/&output=cite&scirp=407&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D400%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3QxHdz5VaRUJ&ei=k4ABZ4ShO4HOy9YPxaCm4AE&json=",
        "num_citations": 17,
        "citedby_url": "/scholar?cites=1542858074128583901&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:3QxHdz5VaRUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9403584/"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Diffusion kernel attention network for brain disorder classification",
            "author": [
                "J Zhang",
                "L Zhou",
                "L Wang",
                "M Liu"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on …",
            "abstract": "attention to replace the original dot-product attention module  of the architecture of the  original Transformer and specific  attention mechanism is proposed. With this, the improved"
        },
        "filled": false,
        "gsrank": 409,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9763540/",
        "author_id": [
            "5U3237kAAAAJ",
            "SgofT2MAAAAJ",
            "5ClujcoAAAAJ",
            "vK9V8e0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:JmKVa54gbuYJ:scholar.google.com/&output=cite&scirp=408&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D400%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JmKVa54gbuYJ&ei=k4ABZ4ShO4HOy9YPxaCm4AE&json=",
        "num_citations": 35,
        "citedby_url": "/scholar?cites=16604244740943471142&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:JmKVa54gbuYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/42/4359023/09763540.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Rethinking U‐net from an attention perspective with transformers for osteosarcoma MRI image segmentation",
            "author": [
                "T Ouyang",
                "S Yang",
                "F Gou",
                "Z Dai"
            ],
            "pub_year": "2022",
            "venue": "Computational …",
            "abstract": "osteosarcoma image segmentation architecture, UATransNet , multilevel guided attention  mechanism, and multiscale jump  model with a multilevel guided self-aware attention module ("
        },
        "filled": false,
        "gsrank": 410,
        "pub_url": "https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/7973404",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:m-0drz2ejl8J:scholar.google.com/&output=cite&scirp=409&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D400%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=m-0drz2ejl8J&ei=k4ABZ4ShO4HOy9YPxaCm4AE&json=",
        "num_citations": 32,
        "citedby_url": "/scholar?cites=6885614868064234907&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:m-0drz2ejl8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://onlinelibrary.wiley.com/doi/pdf/10.1155/2022/7973404"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer tracking with multi-scale dual-attention",
            "author": [
                "J Wang",
                "C Lai",
                "W Zhang",
                "Y Wang",
                "C Meng"
            ],
            "pub_year": "2023",
            "venue": "Complex & Intelligent Systems",
            "abstract": "Attention mechanism in Transformer can fully explore the  In this paper, a novel multi-scale  dual-attention-based tracking  In this work, a Siamese network architecture is designed."
        },
        "filled": false,
        "gsrank": 411,
        "pub_url": "https://link.springer.com/article/10.1007/s40747-023-01043-1",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:uhYWPLol2pIJ:scholar.google.com/&output=cite&scirp=410&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D410%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=uhYWPLol2pIJ&ei=XoEBZ-vJK6yCy9YPseaHkQg&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=10581811756262627002&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:uhYWPLol2pIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s40747-023-01043-1.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A hybrid enhanced attention transformer network for medical ultrasound image segmentation",
            "author": [
                "T Jiang",
                "W Xing",
                "M Yu",
                "D Ta"
            ],
            "pub_year": "2023",
            "venue": "Biomedical Signal Processing and Control",
            "abstract": "a channel enhanced self-attention-based transformer (ECAT 1, the basic architecture of our  proposed HEAT-Net is a U-Net its insertion at a shallow network layer would introduce more"
        },
        "filled": false,
        "gsrank": 412,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1746809423007620",
        "author_id": [
            "",
            "ZIGEbjoAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:jW5OPiS0j-UJ:scholar.google.com/&output=cite&scirp=411&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D410%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jW5OPiS0j-UJ&ei=XoEBZ-vJK6yCy9YPseaHkQg&json=",
        "num_citations": 6,
        "citedby_url": "/scholar?cites=16541638024112270989&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:jW5OPiS0j-UJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1746809423007620"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Lightweight transformer image feature extraction network",
            "author": [
                "W Zheng",
                "S Lu",
                "Y Yang",
                "Z Yin",
                "L Yin"
            ],
            "pub_year": "2024",
            "venue": "PeerJ Computer Science",
            "abstract": "Two changes were made to the Transformer architecture,  are compared with the attention  mechanism that retains two  After the model lightweight token pruning module is introduced"
        },
        "filled": false,
        "gsrank": 413,
        "pub_url": "https://peerj.com/articles/cs-1755/",
        "author_id": [
            "0pRRjPgAAAAJ",
            "SPIm-_UAAAAJ",
            "",
            "doM8Z-4AAAAJ",
            "sxmGyFEAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:sFASE253XWsJ:scholar.google.com/&output=cite&scirp=412&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D410%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=sFASE253XWsJ&ei=XoEBZ-vJK6yCy9YPseaHkQg&json=",
        "num_citations": 64,
        "citedby_url": "/scholar?cites=7736471049542455472&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:sFASE253XWsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://peerj.com/articles/cs-1755.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multi-scale hierarchical vision transformer with cascaded attention decoding for medical image segmentation",
            "author": [
                "MM Rahman",
                "R Marculescu"
            ],
            "pub_year": "2024",
            "venue": "Medical Imaging with Deep …",
            "abstract": "an attention-based decoder, namely Cascaded Attention  as follows: • New Transformer  Architecture: We propose a  vision transformer using a spatial reduction attention mechanism."
        },
        "filled": false,
        "gsrank": 414,
        "pub_url": "https://proceedings.mlr.press/v227/rahman24a.html",
        "author_id": [
            "e0SzHPMAAAAJ",
            "ZCmYP5cAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:gdVW9fHbaS8J:scholar.google.com/&output=cite&scirp=413&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D410%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=gdVW9fHbaS8J&ei=XoEBZ-vJK6yCy9YPseaHkQg&json=",
        "num_citations": 37,
        "citedby_url": "/scholar?cites=3416503624586941825&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:gdVW9fHbaS8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.mlr.press/v227/rahman24a/rahman24a.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Self-attention encoding and pooling for speaker recognition",
            "author": [
                "P Safari",
                "M India",
                "J Hernando"
            ],
            "pub_year": "2020",
            "venue": "arXiv preprint arXiv:2008.01077",
            "abstract": "[8] for a Transformer architecture and appeared very effective  employ multi-head attention  mechanism in the pooling layer to  This layer is an additive attention based mechanism, which"
        },
        "filled": false,
        "gsrank": 415,
        "pub_url": "https://arxiv.org/abs/2008.01077",
        "author_id": [
            "MtAVgCQAAAAJ",
            "xwXNJl4AAAAJ",
            "dTPbsfMAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:_88NfFnLpZgJ:scholar.google.com/&output=cite&scirp=414&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D410%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_88NfFnLpZgJ&ei=XoEBZ-vJK6yCy9YPseaHkQg&json=",
        "num_citations": 92,
        "citedby_url": "/scholar?cites=10999421250116112383&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:_88NfFnLpZgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2008.01077"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Video transformer network",
            "author": [
                "D Neimark",
                "O Bar",
                "M Zohar"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the …",
            "abstract": "Followed by a temporal attention-based encoder (Longformer in this  NLN demonstrated  that the core attention mechanism in Transformers can  1 demonstrates our architecture layout."
        },
        "filled": false,
        "gsrank": 416,
        "pub_url": "http://openaccess.thecvf.com/content/ICCV2021W/CVEU/html/Neimark_Video_Transformer_Network_ICCVW_2021_paper.html",
        "author_id": [
            "TBRi8vAAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:bjnXbAyK9cwJ:scholar.google.com/&output=cite&scirp=415&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D410%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bjnXbAyK9cwJ&ei=XoEBZ-vJK6yCy9YPseaHkQg&json=",
        "num_citations": 532,
        "citedby_url": "/scholar?cites=14768862339001694574&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:bjnXbAyK9cwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/ICCV2021W/CVEU/papers/Neimark_Video_Transformer_Network_ICCVW_2021_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Glit: Neural architecture search for global and local image transformer",
            "author": [
                "B Chen",
                "P Li",
                "C Li",
                "B Li",
                "L Bai",
                "C Lin"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the …",
            "abstract": "Specifically, we introduce a locality module that models the  All the above methods manually  design attention mechanism  We utilize 1D convolution layer instead of 2D convolution layer"
        },
        "filled": false,
        "gsrank": 417,
        "pub_url": "http://openaccess.thecvf.com/content/ICCV2021/html/Chen_GLiT_Neural_Architecture_Search_for_Global_and_Local_Image_Transformer_ICCV_2021_paper.html",
        "author_id": [
            "o5wjqPEAAAAJ",
            "O9wW1OQAAAAJ",
            "",
            "",
            "sakOO04AAAAJ",
            "rObgGWIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:JTcD1_PjfZsJ:scholar.google.com/&output=cite&scirp=416&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D410%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JTcD1_PjfZsJ&ei=XoEBZ-vJK6yCy9YPseaHkQg&json=",
        "num_citations": 108,
        "citedby_url": "/scholar?cites=11204362084391532325&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:JTcD1_PjfZsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_GLiT_Neural_Architecture_Search_for_Global_and_Local_Image_Transformer_ICCV_2021_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Advancing drug discovery with deep attention neural networks",
            "author": [
                "A Lavecchia"
            ],
            "pub_year": "2024",
            "venue": "Drug Discovery Today",
            "abstract": "attention mechanism and its extended architectures, including graph attention networks (GATs),  transformers The present article focuses on recent advancements of attention-based"
        },
        "filled": false,
        "gsrank": 418,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1359644624001922",
        "author_id": [
            "TAm9ir8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:SP_q8ZNwaK4J:scholar.google.com/&output=cite&scirp=417&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D410%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=SP_q8ZNwaK4J&ei=XoEBZ-vJK6yCy9YPseaHkQg&json=",
        "num_citations": 3,
        "citedby_url": "/scholar?cites=12567418540898582344&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:SP_q8ZNwaK4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1359644624001922"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Fusformer: A transformer-based fusion network for hyperspectral image super-resolution",
            "author": [
                "JF Hu",
                "TZ Huang",
                "LJ Deng",
                "HX Dou"
            ],
            "pub_year": "2022",
            "venue": "… and Remote Sensing …",
            "abstract": "transformer module, a network architecture (called Fusformer) is designed for the HISR  problem. Our method integrates a self-attention mechanism  With a simple fully connected layer,"
        },
        "filled": false,
        "gsrank": 419,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9841513/",
        "author_id": [
            "hT-EiJEAAAAJ",
            "",
            "TZs9NxkAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Y_VL4yy1P_UJ:scholar.google.com/&output=cite&scirp=418&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D410%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Y_VL4yy1P_UJ&ei=XoEBZ-vJK6yCy9YPseaHkQg&json=",
        "num_citations": 121,
        "citedby_url": "/scholar?cites=17672042667221710179&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Y_VL4yy1P_UJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/8859/4357975/09841513.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multimodal transformer for accelerated MR imaging",
            "author": [
                "CM Feng",
                "Y Yan",
                "G Chen",
                "Y Xu",
                "Y Hu"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on …",
            "abstract": "multi-head attention mechanism, named cross attention module propose a novel transformer  architecture, named MTrans, to  task, we add a sub-pixel convolutional layer [66] to Tailtar to"
        },
        "filled": false,
        "gsrank": 420,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9796552/",
        "author_id": [
            "g2nqHBcAAAAJ",
            "Ja0QBOgAAAAJ",
            "sJGCnjsAAAAJ",
            "zOVgYQYAAAAJ",
            "5dLafosAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:TNNyItB4Zq0J:scholar.google.com/&output=cite&scirp=419&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D410%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TNNyItB4Zq0J&ei=XoEBZ-vJK6yCy9YPseaHkQg&json=",
        "num_citations": 85,
        "citedby_url": "/scholar?cites=12494807051512501068&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:TNNyItB4Zq0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/42/4359023/09796552.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Activating more pixels in image super-resolution transformer",
            "author": [
                "X Chen",
                "X Wang",
                "J Zhou",
                "Y Qiao"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the …",
            "abstract": "Besides, we introduce an overlapping crossattention module to achieve more direct   self-attention. Our OCAB consists of an overlapping cross-attention (OCA) layer and an MLP layer"
        },
        "filled": false,
        "gsrank": 421,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_Activating_More_Pixels_in_Image_Super-Resolution_Transformer_CVPR_2023_paper.html",
        "author_id": [
            "oHQHAYcAAAAJ",
            "FQgZpQoAAAAJ",
            "mcROAxAAAAAJ",
            "gFtI-8QAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:RdApUCpq-0QJ:scholar.google.com/&output=cite&scirp=420&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D420%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RdApUCpq-0QJ&ei=KYIBZ-KqI6yCy9YPseaHkQg&json=",
        "num_citations": 554,
        "citedby_url": "/scholar?cites=4970683343699562565&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:RdApUCpq-0QJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Activating_More_Pixels_in_Image_Super-Resolution_Transformer_CVPR_2023_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Ds-transunet: Dual swin transformer u-net for medical image segmentation",
            "author": [
                "A Lin",
                "B Chen",
                "J Xu",
                "Z Zhang",
                "G Lu"
            ],
            "pub_year": "2022",
            "venue": "IEEE Transactions on …",
            "abstract": "a well-established transformer interactive fusion (TIF) module.  Res-UNet [20] adds a weighted  attention mechanism and  UNet-like architecture which applies swin transformer block to"
        },
        "filled": false,
        "gsrank": 422,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9785614/",
        "author_id": [
            "yz-PwdUAAAAJ",
            "dE0UAg0AAAAJ",
            "",
            "tpVOb2EAAAAJ",
            "fhwB7UwAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:kpwj1awscgMJ:scholar.google.com/&output=cite&scirp=421&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D420%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kpwj1awscgMJ&ei=KYIBZ-KqI6yCy9YPseaHkQg&json=",
        "num_citations": 581,
        "citedby_url": "/scholar?cites=248310050280676498&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:kpwj1awscgMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/19/4407674/09785614.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A time-aware self-attention based neural network model for sequential recommendation",
            "author": [
                "Y Zhang",
                "B Yang",
                "H Liu",
                "D Li"
            ],
            "pub_year": "2023",
            "venue": "Applied Soft Computing",
            "abstract": "In this paper, we propose a Time-Aware Transformer for  Essentially the idea behind the  attention mechanism is to  accelerate neural network training, following [14], [17], we apply Layer"
        },
        "filled": false,
        "gsrank": 423,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1568494622009437",
        "author_id": [
            "",
            "H2Oo_MkAAAAJ",
            "",
            "VNg5rA8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:8Fxlb41QWwMJ:scholar.google.com/&output=cite&scirp=422&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D420%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=8Fxlb41QWwMJ&ei=KYIBZ-KqI6yCy9YPseaHkQg&json=",
        "num_citations": 25,
        "citedby_url": "/scholar?cites=241875573383978224&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:8Fxlb41QWwMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1568494622009437"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Medical image segmentation via cascaded attention decoding",
            "author": [
                "MM Rahman",
                "R Marculescu"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the IEEE/CVF …",
            "abstract": "Transformers rely on an attention-based network architecture;  Finally, we send the output  from each CAM layer to a  It is evident from Figure 2 that the attention mechanism used in our"
        },
        "filled": false,
        "gsrank": 424,
        "pub_url": "https://openaccess.thecvf.com/content/WACV2023/html/Rahman_Medical_Image_Segmentation_via_Cascaded_Attention_Decoding_WACV_2023_paper.html",
        "author_id": [
            "e0SzHPMAAAAJ",
            "ZCmYP5cAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:7bia2iiYhcAJ:scholar.google.com/&output=cite&scirp=423&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D420%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7bia2iiYhcAJ&ei=KYIBZ-KqI6yCy9YPseaHkQg&json=",
        "num_citations": 131,
        "citedby_url": "/scholar?cites=13872661528418367725&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:7bia2iiYhcAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/WACV2023/papers/Rahman_Medical_Image_Segmentation_via_Cascaded_Attention_Decoding_WACV_2023_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "KEAHT: A knowledge-enriched attention-based hybrid transformer model for social sentiment analysis",
            "author": [
                "D Tiwari",
                "B Nagpal"
            ],
            "pub_year": "2022",
            "venue": "New Generation Computing",
            "abstract": "It provides the facility of attention mechanism and can solve  The whole architecture of  the proposed KEAHT model is  hybrid model by adding the knowledge-enriched layer in the"
        },
        "filled": false,
        "gsrank": 425,
        "pub_url": "https://link.springer.com/article/10.1007/s00354-022-00182-2",
        "author_id": [
            "MkuAGToAAAAJ",
            "9GT8SFIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Z8q2AL4pBt4J:scholar.google.com/&output=cite&scirp=424&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D420%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Z8q2AL4pBt4J&ei=KYIBZ-KqI6yCy9YPseaHkQg&json=",
        "num_citations": 21,
        "citedby_url": "/scholar?cites=15998520622312770151&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Z8q2AL4pBt4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s00354-022-00182-2.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Hand-transformer: Non-autoregressive structured modeling for 3d hand pose estimation",
            "author": [
                "L Huang",
                "J Tan",
                "J Liu",
                "J Yuan"
            ],
            "pub_year": "2020",
            "venue": "… Conference, Glasgow, UK, August 23–28 …",
            "abstract": "propose to leverage the Transformer architecture with a novel  -decoder attention mechanism  used in the Transformer to  masked self-attention module of the traditional Transformer"
        },
        "filled": false,
        "gsrank": 426,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-58595-2_2",
        "author_id": [
            "Kwhor_EAAAAJ",
            "1Gywy80AAAAJ",
            "RRzVwKkAAAAJ",
            "fJ7seq0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:TuvrFvAMBJkJ:scholar.google.com/&output=cite&scirp=425&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D420%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TuvrFvAMBJkJ&ei=KYIBZ-KqI6yCy9YPseaHkQg&json=",
        "num_citations": 124,
        "citedby_url": "/scholar?cites=11025952013026061134&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:TuvrFvAMBJkJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700018.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Defect transformer: An efficient hybrid transformer architecture for surface defect detection",
            "author": [
                "J Wang",
                "G Xu",
                "F Yan",
                "J Wang",
                "Z Wang"
            ],
            "pub_year": "2023",
            "venue": "Measurement",
            "abstract": "setting of encoder module in our network architecture. The  of upsampling operation and  convolution layer utilized in this paper  it with the self-attention modules as in most transformers,"
        },
        "filled": false,
        "gsrank": 427,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0263224123001781",
        "author_id": [
            "9inBwf8AAAAJ",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:jOIb01ihOc8J:scholar.google.com/&output=cite&scirp=426&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D420%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=jOIb01ihOc8J&ei=KYIBZ-KqI6yCy9YPseaHkQg&json=",
        "num_citations": 36,
        "citedby_url": "/scholar?cites=14932143442394604172&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:jOIb01ihOc8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0263224123001781"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Weighted residual self-attention graph-based transformer for spectral–spatial hyperspectral image classification",
            "author": [
                "B Zu",
                "H Wang",
                "J Li",
                "Z He",
                "Y Li",
                "Z Yin"
            ],
            "pub_year": "2023",
            "venue": "International Journal of …",
            "abstract": "Therefore, in order to improve the layer-to-layer  The transformer architecture is performing  well due to its  Residual Self-attention Graph-based Transformer for the self-attention module ("
        },
        "filled": false,
        "gsrank": 428,
        "pub_url": "https://www.tandfonline.com/doi/abs/10.1080/01431161.2023.2171744",
        "author_id": [
            "",
            "",
            "yMRStxYAAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:s4KTwFHfXSEJ:scholar.google.com/&output=cite&scirp=427&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D420%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=s4KTwFHfXSEJ&ei=KYIBZ-KqI6yCy9YPseaHkQg&json=",
        "num_citations": 3,
        "citedby_url": "/scholar?cites=2404323318301950643&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:s4KTwFHfXSEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Boosted transformer for image captioning",
            "author": [
                "J Li",
                "P Yao",
                "L Guo",
                "W Zhang"
            ],
            "pub_year": "2019",
            "venue": "Applied Sciences",
            "abstract": "We apply the transformer architecture to the image captioning  In this paper, we stack a  vision-guided attention mechanism  a CGA module in the first encoder layer and a VGA module in"
        },
        "filled": false,
        "gsrank": 429,
        "pub_url": "https://www.mdpi.com/2076-3417/9/16/3260",
        "author_id": [
            "",
            "M4R9-KEAAAAJ",
            "OaGRHWYAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:F06x4CvasXEJ:scholar.google.com/&output=cite&scirp=428&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D420%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=F06x4CvasXEJ&ei=KYIBZ-KqI6yCy9YPseaHkQg&json=",
        "num_citations": 37,
        "citedby_url": "/scholar?cites=8192569079151808023&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:F06x4CvasXEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2076-3417/9/16/3260/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Double attention transformer for hyperspectral image classification",
            "author": [
                "P Tang",
                "M Zhang",
                "Z Liu",
                "R Song"
            ],
            "pub_year": "2023",
            "venue": "IEEE Geoscience and …",
            "abstract": "is devised for cross-layer feature fusion. Experimental results  Fig.1 shows the architecture  of the proposed framework, which  networks including only one type of self-attention module,"
        },
        "filled": false,
        "gsrank": 430,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10052741/",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:yChLbZlLuEIJ:scholar.google.com/&output=cite&scirp=429&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D420%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yChLbZlLuEIJ&ei=KYIBZ-KqI6yCy9YPseaHkQg&json=",
        "num_citations": 25,
        "citedby_url": "/scholar?cites=4807675724553726152&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:yChLbZlLuEIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/8859/4357975/10052741.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Rethinking graph transformers with spectral attention",
            "author": [
                "D Kreuzer",
                "D Beaini",
                "W Hamilton"
            ],
            "pub_year": "2021",
            "venue": "Advances in …",
            "abstract": ", and outperforms any attention-based model by a wide margin passing model, and we  demonstrate that our SAN architecture  Since we use a similar attention mechanism, our code was"
        },
        "filled": false,
        "gsrank": 431,
        "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2021/hash/b4fd1d2cb085390fbbadae65e07876a7-Abstract.html",
        "author_id": [
            "dpdDE3gAAAAJ",
            "ScCOIx0AAAAJ",
            "T5tm9eQAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:kWUZxuQ0Ud0J:scholar.google.com/&output=cite&scirp=430&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D430%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kWUZxuQ0Ud0J&ei=9IIBZ83AM9KAy9YPm-_p6Ao&json=",
        "num_citations": 487,
        "citedby_url": "/scholar?cites=15947585912676378001&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:kWUZxuQ0Ud0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/b4fd1d2cb085390fbbadae65e07876a7-Paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Facial Expression Recognition Based on Vision Transformer with Hybrid Local Attention",
            "author": [
                "Y Tian",
                "J Zhu",
                "H Yao",
                "D Chen"
            ],
            "pub_year": "2024",
            "venue": "Applied Sciences",
            "abstract": "without changing the architecture of the network model itself.  In this paper, we propose a  local hybrid attention mechanism  Transformer with the proposed hybrid local attention module"
        },
        "filled": false,
        "gsrank": 432,
        "pub_url": "https://www.mdpi.com/2076-3417/14/15/6471",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:UFXlVVjujegJ:scholar.google.com/&output=cite&scirp=431&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D430%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=UFXlVVjujegJ&ei=9IIBZ83AM9KAy9YPm-_p6Ao&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:UFXlVVjujegJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2076-3417/14/15/6471"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Domain adaptation via bidirectional cross-attention transformer",
            "author": [
                "X Wang",
                "P Guo",
                "Y Zhang"
            ],
            "pub_year": "2022",
            "venue": "arXiv preprint arXiv:2201.05887",
            "abstract": "transformer with a bidirectional cross-attention mechanism to  We leverage the cross-attention  module to produce mixup  As shown in the top of Figure 3, the overall architecture of the"
        },
        "filled": false,
        "gsrank": 433,
        "pub_url": "https://arxiv.org/abs/2201.05887",
        "author_id": [
            "",
            "v1oYGZQAAAAJ",
            "jaRS5w4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:NBctQ-KF1FAJ:scholar.google.com/&output=cite&scirp=432&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D430%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NBctQ-KF1FAJ&ei=9IIBZ83AM9KAy9YPm-_p6Ao&json=",
        "num_citations": 29,
        "citedby_url": "/scholar?cites=5824427424933025588&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:NBctQ-KF1FAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2201.05887"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Informer: Beyond efficient transformer for long sequence time-series forecasting",
            "author": [
                "H Zhou",
                "S Zhang",
                "J Peng",
                "S Zhang",
                "J Li"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the …",
            "abstract": "of the self-attention mechanism and Transformer architecture  ProbSparse Self-attention  Based on the proposed mea We add a max-pooling layer with stride 2 and downsample Xt"
        },
        "filled": false,
        "gsrank": 434,
        "pub_url": "http://ojs.aaai.org/index.php/AAAI/article/view/17325",
        "author_id": [
            "mbrFlN0AAAAJ",
            "voqw10cAAAAJ",
            "",
            "VpCt3hMAAAAJ",
            "EY2lqD0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:VigqltkXN1QJ:scholar.google.com/&output=cite&scirp=433&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D430%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VigqltkXN1QJ&ei=9IIBZ83AM9KAy9YPm-_p6Ao&json=",
        "num_citations": 3609,
        "citedby_url": "/scholar?cites=6068345246199720022&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:VigqltkXN1QJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17325/17132"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Easy attention: A simple self-attention mechanism for transformer-based time-series reconstruction and prediction",
            "author": [
                "R Vinuesa",
                "M Sanchis-Agudo",
                "Y Wang",
                "L Guastoni"
            ],
            "pub_year": "2023",
            "venue": "NA",
            "abstract": ", we propose a novel attention mechanism called easy attention which we demonstrate in   sparse multi-head easy attention as the easy attention module on the proposed architecture"
        },
        "filled": false,
        "gsrank": 435,
        "pub_url": "https://www.researchsquare.com/article/rs-3545247/latest",
        "author_id": [
            "UbyF8_oAAAAJ",
            "egkPCv4AAAAJ",
            "",
            "CjwzqlcAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:EklxoCg3GmUJ:scholar.google.com/&output=cite&scirp=434&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D430%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EklxoCg3GmUJ&ei=9IIBZ83AM9KAy9YPm-_p6Ao&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:EklxoCg3GmUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.researchsquare.com/article/rs-3545247/latest.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multi-focus image fusion: Transformer and shallow feature attention matters",
            "author": [
                "P Wu",
                "L Jiang",
                "Z Hua",
                "J Li"
            ],
            "pub_year": "2023",
            "venue": "Displays",
            "abstract": "network based on Transformer and attention mechanism,  decoding module has no  intermediate attention mechanism,  of the network architecture composed of the two Transformer"
        },
        "filled": false,
        "gsrank": 436,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0141938222001718",
        "author_id": [
            "",
            "65AeQaYAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:knpb7aTBvBMJ:scholar.google.com/&output=cite&scirp=435&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D430%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=knpb7aTBvBMJ&ei=9IIBZ83AM9KAy9YPm-_p6Ao&json=",
        "num_citations": 11,
        "citedby_url": "/scholar?cites=1422224496443226770&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:knpb7aTBvBMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0141938222001718"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Dependency graph enhanced dual-transformer structure for aspect-based sentiment classification",
            "author": [
                "H Tang",
                "D Ji",
                "C Li",
                "Q Zhou"
            ],
            "pub_year": "2020",
            "venue": "… of the 58th annual meeting of the …",
            "abstract": "The network architecture of our proposed DGEDT is shown  maxpooling and apply an  attention module to align contextual  Here, we utilize an attention mechanism to identify relevant"
        },
        "filled": false,
        "gsrank": 437,
        "pub_url": "https://aclanthology.org/2020.acl-main.588/",
        "author_id": [
            "",
            "2Q-7u3AAAAAJ",
            "p4RNNCQAAAAJ",
            "bKaielcAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:wEID0tw1XYsJ:scholar.google.com/&output=cite&scirp=436&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D430%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wEID0tw1XYsJ&ei=9IIBZ83AM9KAy9YPm-_p6Ao&json=",
        "num_citations": 327,
        "citedby_url": "/scholar?cites=10042241966638580416&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:wEID0tw1XYsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://aclanthology.org/2020.acl-main.588.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "PL-Transformer: a POS-aware and layer ensemble transformer for text classification",
            "author": [
                "Y Shi",
                "X Zhang",
                "N Yu"
            ],
            "pub_year": "2023",
            "venue": "Neural Computing and Applications",
            "abstract": "a masked multi-head attention module, a normalization layer,  a The architecture of using  transformer for text classification.  our hook function with the attention mechanism in Sect. 4.6)."
        },
        "filled": false,
        "gsrank": 438,
        "pub_url": "https://link.springer.com/article/10.1007/s00521-022-07872-4",
        "author_id": [
            "",
            "6sRtx0cAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:JI3M5HnclxAJ:scholar.google.com/&output=cite&scirp=437&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D430%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=JI3M5HnclxAJ&ei=9IIBZ83AM9KAy9YPm-_p6Ao&json=",
        "num_citations": 10,
        "citedby_url": "/scholar?cites=1195666642177920292&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:JI3M5HnclxAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s00521-022-07872-4.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "ACTNet: A dual-attention adapter with a CNN-transformer network for the semantic segmentation of remote sensing imagery",
            "author": [
                "Z Zhang",
                "F Liu",
                "C Liu",
                "Q Tian",
                "H Qu"
            ],
            "pub_year": "2023",
            "venue": "Remote Sensing",
            "abstract": "Transformer, the ResAttn module is designed as an adapter in this paper. Its dual attention  mechanism  and presented in the Vision Transformer architecture, which relies on its attention"
        },
        "filled": false,
        "gsrank": 439,
        "pub_url": "https://www.mdpi.com/2072-4292/15/9/2363",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:lgovKZdo4DwJ:scholar.google.com/&output=cite&scirp=438&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D430%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lgovKZdo4DwJ&ei=9IIBZ83AM9KAy9YPm-_p6Ao&json=",
        "num_citations": 7,
        "citedby_url": "/scholar?cites=4386621035499162262&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:lgovKZdo4DwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.mdpi.com/2072-4292/15/9/2363/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Uformer: A general u-shaped transformer for image restoration",
            "author": [
                "Z Wang",
                "X Cun",
                "J Bao",
                "W Zhou"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the …",
            "abstract": "and efficient Transformer-based architecture for image  into the attention module, so the  attention calculation can be  As shown in Figure 3, we first apply a linear projection layer to each"
        },
        "filled": false,
        "gsrank": 440,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2022/html/Wang_Uformer_A_General_U-Shaped_Transformer_for_Image_Restoration_CVPR_2022_paper.html",
        "author_id": [
            "Ya5VDjQAAAAJ",
            "p42qwXcAAAAJ",
            "hjwvkYUAAAAJ",
            "8s1JF8YAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:FMpNd9sguMIJ:scholar.google.com/&output=cite&scirp=439&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D430%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FMpNd9sguMIJ&ei=9IIBZ83AM9KAy9YPm-_p6Ao&json=",
        "num_citations": 1463,
        "citedby_url": "/scholar?cites=14031000766044293652&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:FMpNd9sguMIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Uformer_A_General_U-Shaped_Transformer_for_Image_Restoration_CVPR_2022_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Efficient brain tumor segmentation using Swin transformer and enhanced local self-attention",
            "author": [
                "F Ghazouani",
                "P Vera",
                "S Ruan"
            ],
            "pub_year": "2024",
            "venue": "International Journal of Computer Assisted …",
            "abstract": "presented model that uses the ELSA transformer module in  lower accuracy with the attention  mechanism may be due to  the proposed architecture with ELSA transformer blocks into the"
        },
        "filled": false,
        "gsrank": 441,
        "pub_url": "https://link.springer.com/article/10.1007/s11548-023-03024-8",
        "author_id": [
            "n5qawqMAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:LARSrixh4AEJ:scholar.google.com/&output=cite&scirp=440&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D440%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LARSrixh4AEJ&ei=v4MBZ-GAHL-Ay9YPkfXz2Qs&json=",
        "num_citations": 7,
        "citedby_url": "/scholar?cites=135214833352180780&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:LARSrixh4AEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s11548-023-03024-8.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "ST-YOLOA: a Swin-transformer-based YOLO model with an attention mechanism for SAR ship detection under complex background",
            "author": [
                "K Zhao",
                "R Lu",
                "S Wang",
                "X Yang",
                "Q Li"
            ],
            "pub_year": "2023",
            "venue": "Frontiers in …",
            "abstract": "First, the Swin Transformer network architecture and coordinate attention ( after the Patch  Merging layer. CA  the SE attention mechanism in the Bottleneck module to selectively"
        },
        "filled": false,
        "gsrank": 442,
        "pub_url": "https://www.frontiersin.org/articles/10.3389/fnbot.2023.1170163/full",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:rfEsiNK66pMJ:scholar.google.com/&output=cite&scirp=441&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D440%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rfEsiNK66pMJ&ei=v4MBZ-GAHL-Ay9YPkfXz2Qs&json=",
        "num_citations": 7,
        "citedby_url": "/scholar?cites=10658536881516245421&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:rfEsiNK66pMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.frontiersin.org/articles/10.3389/fnbot.2023.1170163/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Hybrid transformer and cnn attention network for stereo image super-resolution",
            "author": [
                "M Cheng",
                "H Ma",
                "Q Ma",
                "X Sun",
                "W Li"
            ],
            "pub_year": "2023",
            "venue": "Proceedings of the …",
            "abstract": ", we propose a hybrid architecture that utilizes the strong long RCAN [39] incorporates the  attention mechanism in a basic  images using a transformer-based SISR module and a CNN"
        },
        "filled": false,
        "gsrank": 443,
        "pub_url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Cheng_Hybrid_Transformer_and_CNN_Attention_Network_for_Stereo_Image_Super-Resolution_CVPRW_2023_paper.html",
        "author_id": [
            "",
            "",
            "",
            "MumqpkkAAAAJ",
            "SIkQdEsAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:hHfGAZN71IAJ:scholar.google.com/&output=cite&scirp=442&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D440%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=hHfGAZN71IAJ&ei=v4MBZ-GAHL-Ay9YPkfXz2Qs&json=",
        "num_citations": 17,
        "citedby_url": "/scholar?cites=9283180603237627780&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:hHfGAZN71IAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Cheng_Hybrid_Transformer_and_CNN_Attention_Network_for_Stereo_Image_Super-Resolution_CVPRW_2023_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Reason Generation for Point of Interest Recommendation Via a Hierarchical Attention-Based Transformer Model",
            "author": [
                "Y Wu",
                "G Zhao",
                "M Li",
                "Z Zhang"
            ],
            "pub_year": "2023",
            "venue": "IEEE Transactions on …",
            "abstract": "architecture of our hierarchical attention-based Transformer ( as the pretraining model in the  embedding layer. In addition,  archical attention mechanism containing word-level attention"
        },
        "filled": false,
        "gsrank": 444,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10330074/",
        "author_id": [
            "bRgptuYAAAAJ",
            "M1tGUAwAAAAJ",
            "",
            "Ox6GOgcAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:DEmrsOmCcnEJ:scholar.google.com/&output=cite&scirp=443&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D440%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DEmrsOmCcnEJ&ei=v4MBZ-GAHL-Ay9YPkfXz2Qs&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=8174740213833877772&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:DEmrsOmCcnEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6046/4456689/10330074.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Adaptive semantic-enhanced transformer for image captioning",
            "author": [
                "J Zhang",
                "Z Fang",
                "H Sun",
                "Z Wang"
            ],
            "pub_year": "2022",
            "venue": "… on Neural Networks and …",
            "abstract": "gated mechanism (AGM) module to adjust the attention  has gradually replaced the CNN–RNN  architecture and  the multihead attention mechanism, our AS-Transformer can adaptively"
        },
        "filled": false,
        "gsrank": 445,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9810877/",
        "author_id": [
            "e9vchiMAAAAJ",
            "",
            "",
            "Pf_SU3EAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:e12I-kVY8ncJ:scholar.google.com/&output=cite&scirp=444&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D440%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=e12I-kVY8ncJ&ei=v4MBZ-GAHL-Ay9YPkfXz2Qs&json=",
        "num_citations": 24,
        "citedby_url": "/scholar?cites=8643067692456631675&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:e12I-kVY8ncJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/5962385/10422842/09810877.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "DSS-TRM: Deep spatial–spectral transformer for hyperspectral image classification",
            "author": [
                "B Liu",
                "A Yu",
                "K Gao",
                "X Tan",
                "Y Sun"
            ],
            "pub_year": "2022",
            "venue": "European Journal of …",
            "abstract": "of deep convolution networks, attention mechanism is also  convolutional neural networks  and attention based methods.  A simplified 2D-3D CNN architecture for hyperspectral image"
        },
        "filled": false,
        "gsrank": 446,
        "pub_url": "https://www.tandfonline.com/doi/abs/10.1080/22797254.2021.2023910",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Om-YUVKjclIJ:scholar.google.com/&output=cite&scirp=445&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D440%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Om-YUVKjclIJ&ei=v4MBZ-GAHL-Ay9YPkfXz2Qs&json=",
        "num_citations": 40,
        "citedby_url": "/scholar?cites=5940990432406695738&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Om-YUVKjclIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.tandfonline.com/doi/pdf/10.1080/22797254.2021.2023910"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Visualizing and understanding patch interactions in vision transformer",
            "author": [
                "J Ma",
                "Y Bai",
                "B Zhong",
                "W Zhang"
            ],
            "pub_year": "2023",
            "venue": "… on Neural Networks …",
            "abstract": "We propose a window-free modified attention mechanism to  a simple yet transformer  architecture by incorporating the  Therefore, the window-free multihead attention module computes"
        },
        "filled": false,
        "gsrank": 447,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10132428/",
        "author_id": [
            "knf5BREAAAAJ",
            "iYMBoHwAAAAJ",
            "hvRBydsAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:H4i9xNmp6VAJ:scholar.google.com/&output=cite&scirp=446&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D440%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=H4i9xNmp6VAJ&ei=v4MBZ-GAHL-Ay9YPkfXz2Qs&json=",
        "num_citations": 25,
        "citedby_url": "/scholar?cites=5830377945381570591&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:H4i9xNmp6VAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/5962385/6104215/10132428.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Skeleton-based action recognition via spatial and temporal transformer networks",
            "author": [
                "C Plizzari",
                "M Cannici",
                "M Matteucci"
            ],
            "pub_year": "2021",
            "venue": "Computer Vision and Image …",
            "abstract": ", we introduce a Temporal Self-Attention (TSA) module to study the  Transformer (ST-TR)  network, an architecture which uses  , a layer of standard convolution with our transformer"
        },
        "filled": false,
        "gsrank": 448,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1077314221000631",
        "author_id": [
            "OlK2hyIAAAAJ",
            "Xd9geyMAAAAJ",
            "PdbEg5YAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Kak2qrTPBcMJ:scholar.google.com/&output=cite&scirp=447&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D440%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Kak2qrTPBcMJ&ei=v4MBZ-GAHL-Ay9YPkfXz2Qs&json=",
        "num_citations": 304,
        "citedby_url": "/scholar?cites=14052866587136272681&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Kak2qrTPBcMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1077314221000631"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Human behavior recognition based on sparse transformer with channel attention mechanism",
            "author": [
                "K Cao",
                "M Wang"
            ],
            "pub_year": "2023",
            "venue": "Frontiers in Physiology",
            "abstract": "The Transformer architecture is a deep neural network initially developed for natural language  processing.  In the self-attention module of the Transformer model, we introduced a dual"
        },
        "filled": false,
        "gsrank": 449,
        "pub_url": "https://www.frontiersin.org/articles/10.3389/fphys.2023.1239453/full",
        "author_id": [
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:XN2kBpEqRQYJ:scholar.google.com/&output=cite&scirp=448&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D440%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XN2kBpEqRQYJ&ei=v4MBZ-GAHL-Ay9YPkfXz2Qs&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:XN2kBpEqRQYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.frontiersin.org/articles/10.3389/fphys.2023.1239453/pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting",
            "author": [
                "S Li",
                "X Jin",
                "Y Xuan",
                "X Zhou",
                "W Chen"
            ],
            "pub_year": "2019",
            "venue": "Advances in neural …",
            "abstract": "new architecture which leverages attention mechanism to  The well-known self-attention  based Transformer [1] has recently  a 3-layer canonical Transformer with standard self-attention."
        },
        "filled": false,
        "gsrank": 450,
        "pub_url": "https://proceedings.neurips.cc/paper/2019/hash/6775a0635c302542da2c32aa19d86be0-Abstract.html",
        "author_id": [
            "4zli0KkAAAAJ",
            "EWiYf7YAAAAJ",
            "oOrQ3TMAAAAJ",
            "vbZv3_YAAAAJ",
            "U8ShbhUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:d25xHLggwU8J:scholar.google.com/&output=cite&scirp=449&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D440%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=d25xHLggwU8J&ei=v4MBZ-GAHL-Ay9YPkfXz2Qs&json=",
        "num_citations": 1616,
        "citedby_url": "/scholar?cites=5746910574624730743&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:d25xHLggwU8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "TGF: Multiscale transformer graph attention network for multi-sensor image fusion",
            "author": [
                "HT Mustafa",
                "P Shamsolmoali",
                "IH Lee"
            ],
            "pub_year": "2024",
            "venue": "Expert Systems with Applications",
            "abstract": "The graph attention mechanism provides additional  IR and VI fusion methods and graph  attention-based low-level vision  network architecture that consists of conv layers, transformer"
        },
        "filled": false,
        "gsrank": 451,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0957417423022911",
        "author_id": [
            "3xJiTWwAAAAJ",
            "SGNlMswAAAAJ",
            "nxxe9zEAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:rkX1NuB4g44J:scholar.google.com/&output=cite&scirp=450&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D450%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rkX1NuB4g44J&ei=iYQBZ7_aNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 8,
        "citedby_url": "/scholar?cites=10269184479724914094&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:rkX1NuB4g44J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0957417423022911"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Dual-branch adaptive attention transformer for occluded person re-identification",
            "author": [
                "Y Lu",
                "M Jiang",
                "Z Liu",
                "X Mu"
            ],
            "pub_year": "2023",
            "venue": "Image and Vision Computing",
            "abstract": "this two-stage architecture also complicates the model. To solve  Token Attention (STA)  module. STA can utilize the multi-headed  , the attention mechanism is used to select the network"
        },
        "filled": false,
        "gsrank": 452,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0262885623000070",
        "author_id": [
            "",
            "",
            "_FrbidoAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:iYH6lpVweTsJ:scholar.google.com/&output=cite&scirp=451&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D450%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=iYH6lpVweTsJ&ei=iYQBZ7_aNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 12,
        "citedby_url": "/scholar?cites=4285580308205175177&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:iYH6lpVweTsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0262885623000070"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Axial Attention Transformer Networks: A New Frontier in Breast Cancer Detection",
            "author": [
                "W He",
                "R Bao",
                "Y Cang",
                "J Wei",
                "Y Zhang",
                "J Hu"
            ],
            "pub_year": "2024",
            "venue": "arXiv preprint arXiv …",
            "abstract": "an axial attention mechanism with gated units. (4) By using the axial attention module with   The U-Net architecture has been a standard in medical image segmentation, yet it falls short"
        },
        "filled": false,
        "gsrank": 453,
        "pub_url": "https://arxiv.org/abs/2409.12347",
        "author_id": [
            "",
            "VONJa7gAAAAJ",
            "",
            "y05Qfi0AAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:IYgCagJb68gJ:scholar.google.com/&output=cite&scirp=452&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D450%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IYgCagJb68gJ&ei=iYQBZ7_aNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:IYgCagJb68gJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2409.12347"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multi-encoder-decoder transformer for code-switching speech recognition",
            "author": [
                "X Zhou",
                "E Yılmaz",
                "Y Long",
                "Y Li",
                "H Li"
            ],
            "pub_year": "2020",
            "venue": "arXiv preprint arXiv:2006.10414",
            "abstract": "multi-head attention mechanism in the decoder module. Each  We propose a modified  Transformer architecture that takes the  The MED Transformer in our experiment contains 12-layer"
        },
        "filled": false,
        "gsrank": 454,
        "pub_url": "https://arxiv.org/abs/2006.10414",
        "author_id": [
            "",
            "s8mECnwAAAAJ",
            "dR3syukAAAAJ",
            "",
            "z8_x7C8AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:XBx-7TJlaYcJ:scholar.google.com/&output=cite&scirp=453&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D450%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XBx-7TJlaYcJ&ei=iYQBZ7_aNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 53,
        "citedby_url": "/scholar?cites=9757441337082125404&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:XBx-7TJlaYcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2006.10414"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Learning multiple attention transformer super-resolution method for grape disease recognition",
            "author": [
                "H Jin",
                "X Chu",
                "J Qi",
                "J Feng",
                "W Mu"
            ],
            "pub_year": "2024",
            "venue": "Expert Systems with Applications",
            "abstract": "enhanced feature attention mechanism module CSAB are  layer, two concatenate layers  and an upper sampling layer ( of transformer structure construction, the shuffle architecture"
        },
        "filled": false,
        "gsrank": 455,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0957417423032190",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Pts9F9ffLTQJ:scholar.google.com/&output=cite&scirp=454&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D450%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Pts9F9ffLTQJ&ei=iYQBZ7_aNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 8,
        "citedby_url": "/scholar?cites=3759907378825124670&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Pts9F9ffLTQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0957417423032190"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Query2label: A simple transformer way to multi-label classification",
            "author": [
                "S Liu",
                "L Zhang",
                "X Yang",
                "H Su",
                "J Zhu"
            ],
            "pub_year": "2021",
            "venue": "arXiv preprint arXiv:2107.10834",
            "abstract": "Transformer decoder architecture is used in classification.  -in cross-attention mechanism in  Transformer decoders, our  and remove the selfattention module in Transformer decoders for"
        },
        "filled": false,
        "gsrank": 456,
        "pub_url": "https://arxiv.org/abs/2107.10834",
        "author_id": [
            "nkSVY3MAAAAJ",
            "fIlGZToAAAAJ",
            "bwkwp0MAAAAJ",
            "dxN1_X0AAAAJ",
            "axsP38wAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:vmRFIEKM7xsJ:scholar.google.com/&output=cite&scirp=455&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D450%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vmRFIEKM7xsJ&ei=iYQBZ7_aNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 208,
        "citedby_url": "/scholar?cites=2012981774095049918&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:vmRFIEKM7xsJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2107.10834"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "TiM‐Net: Transformer in M‐Net for Retinal Vessel Segmentation",
            "author": [
                "H Zhang",
                "X Zhong",
                "Z Li",
                "Y Chen",
                "Z Zhu"
            ],
            "pub_year": "2022",
            "venue": "Journal of …",
            "abstract": "dual-attention mechanism and Transformer module are plug- the dual-attention mechanism  behind the encoder layer to  Hence, it consists of the M-Net architecture, a new encoder"
        },
        "filled": false,
        "gsrank": 457,
        "pub_url": "https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/9016401",
        "author_id": [
            "",
            "",
            "",
            "",
            "rOnqD2IAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:OY6_sKriLCUJ:scholar.google.com/&output=cite&scirp=456&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D450%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OY6_sKriLCUJ&ei=iYQBZ7_aNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 20,
        "citedby_url": "/scholar?cites=2678765101116263993&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:OY6_sKriLCUJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://onlinelibrary.wiley.com/doi/pdf/10.1155/2022/9016401"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Deformable cross-attention transformer for medical image registration",
            "author": [
                "J Chen",
                "Y Liu",
                "Y He",
                "Y Du"
            ],
            "pub_year": "2023",
            "venue": "… Workshop on Machine Learning in Medical …",
            "abstract": "module differs from existing SA and CA modules in that it employs the windowed attention  mechanism [ computation and the full Transformer architecture used by the model. To address"
        },
        "filled": false,
        "gsrank": 458,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-031-45673-2_12",
        "author_id": [
            "9jIpgScAAAAJ",
            "DlOPRbAAAAAJ",
            "btkEZXIAAAAJ",
            "rXoK-TcAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:aViqX9OjJOoJ:scholar.google.com/&output=cite&scirp=457&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D450%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=aViqX9OjJOoJ&ei=iYQBZ7_aNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 11,
        "citedby_url": "/scholar?cites=16871790232275146857&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:aViqX9OjJOoJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2303.06179"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Efficient visual tracking via hierarchical cross-attention transformer",
            "author": [
                "X Chen",
                "B Kang",
                "D Wang",
                "D Li",
                "H Lu"
            ],
            "pub_year": "2022",
            "venue": "European Conference on Computer …",
            "abstract": "Without the FS module in our method, the attention mechanism  architecture of our hierarchical  cross-attention transformer is  In experiments, we find that the self-attention layer brings"
        },
        "filled": false,
        "gsrank": 459,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-031-25085-9_26",
        "author_id": [
            "A04HWTIAAAAJ",
            "By9F6bwAAAAJ",
            "nVgPQpoAAAAJ",
            "",
            "D3nE0agAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:yykRlI5aas4J:scholar.google.com/&output=cite&scirp=458&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D450%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yykRlI5aas4J&ei=iYQBZ7_aNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 49,
        "citedby_url": "/scholar?cites=14873800287760493003&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:yykRlI5aas4J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2203.13537"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Graph transformer network with temporal kernel attention for skeleton-based action recognition",
            "author": [
                "Y Liu",
                "H Zhang",
                "D Xu",
                "K He"
            ],
            "pub_year": "2022",
            "venue": "Knowledge-Based Systems",
            "abstract": ", and hence incorporating an attention mechanism to weigh  encoding module is unnecessary  in our graph transformer layer  robust network architecture kernel attention adaptive graph"
        },
        "filled": false,
        "gsrank": 460,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0950705122000211",
        "author_id": [
            "",
            "FghvaskAAAAJ",
            "OuSPv-AAAAAJ",
            "LdAyR2oAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:XXvIZsLhN_EJ:scholar.google.com/&output=cite&scirp=459&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D450%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XXvIZsLhN_EJ&ei=iYQBZ7_aNYWoy9YPtZ-o2Q4&json=",
        "num_citations": 91,
        "citedby_url": "/scholar?cites=17381609511924038493&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:XXvIZsLhN_EJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0950705122000211"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A Joint Architecture of Mixed-Attention Transformer and Octave Module for Hyperspectral Image Denoising",
            "author": [
                "M Ashraf",
                "L Chen",
                "X Zhou"
            ],
            "pub_year": "2024",
            "venue": "IEEE Journal of Selected …",
            "abstract": "extraction module and spatial-channel attention mechanism.  built with layer normalization  (LayerNorm), attention module,  module without the inclusion of an attention network or base"
        },
        "filled": false,
        "gsrank": 461,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10410657/",
        "author_id": [
            "aDHhsiMAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:me1HZM_TT1UJ:scholar.google.com/&output=cite&scirp=460&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D460%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=me1HZM_TT1UJ&ei=VYUBZ-H_D9KAy9YPm-_p6Ao&json=",
        "num_citations": 2,
        "citedby_url": "/scholar?cites=6147364904078142873&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:me1HZM_TT1UJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/4609443/4609444/10410657.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Enhancing dynamic ECG heartbeat classification with lightweight transformer model",
            "author": [
                "L Meng",
                "W Tan",
                "J Ma",
                "R Wang",
                "X Yin"
            ],
            "pub_year": "2022",
            "venue": "Artificial Intelligence in …",
            "abstract": "a two-level attention mechanism: the local attention mechanism in the  Architecture of CNN  Input embedding with local attention.  ) is embedded into a CNN module that learns the local"
        },
        "filled": false,
        "gsrank": 462,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S093336572200001X",
        "author_id": [
            "58R6y5cAAAAJ",
            "IjNEk4UAAAAJ",
            "1DX4WFkAAAAJ",
            "",
            "ooPNqqcAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:GQInznZOK9wJ:scholar.google.com/&output=cite&scirp=461&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D460%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GQInznZOK9wJ&ei=VYUBZ-H_D9KAy9YPm-_p6Ao&json=",
        "num_citations": 67,
        "citedby_url": "/scholar?cites=15864860384514474521&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:GQInznZOK9wJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S093336572200001X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Medical image segmentation using transformer networks",
            "author": [
                "D Karimi",
                "H Dou",
                "A Gholipour"
            ],
            "pub_year": "2022",
            "venue": "IEEE Access",
            "abstract": "neural network architecture, based entirely on self-attention  propose a self-attention-based  deep neural network for 3D  of self-attention models because the attention mechanism is"
        },
        "filled": false,
        "gsrank": 463,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9729189/",
        "author_id": [
            "",
            "H1ZHZ14AAAAJ",
            "mPB7nkYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:TIJejvoNnHwJ:scholar.google.com/&output=cite&scirp=462&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D460%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TIJejvoNnHwJ&ei=VYUBZ-H_D9KAy9YPm-_p6Ao&json=",
        "num_citations": 32,
        "citedby_url": "/scholar?cites=8979067126851469900&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:TIJejvoNnHwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/6287639/6514899/09729189.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A transformer-based siamese network for change detection",
            "author": [
                "WGC Bandara",
                "VM Patel"
            ],
            "pub_year": "2022",
            "venue": "IGARSS 2022-2022 IEEE …",
            "abstract": "paper presents a transformer-based Siamese network architecture ( structured transformer  encoder with Multi-Layer Perception ( encoder is self-attention module. In the original work [7],"
        },
        "filled": false,
        "gsrank": 464,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9883686/",
        "author_id": [
            "WwLxOJYAAAAJ",
            "AkEXTbIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:tcxyXLEwcWgJ:scholar.google.com/&output=cite&scirp=463&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D460%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=tcxyXLEwcWgJ&ei=VYUBZ-H_D9KAy9YPm-_p6Ao&json=",
        "num_citations": 467,
        "citedby_url": "/scholar?cites=7525849990631181493&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:tcxyXLEwcWgJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/9883023/9883024/09883686.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "HCA-former: Hybrid Convolution Attention Transformer for 3D Medical Image Segmentation",
            "author": [
                "F Yang",
                "F Wang",
                "P Dong",
                "B Wang"
            ],
            "pub_year": "2024",
            "venue": "Biomedical Signal Processing and …",
            "abstract": "of CNN into the architecture of the Transformer. It confines  transformer layers are introduced  in each module, where the  a new dual-headed attention mechanism inserted into the DFB."
        },
        "filled": false,
        "gsrank": 465,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1746809423012673",
        "author_id": [
            "",
            "",
            "",
            "5zWXDFEAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:O3bezNMa9hMJ:scholar.google.com/&output=cite&scirp=464&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D460%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=O3bezNMa9hMJ&ei=VYUBZ-H_D9KAy9YPm-_p6Ao&json=",
        "num_citations": 2,
        "citedby_url": "/scholar?cites=1438366627969005115&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:O3bezNMa9hMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1746809423012673"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A lightweight transformer network for hyperspectral image classification",
            "author": [
                "X Zhang",
                "Y Su",
                "L Gao",
                "L Bruzzone"
            ],
            "pub_year": "2023",
            "venue": "IEEE Transactions on …",
            "abstract": "attention module that feeds attention heads with different splits of the full features to reduce  computational redundancy. A super token attention mechanism [ a hybrid architecture for HSI"
        },
        "filled": false,
        "gsrank": 466,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10189879/",
        "author_id": [
            "",
            "-3MRWsgAAAAJ",
            "La-8gLMAAAAJ",
            "ff9-TK4AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:yOel-Kg9nB0J:scholar.google.com/&output=cite&scirp=465&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D460%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yOel-Kg9nB0J&ei=VYUBZ-H_D9KAy9YPm-_p6Ao&json=",
        "num_citations": 29,
        "citedby_url": "/scholar?cites=2133648119402194888&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:yOel-Kg9nB0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/36/10006360/10189879.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Hyperspectral image classification with multi-attention transformer and adaptive superpixel segmentation-based active learning",
            "author": [
                "C Zhao",
                "B Qin",
                "S Feng",
                "W Zhu",
                "W Sun"
            ],
            "pub_year": "2023",
            "venue": "IEEE Transactions on …",
            "abstract": ", the self-attention module of Transformer is applied to model  The detailed architecture of  multi-attention Transformer (MAT The value is computed via a linear layer WV ∈ RC×C, and V"
        },
        "filled": false,
        "gsrank": 467,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10167502/",
        "author_id": [
            "",
            "",
            "",
            "15g3dDoAAAAJ",
            "nrzPzUIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Vid7gKgVXmcJ:scholar.google.com/&output=cite&scirp=466&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D460%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Vid7gKgVXmcJ&ei=VYUBZ-H_D9KAy9YPm-_p6Ao&json=",
        "num_citations": 83,
        "citedby_url": "/scholar?cites=7448414647171622742&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Vid7gKgVXmcJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/83/4358840/10167502.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "RTFormer: Efficient design for real-time semantic segmentation with transformer",
            "author": [
                "J Wang",
                "C Gou",
                "Q Wu",
                "H Feng",
                "J Han"
            ],
            "pub_year": "2022",
            "venue": "Advances in Neural …",
            "abstract": "attention [15], we developed a GPU-Friendly attention module  attention mechanism while  achieving the real-time speed.  And for the design of multi-resolution architecture, we can"
        },
        "filled": false,
        "gsrank": 468,
        "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/30e10e671c5e43edb67eb257abb6c3ea-Abstract-Conference.html",
        "author_id": [
            "hDPRTekAAAAJ",
            "tlhShPsAAAAJ",
            "",
            "pnuQ5UsAAAAJ",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:lZu1bfznIAAJ:scholar.google.com/&output=cite&scirp=467&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D460%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=lZu1bfznIAAJ&ei=VYUBZ-H_D9KAy9YPm-_p6Ao&json=",
        "num_citations": 74,
        "citedby_url": "/scholar?cites=9262270613134229&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:lZu1bfznIAAJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/30e10e671c5e43edb67eb257abb6c3ea-Paper-Conference.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A time patch dynamic attention transformer for enhanced well production forecasting in complex oilfield operations",
            "author": [
                "T Huang",
                "H Qian",
                "Z Huang",
                "NH Xu",
                "X Huang",
                "D Yin"
            ],
            "pub_year": "2024",
            "venue": "Energy",
            "abstract": "fusion causal temporal attention mechanism, effectively overcoming  into an improved  Transformer architecture, retaining its  enables the model to adaptively allocate attention based on"
        },
        "filled": false,
        "gsrank": 469,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S036054422402961X",
        "author_id": [
            "PnBlwhgAAAAJ",
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:1U-WhCbFq54J:scholar.google.com/&output=cite&scirp=468&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D460%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=1U-WhCbFq54J&ei=VYUBZ-H_D9KAy9YPm-_p6Ao&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:1U-WhCbFq54J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S036054422402961X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Convolution transformer mixer for hyperspectral image classification",
            "author": [
                "J Zhang",
                "Z Meng",
                "F Zhao",
                "H Liu"
            ],
            "pub_year": "2022",
            "venue": "IEEE Geoscience and …",
            "abstract": "spatial convolution module and a transformer encoder module,  of convolution operation  and attention mechanism, we intro a 1D pooling layer is adopted to reduce network parameters"
        },
        "filled": false,
        "gsrank": 470,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9903640/",
        "author_id": [
            "",
            "fT7w5MAAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:KWpD3nVG6iMJ:scholar.google.com/&output=cite&scirp=469&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D460%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=KWpD3nVG6iMJ&ei=VYUBZ-H_D9KAy9YPm-_p6Ao&json=",
        "num_citations": 71,
        "citedby_url": "/scholar?cites=2587958407931849257&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:KWpD3nVG6iMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/8859/4357975/09903640.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "MSGformer: A multi-scale grid transformer network for 12-lead ECG arrhythmia detection",
            "author": [
                "C Ji",
                "L Wang",
                "J Qin",
                "L Liu",
                "Y Han",
                "Z Wang"
            ],
            "pub_year": "2024",
            "venue": "Biomedical Signal Processing …",
            "abstract": "grid attention mechanism to capture temporal features. The  Specifically, through  enhanced attention-based QKV  The original Transformer architecture is an effective model"
        },
        "filled": false,
        "gsrank": 471,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1746809423009321",
        "author_id": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:RGxaBmgwkpEJ:scholar.google.com/&output=cite&scirp=470&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D470%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RGxaBmgwkpEJ&ei=IIYBZ_rdHoWoy9YPtZ-o2Q4&json=",
        "num_citations": 14,
        "citedby_url": "/scholar?cites=10489499705440627780&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:RGxaBmgwkpEJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S1746809423009321"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Rstnet: Captioning with adaptive attention on visual and non-visual words",
            "author": [
                "X Zhang",
                "X Sun",
                "Y Luo",
                "J Ji",
                "Y Zhou"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the …",
            "abstract": "We apply the GA module and AA module to our transformer based image captioning   transformer architecture for captioning. [13] proposed a GLU like structure on attention mechanism"
        },
        "filled": false,
        "gsrank": 472,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2021/html/Zhang_RSTNet_Captioning_With_Adaptive_Attention_on_Visual_and_Non-Visual_Words_CVPR_2021_paper.html",
        "author_id": [
            "76_hOG0AAAAJ",
            "KPMK3B4AAAAJ",
            "xj0KzfEAAAAJ",
            "xp_rICcAAAAJ",
            "w3_2ep0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:YIRVnlE7WEYJ:scholar.google.com/&output=cite&scirp=471&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D470%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YIRVnlE7WEYJ&ei=IIYBZ_rdHoWoy9YPtZ-o2Q4&json=",
        "num_citations": 228,
        "citedby_url": "/scholar?cites=5068866602340287584&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:YIRVnlE7WEYJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_RSTNet_Captioning_With_Adaptive_Attention_on_Visual_and_Non-Visual_Words_CVPR_2021_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "BVA-Transformer: Image-text multimodal classification and dialogue model architecture based on Blip and visual attention mechanism",
            "author": [
                "K Zhang",
                "F Wu",
                "G Zhang",
                "J Liu",
                "M Li"
            ],
            "pub_year": "2024",
            "venue": "Displays",
            "abstract": "This paper proposes BVA-Transformer model architecture for  ) module based on visual  attention in the BVA-Transformer,  network (FFN) layer in Transformers. In the Decoder of BVA-"
        },
        "filled": false,
        "gsrank": 473,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S014193822400074X",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:MEkWxvIJFREJ:scholar.google.com/&output=cite&scirp=472&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D470%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MEkWxvIJFREJ&ei=IIYBZ_rdHoWoy9YPtZ-o2Q4&json=",
        "num_citations": 0,
        "url_related_articles": "/scholar?q=related:MEkWxvIJFREJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S014193822400074X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Dynamic detr: End-to-end object detection with dynamic attention",
            "author": [
                "X Dai",
                "Y Chen",
                "J Yang",
                "P Zhang"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the …",
            "abstract": "self-attention module in Transformer encoders, we propose a dynamic encoder to approximate  the Transformer encoder’s attention mechanism  ral network and a top down architecture"
        },
        "filled": false,
        "gsrank": 474,
        "pub_url": "https://openaccess.thecvf.com/content/ICCV2021/html/Dai_Dynamic_DETR_End-to-End_Object_Detection_With_Dynamic_Attention_ICCV_2021_paper.html?ref=https://githubhelp.com",
        "author_id": [
            "QC8RwcoAAAAJ",
            "V_VpLksAAAAJ",
            "Cl9byD8AAAAJ",
            "3VZ_E64AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Qk06w_64QcMJ:scholar.google.com/&output=cite&scirp=473&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D470%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Qk06w_64QcMJ&ei=IIYBZ_rdHoWoy9YPtZ-o2Q4&json=",
        "num_citations": 298,
        "citedby_url": "/scholar?cites=14069730115218722114&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Qk06w_64QcMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Dai_Dynamic_DETR_End-to-End_Object_Detection_With_Dynamic_Attention_ICCV_2021_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "HyperAttentionDTI: improving drug–protein interaction prediction by sequence-based deep learning with attention mechanism",
            "author": [
                "Q Zhao",
                "H Zhao",
                "K Zheng",
                "J Wang"
            ],
            "pub_year": "2022",
            "venue": "Bioinformatics",
            "abstract": "attention-based models, our model infers an attention vector  into attention vectors, da i and  pa j , by multi-layer perceptron  Then it utilizes Transformer embedding modules to obtain the"
        },
        "filled": false,
        "gsrank": 475,
        "pub_url": "https://academic.oup.com/bioinformatics/article-abstract/38/3/655/6401997",
        "author_id": [
            "7ixMXO0AAAAJ",
            "",
            "wGc3CvcAAAAJ",
            "7pgY2F0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:wXEBkLBTYRIJ:scholar.google.com/&output=cite&scirp=474&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D470%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wXEBkLBTYRIJ&ei=IIYBZ_rdHoWoy9YPtZ-o2Q4&json=",
        "num_citations": 113,
        "citedby_url": "/scholar?cites=1324431783218999745&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:wXEBkLBTYRIJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://academic.oup.com/bioinformatics/article-pdf/38/3/655/49008432/btab715.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "A sparse-based transformer network with associated spatiotemporal feature for micro-expression recognition",
            "author": [
                "J Zhu",
                "Y Zong",
                "H Chang",
                "Y Xiao"
            ],
            "pub_year": "2022",
            "venue": "IEEE Signal Processing …",
            "abstract": "architecture of our proposed method is presented. Specially, we will illustrate the sparse  multi-head self-attention module  [21] network except for the final average pooling layer as the"
        },
        "filled": false,
        "gsrank": 476,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/9906452/",
        "author_id": [
            "",
            "rv14ap4AAAAJ",
            "zNyXZAoAAAAJ",
            "8PbTFgMAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:Om1PvPy6t44J:scholar.google.com/&output=cite&scirp=475&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D470%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Om1PvPy6t44J&ei=IIYBZ_rdHoWoy9YPtZ-o2Q4&json=",
        "num_citations": 13,
        "citedby_url": "/scholar?cites=10283893868777663802&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:Om1PvPy6t44J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel7/97/4358004/09906452.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Multimodal Transformer Networks for Pedestrian Trajectory Prediction.",
            "author": [
                "Z Yin",
                "R Liu",
                "Z Xiong",
                "Z Yuan"
            ],
            "pub_year": "2021",
            "venue": "IJCAI",
            "abstract": "network is only attention-based that can efficiently model long of a cross-attention module  and a feed-forward layer. Then the  architecture takes the advantage of attention mechanism to"
        },
        "filled": false,
        "gsrank": 477,
        "pub_url": "https://www.ijcai.org/proceedings/2021/0174.pdf",
        "author_id": [
            "wvbK37AAAAAJ",
            "AsrEjh4AAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:s2CWM4fYELMJ:scholar.google.com/&output=cite&scirp=476&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D470%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=s2CWM4fYELMJ&ei=IIYBZ_rdHoWoy9YPtZ-o2Q4&json=",
        "num_citations": 37,
        "citedby_url": "/scholar?cites=12903051007614148787&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:s2CWM4fYELMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.ijcai.org/proceedings/2021/0174.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "ResneSt-Transformer: Joint attention segmentation-free for end-to-end handwriting paragraph recognition model",
            "author": [
                "M Hamdan",
                "M Cheriet"
            ],
            "pub_year": "2023",
            "venue": "Array",
            "abstract": "Subsequently, we develop an encoder module containing four transformer  The attention  mechanism in each block of our proposed architecture plays a crucial role in helping the model"
        },
        "filled": false,
        "gsrank": 478,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S2590005623000255",
        "author_id": [
            "gx_Dn_kAAAAJ",
            "oG89PhIAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:TRdUgYdHQMwJ:scholar.google.com/&output=cite&scirp=477&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D470%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=TRdUgYdHQMwJ&ei=IIYBZ_rdHoWoy9YPtZ-o2Q4&json=",
        "num_citations": 2,
        "citedby_url": "/scholar?cites=14717842229562709837&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:TRdUgYdHQMwJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S2590005623000255"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transmil: Transformer based correlated multiple instance learning for whole slide image classification",
            "author": [
                "Z Shao",
                "H Bian",
                "Y Chen",
                "Y Wang"
            ],
            "pub_year": "2021",
            "venue": "Advances in neural …",
            "abstract": "Then the attention mechanism was gradually applied to computer vision tasks, including   a TPT module with two Transformer layers and a position encoding layer, where Transformer"
        },
        "filled": false,
        "gsrank": 479,
        "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2021/hash/10c272d06794d3e5785d5e7c5356e9ff-Abstract.html",
        "author_id": [
            "",
            "7brFI_4AAAAJ",
            "bYtCPiYAAAAJ",
            "3KKVM5gAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:z1YzunLv27wJ:scholar.google.com/&output=cite&scirp=478&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D470%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=z1YzunLv27wJ&ei=IIYBZ_rdHoWoy9YPtZ-o2Q4&json=",
        "num_citations": 605,
        "citedby_url": "/scholar?cites=13608733975059322575&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:z1YzunLv27wJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/10c272d06794d3e5785d5e7c5356e9ff-Paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Transformer model incorporating local graph semantic attention for image caption",
            "author": [
                "K Qian",
                "Y Pan",
                "H Xu",
                "L Tian"
            ],
            "pub_year": "2023",
            "venue": "The Visual Computer",
            "abstract": "–decoder architecture for image caption, and with the introduction of the attention mechanism,   enters the multi-head self-attention module in the encoder to get the image feature space"
        },
        "filled": false,
        "gsrank": 480,
        "pub_url": "https://link.springer.com/article/10.1007/s00371-023-03180-7",
        "author_id": [
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:OwEH4dsG7I8J:scholar.google.com/&output=cite&scirp=479&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D470%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OwEH4dsG7I8J&ei=IIYBZ_rdHoWoy9YPtZ-o2Q4&json=",
        "num_citations": 2,
        "citedby_url": "/scholar?cites=10370671583370346811&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:OwEH4dsG7I8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://link.springer.com/content/pdf/10.1007/s00371-023-03180-7.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "ResT-ReID: Transformer block-based residual learning for person re-identification",
            "author": [
                "Y Chen",
                "S Xia",
                "J Zhao",
                "Y Zhou",
                "Q Niu",
                "R Yao"
            ],
            "pub_year": "2022",
            "venue": "Pattern Recognition …",
            "abstract": "attention mechanism into GCN by embedding an attention  for Res-Transformer) module  and SIE-AGCN module can  of evaluation protocols over baseline architecture. Especially, we"
        },
        "filled": false,
        "gsrank": 481,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S016786552200085X",
        "author_id": [
            "",
            "",
            "EwyX1nIAAAAJ",
            "",
            "",
            "AveCc5QAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:QIdL_paQVZ0J:scholar.google.com/&output=cite&scirp=480&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D480%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=QIdL_paQVZ0J&ei=64YBZ86HOIWoy9YPtZ-o2Q4&json=",
        "num_citations": 37,
        "citedby_url": "/scholar?cites=11337126615160948544&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:QIdL_paQVZ0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S016786552200085X"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "HMFT: Hyperspectral and Multispectral Image Fusion Super‐Resolution Method Based on Efficient Transformer and Spatial‐Spectral Attention Mechanism",
            "author": [
                "B Qiao",
                "B Xu",
                "Y Xie",
                "Y Lin",
                "Y Liu"
            ],
            "pub_year": "2023",
            "venue": "Computational …",
            "abstract": "and efficient hybrid architecture network based on Transformer is  our model with its variant,  which has no convolutional layer.  the FSM module to the MHA module of the Transformer to"
        },
        "filled": false,
        "gsrank": 482,
        "pub_url": "https://onlinelibrary.wiley.com/doi/abs/10.1155/2023/4725986",
        "author_id": [
            "",
            "",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:3pjcgyN0CEQJ:scholar.google.com/&output=cite&scirp=481&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D480%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3pjcgyN0CEQJ&ei=64YBZ86HOIWoy9YPtZ-o2Q4&json=",
        "num_citations": 3,
        "citedby_url": "/scholar?cites=4902295890277734622&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:3pjcgyN0CEQJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://onlinelibrary.wiley.com/doi/pdf/10.1155/2023/4725986"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Uctransnet: rethinking the skip connections in u-net from a channel-wise perspective with transformer",
            "author": [
                "H Wang",
                "P Cao",
                "J Wang",
                "OR Zaiane"
            ],
            "pub_year": "2022",
            "venue": "Proceedings of the AAAI …",
            "abstract": "decoder architecture. It is still challenging for U-Net with a simple skip connection scheme  to  CTrans module in U-Net), from the channel perspective with attention mechanism. Specif"
        },
        "filled": false,
        "gsrank": 483,
        "pub_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20144",
        "author_id": [
            "KDNRnW0AAAAJ",
            "0OfgZSsAAAAJ",
            "UnCNbQIAAAAJ",
            "j-W_RNYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:_ZefHO3Qj-8J:scholar.google.com/&output=cite&scirp=482&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D480%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_ZefHO3Qj-8J&ei=64YBZ86HOIWoy9YPtZ-o2Q4&json=",
        "num_citations": 615,
        "citedby_url": "/scholar?cites=17262245613540448253&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:_ZefHO3Qj-8J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ojs.aaai.org/index.php/AAAI/article/download/20144/19903"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Parallel Attention-Based Transformer for Channel Estimation in RIS-Aided 6G Wireless Communications",
            "author": [
                "J Guo",
                "G Liu",
                "Q Wu",
                "P Fan"
            ],
            "pub_year": "2024",
            "venue": "IEEE Transactions on Vehicular …",
            "abstract": "Transformer, ie, the attention mechanism, which is a neural network module derived from  human attention  the first convolutional layer in step 16, the attention mechanism from step 18 to"
        },
        "filled": false,
        "gsrank": 484,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10591410/",
        "author_id": [
            "",
            "08suZn4AAAAJ",
            "as1T8CMAAAAJ",
            "89MC1uEAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:d5F231D33NMJ:scholar.google.com/&output=cite&scirp=483&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D480%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=d5F231D33NMJ&ei=64YBZ86HOIWoy9YPtZ-o2Q4&json=",
        "num_citations": 1,
        "citedby_url": "/scholar?cites=15266348763597672823&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:d5F231D33NMJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://ieeexplore.ieee.org/iel8/25/4356907/10591410.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Memory-efficient transformer-based network model for traveling salesman problem",
            "author": [
                "H Yang",
                "M Zhao",
                "L Yuan",
                "Y Yu",
                "Z Li",
                "M Gu"
            ],
            "pub_year": "2023",
            "venue": "Neural Networks",
            "abstract": "to model the network architecture and train the network model. In  Multi-Head Attention  module of the Decoder in Transformer,  scaled dot-product attention mechanism to address the"
        },
        "filled": false,
        "gsrank": 485,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0893608023000771",
        "author_id": [
            "",
            "MgbaOZ4AAAAJ",
            "kbDU8bEAAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:YDho_wypyj0J:scholar.google.com/&output=cite&scirp=484&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D480%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YDho_wypyj0J&ei=64YBZ86HOIWoy9YPtZ-o2Q4&json=",
        "num_citations": 18,
        "citedby_url": "/scholar?cites=4452557054898878560&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:YDho_wypyj0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0893608023000771"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Vision transformer attention with multi-reservoir echo state network for anomaly recognition",
            "author": [
                "W Ullah",
                "T Hussain",
                "SW Baik"
            ],
            "pub_year": "2023",
            "venue": "Information Processing & Management",
            "abstract": "architecture into them. The vision transformer uses the transformer encoder module to   As a result of the deep features extracted with the attention mechanism from the consecutive"
        },
        "filled": false,
        "gsrank": 486,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0306457323000262",
        "author_id": [
            "cVzxZ6YAAAAJ",
            "4fU0t_oAAAAJ",
            "9tXoIf0AAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:j7IWvZ0HFP0J:scholar.google.com/&output=cite&scirp=485&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D480%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=j7IWvZ0HFP0J&ei=64YBZ86HOIWoy9YPtZ-o2Q4&json=",
        "num_citations": 24,
        "citedby_url": "/scholar?cites=18236209165193622159&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:j7IWvZ0HFP0J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://www.sciencedirect.com/science/article/pii/S0306457323000262"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Vitbis: Vision transformer for biomedical image segmentation",
            "author": [
                "A Sagar"
            ],
            "pub_year": "2021",
            "venue": "MICCAI Workshop on Distributed and Collaborative …",
            "abstract": "a novel network incorporating attention mechanism in transformer architecture along with  multi scale module  The structure of Transformer layer used in this work is illustrated in Fig. 1:"
        },
        "filled": false,
        "gsrank": 487,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-90874-4_4",
        "author_id": [
            "5ntkLcgAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:-w-03JVfr18J:scholar.google.com/&output=cite&scirp=486&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D480%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=-w-03JVfr18J&ei=64YBZ86HOIWoy9YPtZ-o2Q4&json=",
        "num_citations": 29,
        "citedby_url": "/scholar?cites=6894834651785072635&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:-w-03JVfr18J:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2201.05920"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Object detection in medical images based on hierarchical transformer and mask mechanism",
            "author": [
                "Y Shou",
                "T Meng",
                "W Ai",
                "C Xie",
                "H Liu"
            ],
            "pub_year": "2022",
            "venue": "Computational …",
            "abstract": "the self-attention mechanism in the Transformer architecture  Therefore, we use the attention  mechanism to capture the  W-MSA module, the SW-MSA module, and the MLP module. The"
        },
        "filled": false,
        "gsrank": 488,
        "pub_url": "https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/5863782",
        "author_id": [
            "",
            "HDQEw7gAAAAJ",
            "",
            "",
            ""
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:R6pSw-8ob1oJ:scholar.google.com/&output=cite&scirp=487&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D480%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=R6pSw-8ob1oJ&ei=64YBZ86HOIWoy9YPtZ-o2Q4&json=",
        "num_citations": 65,
        "citedby_url": "/scholar?cites=6516472196067666503&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:R6pSw-8ob1oJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://onlinelibrary.wiley.com/doi/pdf/10.1155/2022/5863782"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Causal attention for vision-language tasks",
            "author": [
                "X Yang",
                "H Zhang",
                "G Qi",
                "J Cai"
            ],
            "pub_year": "2021",
            "venue": "Proceedings of the IEEE …",
            "abstract": "attention mechanism: Causal Attention (CATT), to remove the ever-elusive confounding effect  in existing attention-based  CATT, BUTD and Transformer based VQA models can attend to"
        },
        "filled": false,
        "gsrank": 489,
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2021/html/Yang_Causal_Attention_for_Vision-Language_Tasks_CVPR_2021_paper.html",
        "author_id": [
            "SqdxMH0AAAAJ",
            "YG0DFyYAAAAJ",
            "Nut-uvoAAAAJ",
            "N6czCoUAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:99Etsis2-3MJ:scholar.google.com/&output=cite&scirp=488&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D480%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=99Etsis2-3MJ&ei=64YBZ86HOIWoy9YPtZ-o2Q4&json=",
        "num_citations": 158,
        "citedby_url": "/scholar?cites=8357333094816928247&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:99Etsis2-3MJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Causal_Attention_for_Vision-Language_Tasks_CVPR_2021_paper.pdf"
    },
    {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Boundary-aware transformers for skin lesion segmentation",
            "author": [
                "J Wang",
                "L Wei",
                "L Wang",
                "Q Zhou",
                "L Zhu"
            ],
            "pub_year": "2021",
            "venue": "Medical Image Computing …",
            "abstract": "employing a powerful global attention mechanism, but one of  -wise attention gate (BAG) in  transformer architecture to  layer in the encoder consists of a multi-head self-attention module"
        },
        "filled": false,
        "gsrank": 490,
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-030-87193-2_20",
        "author_id": [
            "TSje0EkAAAAJ",
            "",
            "",
            "",
            "AQtqhaYAAAAJ"
        ],
        "url_scholarbib": "/scholar?hl=en&q=info:HZW9iZbJe9QJ:scholar.google.com/&output=cite&scirp=489&hl=en",
        "url_add_sclib": "/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D(transformer%2BOR%2Btransformers%2BOR%2Btransformer%2Barchitecture%2BOR%2Btransformer%2Bmodel%2BOR%2Btransformer-based%2BOR%2Btransformer%2Bnetworks)%2BAND%2B(attention%2Bmechanism%2BOR%2Bself-attention%2BOR%2Bmulti-head%2Battention%2BOR%2Battention%2Blayer%2BOR%2Battention%2Bmodule%2BOR%2Battention-based)%26hl%3Den%26start%3D480%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=HZW9iZbJe9QJ&ei=64YBZ86HOIWoy9YPtZ-o2Q4&json=",
        "num_citations": 155,
        "citedby_url": "/scholar?cites=15311053006569313565&as_sdt=5,33&sciodt=0,33&hl=en",
        "url_related_articles": "/scholar?q=related:HZW9iZbJe9QJ:scholar.google.com/&scioq=(transformer+OR+transformers+OR+transformer+architecture+OR+transformer+model+OR+transformer-based+OR+transformer+networks)+AND+(attention+mechanism+OR+self-attention+OR+multi-head+attention+OR+attention+layer+OR+attention+module+OR+attention-based)&hl=en&as_sdt=0,33",
        "eprint_url": "https://arxiv.org/pdf/2110.03864"
    }
]